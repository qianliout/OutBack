# 04_预训练模型

## T5详解

# T5详解

## 概述

T5（Text-to-Text Transfer Transformer）是Google在2019年提出的一个**统一的文本到文本（Text-to-Text）框架**，旨在将所有自然语言处理（NLP）任务都转换为相同的输入和输出格式，即**文本字符串**。这一创新性方法极大地简化了多任务学习和迁移学习的流程。

## 1. T5的核心思想

### 1.1 统一任务格式

T5的核心思想是将所有NLP任务（如翻译、摘要、问答、分类等）都视为"文本到文本"的问题。

*   **输入**：一个文本字符串，通常包含任务前缀（如"translate English to German: "）。
*   **输出**：另一个文本字符串。

### 1.2 Transformer架构

T5采用标准的**编码器-解码器（Encoder-Decoder）Transformer**架构。

### 1.3 大规模预训练

在巨大的C4（Colossal Clean Crawled Corpus）数据集上进行预训练，学习通用的语言知识。

## 2. 统一任务格式示例

| **任务类型** | **输入文本** | **输出文本** |
|---|---|---|
| **翻译** | `translate English to German: That is good.` | `Das ist gut.` |
| **摘要** | `summarize: The quick brown fox...` | `Fox jumps over dog.` |
| **问答** | `question: What is the capital of France? context: Paris is the capital of France.` | `Paris` |
| **分类** | `cola sentence: The dog ate the cat.` | `not acceptable` |
| **自然语言推理** | `nli hypothesis: A man is eating. premise: A man is eating a hot dog.` | `entailment` |
| **去噪** | `The [MASK] brown fox [MASK] over the lazy dog.` | `quick jumps` |

## 3. 模型架构

T5采用标准的**编码器-解码器Transformer**结构：

*   **编码器**：由多层Transformer编码器堆叠而成，负责将输入文本编码为上下文表示。
*   **解码器**：由多层Transformer解码器堆叠而成，负责根据编码器的输出和已生成的文本逐步生成目标文本。

### 3.1 关键特点

*   **共享参数**：编码器和解码器之间共享参数（如词嵌入层）。
*   **相对位置编码**：T5不使用绝对位置编码，而是使用**相对位置编码**，这使得模型能够更好地处理不同长度的序列。
*   **无偏置项**：T5的Transformer层中移除了所有偏置项（bias）。

## 4. 预训练任务

T5的预训练任务是**去噪自编码（Denoising Autoencoder）**，具体称为**"完形填空（Masked Span Prediction）"**：

*   **目标**：随机选择输入文本中15%的token，并将连续的被选中的token替换为一个特殊的`[MASK]`标记（T5中称为`sentinel token`，如`<extra_id_0>`）。模型的目标是预测被遮盖的原始文本片段。
*   **示例**：
    *   原始文本：`Thank you for inviting me to your party.`
    *   输入：`Thank you <extra_id_0> me to your party.`
    *   目标输出：`<extra_id_0> for inviting <extra_id_1>`
*   **创新性**：
    *   与BERT的MLM类似，但T5遮盖的是**连续的文本片段**，并要求模型生成这些片段，这更接近生成任务。
    *   这种预训练方式使得模型在生成长文本片段方面表现出色。

## 5. 微调（Fine-tuning）

T5的微调过程非常简单：只需将特定任务的数据转换为"文本到文本"的格式，然后继续训练模型即可。

*   **多任务微调**：T5可以在多个任务上同时进行微调，通过混合不同任务的数据集来训练一个单一的模型。

## 6. 优缺点

### 6.1 优势

*   **任务通用性**：一套模型解决分类、生成、翻译等所有任务。
*   **简化部署**：无需为不同任务维护多个模型。
*   **迁移学习能力强**：预训练时已接触多任务模式。

### 6.2 局限性

*   **模型规模大**：T5模型通常参数量巨大（从60M到11B），需要大量计算资源进行训练和推理。
*   **生成效率较低**：解码器需自回归生成，比纯编码器（如BERT）慢。
*   **指令依赖性强**：任务前缀设计需谨慎（错误指令导致输出错误）。

## 7. 变体与应用

*   **mT5**：多语言版本的T5，在101种语言上进行预训练。
*   **ByT5**：基于字节（Byte）级别的T5，无需分词器，可以直接处理任何文本。
*   **UL2**：T5的改进版，引入了混合去噪目标，进一步提升了性能。
*   **应用**：机器翻译、文本摘要、问答系统、对话生成、代码生成等。

## 8. T5 vs BERT vs GPT

| 特性 | BERT (编码器) | GPT (解码器) | T5 (编码器-解码器) |
|------|---------------|--------------|--------------------|
| **架构** | 编码器-only | 解码器-only | 编码器-解码器 |
| **预训练目标** | MLM + NSP | 单向语言模型 | 文本到文本统一任务 |
| **注意力** | 双向自注意力 | 掩码自注意力 | 编码器双向 + 解码器掩码 + 交叉注意力 |
| **擅长任务** | 理解类任务 | 生成类任务 | 所有文本到文本任务 |
| **信息流** | 双向 | 单向 | 双向输入，单向输出 |

## 9. 面试回答技巧

*   **强调统一性**：
    > "T5像瑞士军刀，通过文本到文本的框架将NLP任务'归一化'，而BERT/GPT是专用工具。"
*   **举例说明灵活性**：
    > "在客服系统中，T5可同时处理'翻译用户提问'、'生成回复'和'分类问题类型'，而BERT和GPT需分别部署。"
*   **对比模型演进**：
    > "T5的灵感来自GPT的生成能力和BERT的双向理解，但通过编码器-解码器架构实现了更通用的迁移学习。"

## GPT详解

# GPT详解

## 概述

GPT（Generative Pre-trained Transformer）是由OpenAI开发的一系列基于Transformer解码器架构的预训练语言模型。其核心思想是**自回归（Autoregressive）生成**，即模型通过预测下一个词的方式，逐步生成连贯的文本序列。GPT系列模型在文本生成、对话系统等任务上展现出强大的能力。

## 1. GPT的核心思想与架构

### 1.1 核心思想：自回归生成

自回归是一种**序列生成方法**，模型在生成当前词时，**仅依赖已生成的部分序列**（即左侧上下文），并逐步预测下一个词。其数学形式为：

```math
P(x_t | x_{<t}) = \text{Softmax}(f(x_1, x_2, ..., x_{t-1}))
```

*   **生成过程**：给定输入前缀序列 $(x_1, x_2, ..., x_{t-1})$，模型预测第 $(t)$ 个词 $(x_t)$，再将 $(x_t)$ 加入输入序列，迭代生成后续词。
*   **任务适配性**：人类写作或说话通常是从左到右逐步进行的，自回归完美匹配这一过程，确保生成内容的连贯性。

### 1.2 架构特点：单向Transformer解码器

GPT模型采用**Transformer的解码器（Decoder-only）架构**，其关键在于：

*   **掩码自注意力（Masked Self-Attention）**：这是实现自回归生成的核心机制。每个词在计算注意力时，**仅允许关注其左侧（包括自身）的词**，而未来词（右侧）的信息被掩码遮盖（设置为负无穷），从而防止信息泄露。

    ```math
    \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V
    ```
    其中 $(M)$ 是上三角掩码矩阵（未来位置为 $-\infty$）。

*   **堆叠解码器层**：由多层Transformer解码器堆叠而成，每层包含掩码自注意力、交叉注意力（在GPT中通常不使用，或仅在微调时引入外部信息）和前馈网络。

### 1.3 训练与推理一致性

*   **训练目标**：GPT 通过**最大似然估计（MLE）** 优化序列概率 $P(x_1, ..., x_T)$，分解为链式条件概率：

    ```math
P(x_1, ..., x_T) = \prod_{t=1}^T P(x_t | x_{<t})
    ```
    自回归生成与训练目标完全对齐，有助于避免**曝光偏差（Exposure Bias）**。

*   **推理过程**：模型逐词生成，将上一步的预测作为下一步的输入，迭代进行。

## 2. 预训练任务与目标

GPT的预训练目标是**单向语言模型（Autoregressive Language Modeling）**：

*   **目标**：给定输入前缀，预测序列中的下一个词。
*   **数据**：在大规模无标注文本语料上进行训练（如BooksCorpus、Common Crawl）。
*   **目的**：学习语言的统计规律、语法结构、语义关系以及世界知识，使其能够生成流畅、连贯且有意义的文本。

## 3. GPT系列模型演进

| 模型 | 发布时间 | 参数量 | 核心特点 |
|------|----------|--------|----------|
| **GPT-1** | 2018 | 1.17亿 | 首次提出基于Transformer解码器的预训练语言模型，采用两阶段训练（预训练+微调） |
| **GPT-2** | 2019 | 1.5亿 | 强调"零样本学习"（Zero-shot Learning），通过扩大模型规模和训练数据，提升泛化能力，无需微调即可执行多种任务 |
| **GPT-3** | 2020 | 1750亿 | 引入"语境学习"（In-context Learning），通过少量示例（Few-shot）或指令（Zero-shot）即可完成任务，无需梯度更新 |
| **GPT-3.5** | 2022 | - | 基于GPT-3的改进，通过**人类反馈强化学习（RLHF）**进行微调，显著提升对话能力和指令遵循能力（如ChatGPT） |
| **GPT-4** | 2023 | - | 多模态能力（接受图像输入），更强的推理和指令遵循能力，更长的上下文窗口 |
| **LLaMA** | 2023 | 7B-65B | Meta发布的开源大模型，采用类似GPT的架构，在高质量数据上训练，性能优异 |

## 4. 自回归的优缺点

| **优点** | **缺点** |
|----------|----------|
| 生成内容连贯，符合语言习惯 | 生成速度慢（需逐词迭代，无法并行） |
| 易于控制生成方向（通过Prompt工程） | 无法利用右侧上下文（如BERT的双向信息） |
| 训练简单（直接优化似然） | 长文本生成可能偏离主题（误差累积） |
| 适合开放式文本生成 | 存在曝光偏差（Exposure Bias）问题 |

## 5. 曝光偏差（Exposure Bias）及其解决方法

### 5.1 什么是曝光偏差？

**曝光偏差**是自回归生成模型在训练与推理阶段**数据分布不一致**导致的性能下降问题。具体表现为：

*   **训练阶段**：模型基于**真实的前缀词**（Ground Truth）预测下一个词（Teacher Forcing）。
*   **推理阶段**：模型依赖**自己生成的前缀词**（可能包含错误）预测下一个词。

**关键矛盾**：模型在训练时从未见过自身生成的错误上下文，导致推理时错误累积（如生成重复或无意义文本）。

### 5.2 解决方法

1.  **Scheduled Sampling（计划采样）**：在训练中逐步从完全使用真实前缀过渡到混合使用模型生成的前缀。
2.  **Beam Search with Penalties（带惩罚的束搜索）**：在生成时对重复词或N-gram添加惩罚项，减少错误累积的影响。
3.  **强化学习（RL）优化**：使用策略梯度（如REINFORCE、PPO），以生成序列的整体质量（如BLEU、ROUGE）为奖励信号，直接优化最终目标。GPT-3和ChatGPT通过RLHF（人类反馈强化学习）显著缓解了曝光偏差。
4.  **对抗训练（GAN-like Methods）**：引入判别器区分模型生成序列与真实序列，迫使生成器输出更真实的中间状态。

## 6. 强化学习在GPT中的应用：RLHF

**人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）**是GPT系列模型（尤其是GPT-3.5及后续版本）实现与人类偏好对齐的关键技术。

### 6.1 RLHF流程

1.  **监督微调（Supervised Fine-Tuning, SFT）**：用人类标注的高质量对话数据微调预训练模型，使其初步具备对话能力。
2.  **奖励模型训练（Reward Model Training）**：收集模型生成的多个回复，由人类对这些回复进行排序或评分。然后训练一个奖励模型，使其能够预测人类的偏好。
3.  **强化学习微调（Reinforcement Learning Fine-Tuning）**：使用奖励模型作为奖励函数，通过PPO（Proximal Policy Optimization）等强化学习算法微调SFT模型，使其生成更高奖励的回复。

### 6.2 作用

*   **对齐人类偏好**：使模型生成的内容更符合人类的价值观、常识和指令。
*   **减少有害输出**：有效降低模型生成偏见、歧视或不安全内容的风险。
*   **提升对话质量**：使模型在多轮对话中表现更连贯、自然和有用。

## 7. 典型应用场景

*   **文本生成**：故事创作、诗歌生成、新闻稿撰写、邮件回复。
*   **对话系统**：智能客服、虚拟助手、开放域聊天机器人（如ChatGPT）。
*   **代码生成与补全**：根据自然语言描述生成代码，或补全现有代码。
*   **创意写作**：提供灵感、扩展思路。
*   **内容摘要**：生成文章、文档的摘要。

## 8. GPT vs BERT vs T5

| 特性 | BERT (编码器) | GPT (解码器) | T5 (编码器-解码器) |
|------|---------------|--------------|--------------------|
| **架构** | 编码器-only | 解码器-only | 编码器-解码器 |
| **预训练目标** | MLM + NSP | 单向语言模型 | 文本到文本统一任务 |
| **注意力** | 双向自注意力 | 掩码自注意力 | 编码器双向 + 解码器掩码 + 交叉注意力 |
| **擅长任务** | 理解类任务 | 生成类任务 | 所有文本到文本任务 |
| **信息流** | 双向 | 单向 | 双向输入，单向输出 |

## 9. 面试回答技巧

*   **核心区别**：
    > "GPT是纯粹的生成式模型，其核心是单向自回归和掩码注意力，使其天然适合文本生成任务."
*   **强调RLHF**：
    > "GPT系列模型通过RLHF实现了与人类偏好的对齐，这是其在对话领域取得突破的关键."
*   **举例说明**：
    > "在生成一篇新闻报道时，GPT会根据已有的标题和开头，逐步续写出连贯的全文，而不会像BERT那样'偷看'后面的内容."


## BERT详解

# BERT详解

## 概述

BERT（Bidirectional Encoder Representations from Transformers）是Google在2018年提出的预训练语言模型，它通过**双向Transformer编码器**和**两个创新的预训练任务**，彻底改变了自然语言处理（NLP）领域，成为许多下游任务的基石。

## 1. BERT的核心思想

BERT的核心思想是：

1.  **深度双向性**：通过Transformer的自注意力机制，模型在处理每个词时，能够同时考虑其左侧和右侧的上下文信息。
2.  **预训练-微调范式**：
    *   **预训练（Pre-training）**：在大规模无标注文本语料上（如维基百科、BookCorpus）学习通用的语言表示。
    *   **微调（Fine-tuning）**：针对特定下游任务，在少量标注数据上对预训练模型进行微调。

## 2. 模型架构

BERT的架构是**多层Transformer编码器**。

*   **BERT-base**：12层Transformer编码器，768隐藏单元，12个注意力头，1.1亿参数。
*   **BERT-large**：24层Transformer编码器，1024隐藏单元，16个注意力头，3.4亿参数。

### 输入表示

BERT的输入是**词嵌入、位置嵌入和段嵌入**的加和：

```math
E_{token} + E_{segment} + E_{position}
```

*   **`[CLS]`**：特殊分类标记，用于分类任务。
*   **`[SEP]`**：特殊分隔标记，用于分隔句子对。

## 3. 预训练任务

BERT通过以下两个无监督任务进行预训练：

### 3.1 掩码语言模型（Masked Language Model, MLM）

*   **目标**：预测被遮盖的词。
*   **方法**：
    *   随机选择输入序列中15%的词。
    *   对这些词：
        *   80%的概率替换为`[MASK]`标记。
        *   10%的概率替换为随机词。
        *   10%的概率保持原词不变。
    *   模型的目标是预测出被替换的原始词。
*   **创新性**：
    *   解决了传统语言模型（如GPT）只能单向学习上下文的问题。
    *   迫使模型学习深度的**双向上下文**表示。
    *   **一词多义学习**：同一词在不同上下文的预测目标不同，迫使模型生成动态词向量。
        *   *示例*：
            *   `"The [MASK] is flowing"` → 预测`"river"`
            *   `"The [MASK] is bankrupt"` → 预测`"bank"`

### 3.2 下一句预测（Next Sentence Prediction, NSP）

*   **目标**：判断两个句子是否在原始文本中连续。
*   **方法**：
    *   输入是两个句子A和B，用`[SEP]`分隔。
    *   50%的概率，B是A的下一句（`IsNext`）。
    *   50%的概率，B是随机抽取的句子（`NotNext`）。
    *   模型通过`[CLS]`标记的最终隐藏状态进行二分类预测。
*   **创新性**：
    *   帮助模型理解句子间关系，对问答和自然语言推理等任务至关重要。
    *   **增强`[CLS]`标记的语义表示**：`[CLS]`标记的向量用于NSP分类，使其成为全局语义的聚合表示，便于下游分类任务微调。

### 3.3 联合训练的必要性

| 任务 | 解决的问题 | BERT的改进 | 单独训练的缺陷 |
|------|------------|------------|----------------|
| **MLM** | 词级语义理解 | 深度双向上下文编码 | 无法建模句子间关系 |
| **NSP** | 句级逻辑关系 | 增强段落/文档级推理能力 | 词义理解不精细 |

#### 协同效应

*   **MLM**为**NSP**提供高质量的词汇语义（如代词指代消解）。
*   **NSP**为**MLM**补充句子边界信息（如判断`"it"`指代前句还是后句的主语）。

## 4. BERT的池化层（Pooling Layer）

在BERT模型中，"池化层"通常指的是将整个输入序列的**上下文表示（contextualized representation）**聚合为一个**固定维度向量**的过程，以便用于下游任务，特别是**分类任务**。BERT本身并没有一个独立的"池化层"模块，而是通过特定的策略来提取序列级别的表示。

### 4.1 `[CLS]` Token 策略（默认且最常用）

*   **原理**：在BERT的输入序列前添加一个特殊的分类标记 `[CLS]`。在预训练阶段，NSP（下一句预测）任务就是通过这个 `[CLS]` token 的最终隐藏状态来完成的。在微调阶段，对于分类任务，通常也直接使用 `[CLS]` token 对应的最终隐藏状态作为整个序列的聚合表示。
*   **优点**：
    *   简单直接，无需额外计算。
    *   `[CLS]` token 在预训练中被训练来聚合序列信息。
*   **缺点**：
    *   `[CLS]` token 的表示能力可能有限，因为它只代表一个单一的token，可能无法完全捕捉整个序列的复杂语义。
    *   有研究表明，`[CLS]` token 并不总是最佳的序列表示。

### 4.2 平均池化（Mean Pooling）

*   **原理**：对所有token（或所有非填充token）的最终隐藏状态进行平均，作为整个序列的表示。
*   **优点**：
    *   考虑了序列中所有token的信息，可能比单一的 `[CLS]` token 更具代表性。
    *   在某些任务（如语义相似度）上表现更好。
*   **缺点**：
    *   需要额外的计算（平均操作）。
    *   对于填充（padding）的token需要特殊处理，避免其影响平均值。

### 4.3 其他"池化"变体

*   **最大池化（Max Pooling）**：取所有token隐藏状态在每个维度上的最大值。
*   **加权池化**：根据某些标准（如TF-IDF、注意力权重）对token进行加权平均。

### 4.4 为什么BERT需要"池化"？

*   **维度统一**：下游分类任务通常需要一个固定维度的输入向量。
*   **序列表示**：将变长的序列信息压缩成一个定长的向量，作为整个序列的语义表示。
*   **任务适配**：为不同的NLP任务提供合适的输入格式。

## 5. BERT可实现的任务

### 5.1 任务类型

| 任务类型 | 典型应用 | 实现方式 |
|----------|----------|----------|
| **文本分类** | 情感分析、垃圾邮件检测 | 使用`[CLS]`标记的输出向量接分类层 |
| **序列标注** | 命名实体识别（NER）、词性标注 | 对每个词的隐藏层输出接分类层 |
| **句子对分类** | 文本相似度、自然语言推理（NLI） | 拼接两个句子（`[SEP]`分隔），用`[CLS]`输出分类 |
| **问答任务** | SQuAD阅读理解 | 用两个线性层分别预测答案的起止位置 |
| **文本生成** | 受限生成（如填空补全） | 结合MLM任务（需特殊设计，非原生强项） |

### 5.2 NER任务实现详解

#### 输入表示

```python
tokens = ["[CLS]", "John", "works", "at", "Google", "[SEP]"]
```

#### 模型结构

-   **BERT编码器**：输出每个词的上下文向量（$H_1, H_2, ..., H_n$）
-   **NER分类头**：在BERT顶部添加线性层+Softmax

$$P(y_i | x_i) = \text{Softmax}(W H_i + b)$$

标签集示例：`{"O", "B-PER", "I-PER", "B-ORG", "I-ORG", ...}`（BIOES格式）

### 5.3 其他任务实现示例

#### 文本分类（情感分析）

```python
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
inputs = tokenizer("I love this movie!", return_tensors="pt")
outputs = model(**inputs)  # logits.shape = [1, 2]
```

#### 问答任务（SQuAD）

```python
from transformers import BertForQuestionAnswering

model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")
inputs = tokenizer("What is BERT?", "BERT is a language model.", return_tensors="pt")
outputs = model(**inputs)  # 输出start_positions和end_positions
```

#### 句子对分类（文本相似度）

```python
inputs = tokenizer("Sentence A", "Sentence B", return_tensors="pt", padding=True)
outputs = model(**inputs)
pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS]向量
```

## 6. BERT的局限性

### 6.1 主要限制

1.  **生成能力弱**：无法直接用于文本生成（需结合Seq2Seq结构如BART）
2.  **长文本处理**：输入长度限制（通常512 Token），需截断或分段处理
3.  **计算资源**：参数量大，实时应用需蒸馏（DistilBERT）或量化

### 6.2 后续改进

*   **RoBERTa**：移除NSP任务，使用更大规模数据和更长时间训练，动态掩码。
*   **ALBERT**：将NSP改为**句子顺序预测（SOP）**，更关注句子间的连贯性。
*   **DistilBERT**：通过知识蒸馏减少参数量，提高推理速度。

## 7. BERT vs GPT vs T5的本质区别

### 7.1 信息流方向

-   **BERT**：双向编码，通过MLM同时利用左右上下文，适合理解任务
-   **GPT**：单向解码，仅使用左侧上下文，适合生成任务
-   **T5**：完整Seq2Seq，编码器处理输入，解码器生成输出

### 7.2 注意力机制

| 模型 | 注意力类型 | 关键限制 |
|------|------------|----------|
| BERT | 全连接Self-Attention | 编码阶段无掩码，可看到全部输入 |
| GPT | 掩码Self-Attention | 解码时只能看到左侧词（防止信息泄露） |
| T5 | 编码器全连接+解码器掩码+交叉注意力 | 解码器可访问编码器的全部隐藏状态 |

### 7.3 任务形式

-   **BERT**：输入`[CLS] A [SEP] B [SEP]`，输出分类标签或序列标签
-   **GPT**：输入`"Once upon a time"`，输出自回归生成`", there was a dragon."`
-   **T5**：输入`"translate English to German: The cat sits on the mat."`，输出`"Die Katze sitzt auf der Matte."`

## 8. 使用场景选择

| 模型 | 典型应用场景 | 示例任务 |
|------|--------------|----------|
| **BERT** | 文本分类、命名实体识别、问答系统 | 输入影评→输出"正面/负面" |
| **GPT** | 文本生成、对话系统、代码补全 | 输入"Once upon a"→生成"time, there was a princess." |
| **T5** | 机器翻译、文本摘要、多任务统一框架 | 输入"summarize: long article"→输出摘要 |

## 9. 面试回答技巧

-   **架构对比**：
    > "BERT像闭卷考试，综合所有信息后答题；GPT像即兴演讲，只能根据已说的内容继续；T5则像翻译官，先听完整句再翻译。"

-   **选择模型的标准**：
    > "如果需要分类或标注，选BERT；生成文本用GPT；多任务或Seq2Seq需求优先T5。"

-   **强调微调灵活性**：
    > "BERT像乐高积木，通过添加不同任务头（如分类、标注）快速适配下游任务。"

## 10. RoBERTa 相比 BERT 的核心改进

RoBERTa（**Robustly Optimized BERT Approach**）通过对 BERT 的预训练策略和模型设计进行系统性优化，显著提升了性能。以下是其核心改进点：

### 10.1 训练数据与规模的扩展

*   **更大规模的数据**：新增 **Common Crawl** 数据（160GB 原始文本，过滤后约 30GB），总数据量是 BERT 的 **10倍以上**。
*   **更长的训练**：训练步数从 BERT 的 1M 步增加到 **500K~2M 步**（动态调整批次大小）。

### 10.2 动态掩码（Dynamic Masking）

*   **BERT 的静态掩码**：在数据预处理时对每个样本固定遮盖部分词（训练全程不变），导致模型可能记住特定位置的遮盖模式。
*   **RoBERTa 的动态掩码**：**每次输入模型时随机生成新的掩码模式**，避免过拟合。

### 10.3 移除 NSP（Next Sentence Prediction）任务

*   **RoBERTa 的发现**：NSP 任务对下游任务帮助有限，甚至可能损害性能（因负例过于简单）。
*   **改为连续文本块**：输入为从文档中连续抽取的多个句子（无需 NSP 标签），最大长度 512 Token。

### 10.4 更大的批次与更优化的超参数

| **参数** | **BERT** | **RoBERTa** |
|----------|----------|-------------|
| 批次大小（Batch Size） | 256 | **2K~8K** |
| 学习率（Learning Rate） | 1e-4 | **3e-4~6e-4** |
| 训练步数（Steps） | 1M | **300K~500K**（更大批次等效更多数据） |

*   **效果**：大批次训练提升模型收敛速度和稳定性（需配合学习率调整）。

### 10.5 字节级 BPE（Byte-Level BPE）分词

*   **BERT**：使用字符级 WordPiece，对罕见词拆分不够高效。
*   **RoBERTa**：改用 **Byte-Pair Encoding (BPE)**，基于字节而非字符，更好处理多语言和生僻词。

### 10.6 性能提升对比

| **任务** | **BERT (Base)** | **RoBERTa (Base)** | **提升幅度** |
|----------|-----------------|--------------------|------------|
| GLUE 平均得分 | 78.3 | **87.6** | +9.3 |
| SQuAD 2.0 F1 | 76.3 | **83.7** | +7.4 |

### 10.7 RoBERTa 的局限性

*   **计算资源需求高**：训练需数千 GPU 小时。
*   **仍为单向优化**：未引入类似 XLNet 的排列语言模型。

### 10.8 面试回答技巧

*   **强调方法论**：
    > "RoBERTa 不是架构创新，而是通过数据、训练策略和超参的极致优化，释放 BERT 的潜力。"
*   **对比实验结论**：
    > "移除 NSP 并扩大数据后，RoBERTa 在 GLUE 上超越 BERT 达 9 分，证明预训练质量比任务多样性更重要。"
*   **引申到后续模型**：
    > "RoBERTa 的优化思想影响了 ALBERT 和 DeBERTa，但后者通过参数共享和分解进一步改进效率。"

