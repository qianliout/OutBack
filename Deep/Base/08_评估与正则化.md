# 08_评估与正则化

## 过拟合详解

# 过拟合详解

过拟合（Overfitting）是机器学习和深度学习中一个非常常见且关键的问题。它指的是模型在训练数据上表现良好，但在**未见过的新数据（测试数据或实际应用数据）上表现不佳**的现象。简单来说，模型"记住了"训练数据中的噪声和特有模式，而不是学习到数据的普遍规律。

## 1. 核心概念

*   **过拟合**：模型复杂度过高，过度学习了训练数据中的细节和噪声，导致泛化能力下降。
*   **欠拟合（Underfitting）**：模型复杂度过低，未能充分学习训练数据中的模式，导致在训练集和测试集上表现均不佳。
*   **泛化能力（Generalization Ability）**：模型对未见过数据的预测能力。

## 2. 过拟合的迹象

*   **训练损失持续下降，但验证损失开始上升或停滞**。
*   **训练准确率很高，但验证准确率显著低于训练准确率**。
*   模型在训练集上表现完美，但在测试集上错误百出。
*   模型对训练数据中的微小扰动过于敏感。

## 3. 导致过拟合的原因

1.  **模型复杂度过高**：模型参数过多，层数过深，容量过大，能够"记住"训练数据中的每一个样本。
2.  **训练数据量不足**：数据量太少，不足以代表真实数据的分布，模型容易学习到噪声。
3.  **训练数据存在噪声**：标签错误、异常值等，模型会试图拟合这些噪声。
4.  **训练时间过长**：模型在训练后期开始过度拟合训练数据。
5.  **特征过多或不相关**：引入了大量与任务无关的特征，增加了模型学习的难度和过拟合的风险。

## 4. 解决过拟合的方法（正则化技术）

### 4.1 数据层面

1.  **增加训练数据**：最直接有效的方法，但成本高。
2.  **数据增强（Data Augmentation）**：通过对现有数据进行变换（如图像旋转、文本同义词替换）生成新数据。
3.  **数据清洗**：去除噪声、异常值和错误标签。

### 4.2 模型层面

1.  **简化模型**：减少模型层数、神经元数量或参数量。
2.  **特征选择/降维**：选择最相关的特征，或使用PCA等方法降维。

### 4.3 训练策略层面

1.  **正则化（Regularization）**：
    *   **L1/L2正则化（权重衰减）**：在损失函数中添加惩罚项，限制模型权重的大小，鼓励模型学习更简单的权重。
        ```math
        \text{Loss} = \text{OriginalLoss} + \lambda \sum |w| \quad (\text{L1})
        \text{Loss} = \text{OriginalLoss} + \lambda \sum w^2 \quad (\text{L2})
        ```
    *   **Dropout**：在训练过程中随机"关闭"一部分神经元，迫使模型学习更鲁棒的特征，防止神经元间的共适应。
        *   **应用**：通常在全连接层或Transformer的FFN层之后。
    *   **Batch Normalization / Layer Normalization**：通过归一化稳定训练，减少对初始化和学习率的敏感性，间接起到正则化作用。
2.  **早停（Early Stopping）**：
    *   监控模型在验证集上的性能，当验证损失不再下降或开始上升时，停止训练。
    *   **原理**：在模型开始过拟合之前停止训练，找到最佳泛化点。
3.  **交叉验证（Cross-Validation）**：
    *   将数据集分成多个子集，轮流作为训练集和验证集，评估模型性能。
    *   **作用**：更可靠地评估模型泛化能力，帮助选择最佳超参数。

## 5. PyTorch中的正则化示例

### 5.1 L2正则化（权重衰减）

```python
import torch.optim as optim

# 在优化器中设置 weight_decay 参数
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) # L2正则化
```

### 5.2 Dropout

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(100, 50)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.5) # 随机丢弃50%的神经元
        self.fc2 = nn.Linear(50, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x) # 在训练时应用Dropout
        x = self.fc2(x)
        return x

# 注意：Dropout在eval模式下会自动关闭
model.train() # 训练模式
# model.eval()  # 评估模式
```

### 5.3 早停

```python
# 伪代码
best_val_loss = float('inf')
patience = 10 # 容忍多少个epoch验证损失不下降
counter = 0

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader)
    val_loss = evaluate_on_validation_set(model, val_loader)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth') # 保存最佳模型
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

## 6. 总结

过拟合是机器学习模型训练中必须面对的挑战。通过**增加数据、简化模型、引入正则化技术（L1/L2、Dropout、BN/LN）和采用早停策略**，可以有效地缓解过拟合，提高模型的泛化能力，使其在真实世界数据上表现更佳。在实践中，通常会结合多种方法来对抗过拟合。


## 归一化详解

# 归一化（Normalization）详解

归一化（Normalization）是深度学习中一种常用的数据预处理技术，旨在将输入数据或中间层激活值调整到统一的尺度范围，从而**加速模型训练、提高模型稳定性并改善泛化能力**。

## 1. 为什么需要归一化？

1.  **加速收敛**：
    *   当输入特征的尺度差异很大时，损失函数的等高线会非常扁平，导致梯度下降路径呈"Z"字形，收敛缓慢。
    *   归一化后，等高线更接近圆形，梯度方向更接近最优方向，加速收敛。
2.  **防止梯度消失/爆炸**：
    *   在深度网络中，激活值过大或过小都可能导致梯度在反向传播时消失或爆炸。
    *   归一化将激活值限制在稳定范围内，有助于梯度传播。
3.  **提高模型稳定性**：
    *   减少内部协变量偏移（Internal Covariate Shift）：指训练过程中，深层网络输入分布的变化。归一化稳定了每层输入的分布，使得后续层更容易学习。
4.  **改善泛化能力**：
    *   减少对初始化参数的敏感性，降低过拟合风险。

## 2. 常见的归一化方法

### 2.1 数据预处理归一化

*   **Min-Max Normalization（Min-Max归一化）**：
    *   将数据缩放到 `[0, 1]` 或 `[-1, 1]` 之间。
    ```math
    x' = \frac{x - x_{min}}{x_{max} - x_{min}}
    ```
    *   **适用**：数据范围已知且稳定。
*   **Z-score Normalization（标准化）**：
    *   将数据转换为均值为0，标准差为1的分布。
    ```math
    x' = \frac{x - \mu}{\sigma}
    ```
    *   **适用**：数据分布近似正态分布，或数据范围未知。

### 2.2 层内归一化（In-layer Normalization）

这些方法在神经网络的隐藏层中应用，对激活值进行归一化。

#### 2.2.1 Batch Normalization (BN)

*   **原理**：对**一个mini-batch**的数据在**特征维度**上进行归一化。
    ```math
    \mu_B = \frac{1}{m} \sum_{i=1}^m x_i, \quad \sigma_B^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2
    ```
    ```math
    \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y_i = \gamma \hat{x}_i + \beta
    ```
    *   $m$：mini-batch大小。
    *   $\gamma, \beta$：可学习的缩放和偏移参数，允许网络恢复原始特征分布。
*   **适用**：CV任务，大Batch Size。
*   **缺点**：
    *   依赖Batch Size：Batch Size过小（如1、2）时，统计量不稳定。
    *   不适用于RNN：RNN序列长度可变，且每个时间步的统计量不同。

#### 2.2.2 Layer Normalization (LN)

*   **原理**：对**单个样本**在**特征维度**上进行归一化。
    ```math
    \mu = \frac{1}{H} \sum_{i=1}^H x_i, \quad \sigma^2 = \frac{1}{H} \sum_{i=1}^H (x_i - \mu)^2
    ```
    ```math
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y_i = \gamma \hat{x}_i + \beta
    ```
    *   $H$：特征维度大小。
*   **适用**：NLP任务（如Transformer），RNN，小Batch Size。
*   **优点**：
    *   不依赖Batch Size：每个样本独立计算统计量。
    *   适用于RNN和Transformer。
*   **缺点**：
    *   对不同特征的尺度变化不敏感。

#### 2.2.3 Instance Normalization (IN)

*   **原理**：对**单个样本**在**每个通道**上进行归一化。
*   **适用**：风格迁移（Style Transfer）。

#### 2.2.4 Group Normalization (GN)

*   **原理**：对**单个样本**在**通道分组**后进行归一化。
*   **适用**：Batch Size很小但通道数很多的情况。

## 3. 归一化层的选择

| **归一化类型** | **适用场景** | **优点** | **缺点** |
|---|---|---|---|
| **Batch Norm** | CV任务，大Batch Size | 效果好，加速收敛 | 依赖Batch Size，不适用于RNN |
| **Layer Norm** | NLP任务（Transformer、RNN），小Batch Size | 不依赖Batch Size，适用于序列模型 | |
| **Instance Norm** | 风格迁移 | 保持实例独立性 | |
| **Group Norm** | 小Batch Size，大通道数 | 不依赖Batch Size，介于BN和LN之间 | |

## 4. PyTorch实现

```python
import torch
import torch.nn as nn

# Batch Normalization
bn = nn.BatchNorm1d(num_features=100) # 对100维特征进行BN

# Layer Normalization
ln = nn.LayerNorm(normalized_shape=512) # 对512维特征进行LN (Transformer中常用)

# Instance Normalization
# in_ = nn.InstanceNorm1d(num_features=100)

# Group Normalization
# gn = nn.GroupNorm(num_groups=32, num_channels=256)

# 示例：在Transformer中使用LayerNorm
class TransformerBlock(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        # ... 其他层
    
    def forward(self, x):
        # ... attention
        x = self.norm1(x + residual) # 残差连接后进行LayerNorm
        # ... feedforward
        x = self.norm2(x + residual)
        return x
```

## 5. 总结

归一化是深度学习中不可或缺的技术，它通过调整数据尺度来优化训练过程。选择合适的归一化方法取决于模型架构和任务特点：**Batch Normalization在CV领域广泛应用，而Layer Normalization则是NLP领域（尤其是Transformer）的首选**。理解不同归一化方法的原理和适用场景，对于构建高效稳定的深度学习模型至关重要。

## 模型评估详解

# 模型评估详解

## 1. 分类任务评估

### 1.1 基础指标

#### 准确率（Accuracy）
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

- **优点**：直观易懂
- **缺点**：在类别不平衡时可能误导

#### 精确率（Precision）
$$Precision = \frac{TP}{TP + FP}$$

- 预测为正例中实际为正例的比例
- 关注预测的准确性

#### 召回率（Recall）
$$Recall = \frac{TP}{TP + FN}$$

- 实际正例中被正确预测的比例
- 关注模型的覆盖能力

#### F1分数
$$F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}$$

- 精确率和召回率的调和平均
- 平衡精确率和召回率

### 1.2 多分类评估

#### 宏平均（Macro-average）
$$Macro-P = \frac{1}{n} \sum_{i=1}^{n} P_i$$
$$Macro-R = \frac{1}{n} \sum_{i=1}^{n} R_i$$
$$Macro-F1 = \frac{2 \times Macro-P \times Macro-R}{Macro-P + Macro-R}$$

#### 微平均（Micro-average）
$$Micro-P = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} TP_i + \sum_{i=1}^{n} FP_i}$$
$$Micro-R = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} TP_i + \sum_{i=1}^{n} FN_i}$$

### 1.3 实现代码

```python
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix

def evaluate_classification(y_true, y_pred, y_proba=None):
    """分类任务评估"""
    results = {}
    
    # 基础指标
    results['accuracy'] = accuracy_score(y_true, y_pred)
    results['precision'] = precision_score(y_true, y_pred, average='weighted')
    results['recall'] = recall_score(y_true, y_pred, average='weighted')
    results['f1'] = f1_score(y_true, y_pred, average='weighted')
    
    # 详细报告
    results['classification_report'] = classification_report(y_true, y_pred)
    results['confusion_matrix'] = confusion_matrix(y_true, y_pred)
    
    # ROC-AUC（如果有概率预测）
    if y_proba is not None:
        from sklearn.metrics import roc_auc_score
        results['roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr')
    
    return results

# 使用示例
y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 0, 1]
y_proba = np.array([[0.8, 0.1, 0.1], [0.1, 0.2, 0.7], [0.2, 0.6, 0.2],
                    [0.9, 0.05, 0.05], [0.7, 0.2, 0.1], [0.1, 0.8, 0.1]])

results = evaluate_classification(y_true, y_pred, y_proba)
print(f"Accuracy: {results['accuracy']:.4f}")
print(f"F1 Score: {results['f1']:.4f}")
```

## 2. 回归任务评估

### 2.1 基础指标

#### 均方误差（MSE）
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

#### 均方根误差（RMSE）
$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

#### 平均绝对误差（MAE）
$$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

#### 决定系数（R²）
$$R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

### 2.2 实现代码

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_regression(y_true, y_pred):
    """回归任务评估"""
    results = {}
    
    results['mse'] = mean_squared_error(y_true, y_pred)
    results['rmse'] = np.sqrt(results['mse'])
    results['mae'] = mean_absolute_error(y_true, y_pred)
    results['r2'] = r2_score(y_true, y_pred)
    
    # 平均绝对百分比误差
    results['mape'] = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    return results

# 使用示例
y_true = np.array([1, 2, 3, 4, 5])
y_pred = np.array([1.1, 1.9, 3.1, 3.9, 5.1])

results = evaluate_regression(y_true, y_pred)
print(f"RMSE: {results['rmse']:.4f}")
print(f"R²: {results['r2']:.4f}")
```

## 3. NLP任务评估

### 3.1 文本生成评估

#### BLEU分数
```python
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def compute_bleu(references, candidates):
    """计算BLEU分数"""
    # 句子级BLEU
    sentence_scores = []
    for ref, cand in zip(references, candidates):
        ref_tokens = [ref.split()]  # 单个参考翻译
        cand_tokens = cand.split()
        score = sentence_bleu(ref_tokens, cand_tokens)
        sentence_scores.append(score)
    
    # 语料库级BLEU
    ref_tokens = [[ref.split()] for ref in references]
    cand_tokens = [cand.split() for cand in candidates]
    corpus_score = corpus_bleu(ref_tokens, cand_tokens)
    
    return {
        'sentence_bleu': np.mean(sentence_scores),
        'corpus_bleu': corpus_score
    }
```

#### ROUGE分数
```python
from rouge_score import rouge_scorer

def compute_rouge(references, candidates):
    """计算ROUGE分数"""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])
    
    rouge1_scores = []
    rouge2_scores = []
    rougeL_scores = []
    
    for ref, cand in zip(references, candidates):
        scores = scorer.score(ref, cand)
        rouge1_scores.append(scores['rouge1'].fmeasure)
        rouge2_scores.append(scores['rouge2'].fmeasure)
        rougeL_scores.append(scores['rougeL'].fmeasure)
    
    return {
        'rouge1': np.mean(rouge1_scores),
        'rouge2': np.mean(rouge2_scores),
        'rougeL': np.mean(rougeL_scores)
    }
```

### 3.2 命名实体识别评估

```python
def evaluate_ner(y_true, y_pred):
    """NER任务评估"""
    from seqeval.metrics import classification_report, f1_score
    
    # 计算F1分数
    f1 = f1_score(y_true, y_pred)
    
    # 详细报告
    report = classification_report(y_true, y_pred)
    
    return {
        'f1': f1,
        'report': report
    }
```

## 4. 交叉验证

### 4.1 K折交叉验证

```python
from sklearn.model_selection import KFold, cross_val_score

def k_fold_cross_validation(model, X, y, k=5, scoring='accuracy'):
    """K折交叉验证"""
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    
    # 交叉验证分数
    scores = cross_val_score(model, X, y, cv=kf, scoring=scoring)
    
    return {
        'scores': scores,
        'mean': scores.mean(),
        'std': scores.std(),
        'cv_results': f"{scores.mean():.4f} (+/- {scores.std() * 2:.4f})"
    }

# 使用示例
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
cv_results = k_fold_cross_validation(model, X, y, k=5)
print(f"Cross-validation score: {cv_results['cv_results']}")
```

### 4.2 分层K折交叉验证

```python
from sklearn.model_selection import StratifiedKFold

def stratified_k_fold_cv(model, X, y, k=5):
    """分层K折交叉验证"""
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    
    scores = []
    for train_idx, val_idx in skf.split(X, y):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        score = accuracy_score(y_val, y_pred)
        scores.append(score)
    
    return {
        'scores': scores,
        'mean': np.mean(scores),
        'std': np.std(scores)
    }
```

## 5. 模型比较

### 5.1 统计显著性检验

```python
from scipy import stats

def compare_models(scores_model1, scores_model2, alpha=0.05):
    """比较两个模型的性能"""
    # t检验
    t_stat, p_value = stats.ttest_rel(scores_model1, scores_model2)
    
    # Wilcoxon符号秩检验
    w_stat, w_p_value = stats.wilcoxon(scores_model1, scores_model2)
    
    return {
        't_test': {
            'statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < alpha
        },
        'wilcoxon_test': {
            'statistic': w_stat,
            'p_value': w_p_value,
            'significant': w_p_value < alpha
        }
    }
```

### 5.2 模型选择

```python
def model_selection(models, X, y, cv=5, scoring='accuracy'):
    """模型选择"""
    results = {}
    
    for name, model in models.items():
        cv_scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
        results[name] = {
            'mean': cv_scores.mean(),
            'std': cv_scores.std(),
            'scores': cv_scores
        }
    
    # 找到最佳模型
    best_model = max(results.keys(), key=lambda x: results[x]['mean'])
    
    return results, best_model

# 使用示例
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(),
    'LogisticRegression': LogisticRegression()
}

results, best_model = model_selection(models, X, y)
print(f"Best model: {best_model}")
```

## 6. 过拟合检测

### 6.1 学习曲线

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

def plot_learning_curves(model, X, y, cv=5):
    """绘制学习曲线"""
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=cv, n_jobs=-1, 
        train_sizes=np.linspace(0.1, 1.0, 10)
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, label='Training score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.plot(train_sizes, val_mean, label='Cross-validation score')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)
    plt.xlabel('Training Examples')
    plt.ylabel('Score')
    plt.title('Learning Curves')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()
    
    return train_sizes, train_mean, val_mean
```

### 6.2 验证曲线

```python
from sklearn.model_selection import validation_curve

def plot_validation_curves(model, X, y, param_name, param_range, cv=5):
    """绘制验证曲线"""
    train_scores, val_scores = validation_curve(
        model, X, y, param_name=param_name, param_range=param_range, cv=cv
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mean, label='Training score')
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.plot(param_range, val_mean, label='Cross-validation score')
    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1)
    plt.xlabel(param_name)
    plt.ylabel('Score')
    plt.title('Validation Curves')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()
    
    return param_range, train_mean, val_mean
```

## 7. 错误分析

### 7.1 混淆矩阵分析

```python
import seaborn as sns

def analyze_confusion_matrix(y_true, y_pred, class_names=None):
    """分析混淆矩阵"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
    
    # 计算每个类别的指标
    class_metrics = {}
    for i in range(len(cm)):
        tp = cm[i, i]
        fp = cm[:, i].sum() - tp
        fn = cm[i, :].sum() - tp
        tn = cm.sum() - tp - fp - fn
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        class_metrics[i] = {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    return cm, class_metrics
```

### 7.2 错误案例分析

```python
def error_analysis(X, y_true, y_pred, feature_names=None):
    """错误分析"""
    # 找出错误预测的样本
    error_indices = np.where(y_true != y_pred)[0]
    
    # 分析错误类型
    error_types = {}
    for idx in error_indices:
        true_label = y_true[idx]
        pred_label = y_pred[idx]
        error_type = f"{true_label} -> {pred_label}"
        
        if error_type not in error_types:
            error_types[error_type] = []
        error_types[error_type].append(idx)
    
    # 统计错误类型
    error_stats = {error_type: len(indices) for error_type, indices in error_types.items()}
    
    # 分析特征重要性（如果使用树模型）
    if hasattr(model, 'feature_importances_'):
        feature_importance = model.feature_importances_
        error_analysis_results = {
            'error_types': error_stats,
            'feature_importance': feature_importance,
            'error_indices': error_indices
        }
    else:
        error_analysis_results = {
            'error_types': error_stats,
            'error_indices': error_indices
        }
    
    return error_analysis_results
```

## 8. 模型解释性

### 8.1 SHAP值分析

```python
import shap

def explain_model(model, X, feature_names=None):
    """使用SHAP解释模型"""
    # 创建SHAP解释器
    if hasattr(model, 'predict_proba'):
        explainer = shap.TreeExplainer(model) if hasattr(model, 'feature_importances_') else shap.KernelExplainer(model.predict_proba, X[:100])
    else:
        explainer = shap.TreeExplainer(model) if hasattr(model, 'feature_importances_') else shap.KernelExplainer(model.predict, X[:100])
    
    # 计算SHAP值
    shap_values = explainer.shap_values(X)
    
    # 绘制摘要图
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X, feature_names=feature_names)
    plt.show()
    
    return explainer, shap_values
```

### 8.2 LIME解释

```python
from lime import lime_tabular

def lime_explanation(model, X, feature_names=None, instance_idx=0):
    """使用LIME解释单个预测"""
    explainer = lime_tabular.LimeTabularExplainer(
        X, feature_names=feature_names, class_names=['0', '1'], mode='classification'
    )
    
    exp = explainer.explain_instance(X[instance_idx], model.predict_proba)
    exp.show_in_notebook()
    
    return exp
```

## 总结

模型评估是机器学习项目中的关键环节：

1. **分类评估**：准确率、精确率、召回率、F1分数
2. **回归评估**：MSE、RMSE、MAE、R²
3. **NLP评估**：BLEU、ROUGE等特定指标
4. **交叉验证**：K折、分层K折验证
5. **模型比较**：统计显著性检验
6. **过拟合检测**：学习曲线、验证曲线
7. **错误分析**：混淆矩阵、错误案例分析
8. **模型解释**：SHAP、LIME等可解释性方法

通过系统性的评估，可以全面了解模型性能并指导模型改进。 

## 数据增强详解

# 数据增强详解

在NLP中，数据增强（Data Augmentation）技术被广泛用于**小样本场景**（如低资源语言、医疗/金融垂直领域）和**模型鲁棒性提升**。以下是NLP数据增强的核心方法、应用场景及最新实践：

## 1. 为什么NLP需要数据增强？

*   **数据瓶颈**：标注成本高（如命名实体识别需专家标注）。
*   **模型泛化**：防止过拟合，提升对噪声和变体的鲁棒性。
*   **公平性**：平衡少数类样本（如罕见疾病术语）。

## 2. 常见NLP数据增强技术

### 2.1 文本表面级增强

| 方法 | 示例 | 适用任务 | 工具库 |
|---|---|---|---|
| **同义词替换** | "好的" → "好的呀"/"没问题" | 文本分类/情感分析 | `Synonyms`（中文） |
| **随机插入/删除** | "我爱苹果" → "我爱吃苹果" | 意图识别 | `nlpaug` |
| **字符级扰动** | "apple" → "app1e"（模拟拼写错误） | 拼写纠错/鲁棒性测试 | `TextAttack` |
| **回译（Back Translation）** | 中文→英文→中文（语义不变，表述变化） | 问答系统/生成任务 | Google Translate API |

### 2.2 语义级增强

| 方法 | 原理 | 适用场景 |
|---|---|---|
| **模板生成** | 基于规则生成新句子（如"${人物}在${地点}\${动作}"） | 低资源NER/关系抽取 |
| **预训练模型生成** | 用GPT-3生成语义相似的句子 | 数据扩充/对话系统 |
| **对抗样本生成** | 添加不易察觉的扰动（FGSM/PGD） | 模型鲁棒性测试 |

### 2.3 隐空间增强

*   **Mixup**：在嵌入空间线性插值
    ```math
    \tilde{x} = \lambda x_i + (1-\lambda)x_j, \quad \tilde{y} = \lambda y_i + (1-\lambda)y_j
    ```
    **应用**：文本分类（需在BERT嵌入层后操作）
*   **EDA (Easy Data Augmentation)**：结合替换/插入/删除/交换
    **工具**：`EDA-NLP`库

## 3. 任务专用增强策略

### 3.1 文本分类

*   **标签不变增强**：确保增强后文本标签不变
    ```python
    from nlpaug.augmenter.word import SynonymAug
    aug = SynonymAug(aug_src='wordnet', aug_max=3)
    augmented_text = aug.augment("This movie is great")
    ```

### 3.2 命名实体识别（NER）

*   **实体替换**：同类型实体互换（如"北京"→"上海"）
*   **上下文扰动**：保持实体不变，修改周围词

### 3.3 机器翻译

*   **双向回译**：
    ```text
    原文：今天天气真好  
    日译：今日は天気が本当に良い  
    回译：今天天气真的很好（新样本）
    ```

## 4. 数据增强的注意事项

### 4.1 语义一致性检查

*   **问题**：同义词替换可能改变语义（如"银行"→"河岸"）。
*   **解决方案**：
    *   使用上下文敏感替换（如BERT-Masked LM预测）。
    *   人工抽样验证增强数据质量。

### 4.2 过增强风险

*   **实验表明**：增强数据占比超过50%可能损害性能。
*   **推荐比例**：原始数据的20%~200%（依任务而定）。

### 4.3 领域适配性

*   **通用增强**（如EDA）在医疗/法律领域效果差 → 需领域词典支持。

## 5. 最新进展（2023）

| 技术 | 说明 | 论文/工具 |
|---|---|---|
| **LLM增强** | 用ChatGPT生成高质量增强数据 | 《GPT3 as Data Augmenter》 |
| **差分隐私增强** | 保证增强数据隐私性（如医疗NLP） | `Diff-Privacy-NLP` |
| **强化学习选择** | 自动选择最优增强策略 | 《RL-Aug: NLP Data Augmentation via Reinforcement Learning》 |

## 6. 完整Pipeline示例

```python
# 使用NLPAUG+回译的增强流程
import nlpaug.augmenter.word as naw

# 同义词增强
syn_aug = naw.SynonymAug(aug_src='wordnet')
texts = ["The quick brown fox jumps over the lazy dog"]
augmented = syn_aug.augment(texts, n=3)  # 生成3个变体

# 回译增强
back_translation = naw.BackTranslationAug(
    from_model_name='facebook/wmt19-en-de',
    to_model_name='facebook/wmt19-de-en'
)
bt_text = back_translation.augment("This is a test")
```

## 7. 总结

*   **何时使用**：数据量<10k时效果显著，尤其推荐用于**低资源语言**和**长尾分布**任务。
*   **避坑指南**：
    1.  避免对**语法敏感任务**（如句法分析）使用字符级扰动。
    2.  生成式增强（如GPT-3）需过滤低质量样本。
*   **未来方向**：
    *   大语言模型（LLM）作为增强引擎
    *   增强策略的元学习自动化选择

> 🔥 **最佳实践**：先尝试**回译+同义词替换**组合，监控验证集表现再调整增强强度!

## 类别不平衡处理详解

# 类别不平衡处理详解

## 1. 类别不平衡问题概述

在NLP任务中，**类别不平衡问题**（如情感分析中负面样本仅占5%）会导致模型偏向多数类，影响模型性能。

### 1.1 问题表现
- 模型在多数类上表现良好，少数类召回率极低
- 整体准确率高但F1分数低
- 模型无法学习到少数类的特征

### 1.2 不平衡程度分类
- **轻度不平衡**：1:3 比例
- **中度不平衡**：1:10 比例  
- **极度不平衡**：1:100+ 比例

## 2. 数据层面的解决方法

### 2.1 过采样（Oversampling）

#### 随机过采样
```python
import numpy as np
from collections import Counter

def random_oversampling(X, y, random_state=42):
    """随机过采样"""
    np.random.seed(random_state)
    
    # 统计各类别数量
    class_counts = Counter(y)
    majority_class = max(class_counts, key=class_counts.get)
    minority_class = min(class_counts, key=class_counts.get)
    
    # 计算需要复制的样本数
    target_count = class_counts[majority_class]
    
    # 获取少数类样本
    minority_indices = np.where(y == minority_class)[0]
    minority_X = X[minority_indices]
    minority_y = y[minority_indices]
    
    # 随机复制少数类样本
    indices_to_repeat = np.random.choice(
        len(minority_indices), 
        size=target_count - len(minority_indices), 
        replace=True
    )
    
    # 合并原始样本和复制样本
    oversampled_X = np.vstack([X, minority_X[indices_to_repeat]])
    oversampled_y = np.hstack([y, minority_y[indices_to_repeat]])
    
    return oversampled_X, oversampled_y
```

#### SMOTE（Synthetic Minority Over-sampling Technique）
```python
from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer

def smote_oversampling(texts, labels):
    """使用SMOTE进行过采样"""
    # 文本向量化
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(texts)
    
    # 应用SMOTE
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, labels)
    
    return X_resampled, y_resampled, vectorizer

# 使用示例
texts = ["positive text", "negative text", "positive text", "positive text"]
labels = [1, 0, 1, 1]  # 3个正类，1个负类

X_res, y_res, vectorizer = smote_oversampling(texts, labels)
print(f"原始样本数: {len(texts)}, 重采样后: {len(X_res)}")
```

#### 回译增强
```python
from googletrans import Translator

def back_translation_augmentation(texts, labels, target_language='en'):
    """回译增强"""
    translator = Translator()
    augmented_texts = []
    augmented_labels = []
    
    for text, label in zip(texts, labels):
        # 翻译到目标语言
        translated = translator.translate(text, dest=target_language)
        # 翻译回原语言
        back_translated = translator.translate(translated.text, dest='zh-cn')
        
        augmented_texts.append(back_translated.text)
        augmented_labels.append(label)
    
    return augmented_texts, augmented_labels
```

### 2.2 欠采样（Undersampling）

#### 随机欠采样
```python
def random_undersampling(X, y, random_state=42):
    """随机欠采样"""
    np.random.seed(random_state)
    
    class_counts = Counter(y)
    minority_class = min(class_counts, key=class_counts.get)
    target_count = class_counts[minority_class]
    
    # 获取多数类样本
    majority_indices = np.where(y != minority_class)[0]
    
    # 随机选择与少数类等量的多数类样本
    selected_indices = np.random.choice(
        majority_indices, 
        size=target_count, 
        replace=False
    )
    
    # 获取少数类样本
    minority_indices = np.where(y == minority_class)[0]
    
    # 合并样本
    undersampled_X = np.vstack([X[selected_indices], X[minority_indices]])
    undersampled_y = np.hstack([y[selected_indices], y[minority_indices]])
    
    return undersampled_X, undersampled_y
```

#### Tomek Links
```python
from imblearn.under_sampling import TomekLinks

def tomek_links_undersampling(X, y):
    """Tomek Links欠采样"""
    tl = TomekLinks()
    X_resampled, y_resampled = tl.fit_resample(X, y)
    return X_resampled, y_resampled
```

### 2.3 混合采样

#### SMOTE + Tomek Links
```python
from imblearn.combine import SMOTETomek

def smote_tomek_sampling(X, y):
    """SMOTE + Tomek Links混合采样"""
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)
    return X_resampled, y_resampled
```

## 3. 算法层面的解决方法

### 3.1 类别权重调整

#### 损失函数权重
```python
import torch
import torch.nn as nn

class WeightedCrossEntropyLoss(nn.Module):
    def __init__(self, class_weights):
        super().__init__()
        self.class_weights = torch.tensor(class_weights)
    
    def forward(self, inputs, targets):
        return nn.functional.cross_entropy(
            inputs, targets, weight=self.class_weights
        )

# 使用示例
# 假设负类样本是正类的10倍
class_weights = [1.0, 10.0]  # [多数类权重, 少数类权重]
criterion = WeightedCrossEntropyLoss(class_weights)
```

#### sklearn中的权重设置
```python
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier

# SVM类别权重
svm_model = LinearSVC(class_weight={0: 1, 1: 10})

# 随机森林类别权重
rf_model = RandomForestClassifier(class_weight='balanced')
```

### 3.2 改进的损失函数

#### Focal Loss
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        bce_loss = nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none'
        )
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss
        return focal_loss.mean()

# 使用示例
focal_loss = FocalLoss(alpha=0.25, gamma=2)
```

#### 标签平滑损失
```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.1, dim=-1):
        super().__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.cls = classes
        self.dim = dim
    
    def forward(self, pred, target):
        pred = pred.log_softmax(dim=self.dim)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.cls - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))
```

### 3.3 阈值移动（Threshold Moving）

```python
from sklearn.metrics import precision_recall_curve, f1_score

def find_optimal_threshold(y_true, y_pred_proba):
    """寻找最优阈值"""
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba)
    
    # 计算F1分数
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)
    f1_scores = f1_scores[:-1]  # 移除最后一个元素（对应threshold=inf）
    
    # 找到最大F1分数对应的阈值
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    return optimal_threshold

# 使用示例
y_true = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred_proba = [0.1, 0.8, 0.2, 0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.9]

optimal_threshold = find_optimal_threshold(y_true, y_pred_proba)
print(f"最优阈值: {optimal_threshold:.3f}")

# 使用最优阈值进行预测
y_pred = [1 if prob > optimal_threshold else 0 for prob in y_pred_proba]
```

## 4. 模型架构的改进

### 4.1 集成方法

#### EasyEnsemble
```python
from imblearn.ensemble import EasyEnsembleClassifier

def easy_ensemble_training(X, y, n_estimators=10):
    """EasyEnsemble训练"""
    eec = EasyEnsembleClassifier(n_estimators=n_estimators, random_state=42)
    eec.fit(X, y)
    return eec

# 使用示例
eec_model = easy_ensemble_training(X_train, y_train)
predictions = eec_model.predict(X_test)
```

#### 自定义集成
```python
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import StratifiedKFold

def custom_ensemble(X, y, base_models, n_splits=5):
    """自定义集成方法"""
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    ensemble_models = []
    
    for train_idx, val_idx in skf.split(X, y):
        X_train_fold, X_val_fold = X[train_idx], X[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]
        
        # 对每个fold进行欠采样
        X_train_balanced, y_train_balanced = random_undersampling(
            X_train_fold, y_train_fold
        )
        
        # 训练模型
        model = base_models[0]  # 使用第一个基础模型
        model.fit(X_train_balanced, y_train_balanced)
        ensemble_models.append(model)
    
    return ensemble_models

def ensemble_predict(ensemble_models, X):
    """集成预测"""
    predictions = []
    for model in ensemble_models:
        pred = model.predict(X)
        predictions.append(pred)
    
    # 多数投票
    ensemble_pred = np.mean(predictions, axis=0) > 0.5
    return ensemble_pred.astype(int)
```

### 4.2 两阶段训练

```python
def two_stage_training(X, y, feature_extractor, classifier):
    """两阶段训练"""
    # 第一阶段：用全部数据训练特征提取器
    print("第一阶段：训练特征提取器")
    feature_extractor.fit(X, y)
    
    # 提取特征
    X_features = feature_extractor.transform(X)
    
    # 第二阶段：对分类层使用平衡数据微调
    print("第二阶段：平衡数据微调分类器")
    X_balanced, y_balanced = smote_oversampling(X_features, y)
    classifier.fit(X_balanced, y_balanced)
    
    return feature_extractor, classifier
```

## 5. 评估指标的调整

### 5.1 不平衡分类的评估指标

```python
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score

def evaluate_imbalanced_classification(y_true, y_pred, y_pred_proba=None):
    """评估不平衡分类结果"""
    # 详细分类报告
    print("分类报告:")
    print(classification_report(y_true, y_pred, target_names=['多数类', '少数类']))
    
    # ROC-AUC
    if y_pred_proba is not None:
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        print(f"ROC-AUC: {roc_auc:.4f}")
    
    # PR-AUC（更适合不平衡数据）
    if y_pred_proba is not None:
        pr_auc = average_precision_score(y_true, y_pred_proba)
        print(f"PR-AUC: {pr_auc:.4f}")
    
    # G-Mean
    from sklearn.metrics import recall_score
    recall_0 = recall_score(y_true, y_pred, pos_label=0)
    recall_1 = recall_score(y_true, y_pred, pos_label=1)
    g_mean = np.sqrt(recall_0 * recall_1)
    print(f"G-Mean: {g_mean:.4f}")

# 使用示例
evaluate_imbalanced_classification(y_true, y_pred, y_pred_proba)
```

### 5.2 混淆矩阵可视化

```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, class_names=None):
    """绘制混淆矩阵"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('混淆矩阵')
    plt.ylabel('真实标签')
    plt.xlabel('预测标签')
    plt.show()

# 使用示例
plot_confusion_matrix(y_true, y_pred, class_names=['多数类', '少数类'])
```

## 6. 领域特定解决方案

### 6.1 文本生成任务

```python
def temperature_scaling_for_generation(model, text, temperature=0.7):
    """温度采样调整生成概率"""
    # 调整softmax温度，增加少数类词汇概率
    logits = model(text)
    scaled_logits = logits / temperature
    probs = torch.softmax(scaled_logits, dim=-1)
    
    return probs
```

### 6.2 命名实体识别（NER）

```python
def entity_level_oversampling(sentences, labels):
    """实体级别过采样"""
    # 统计各类实体数量
    entity_counts = {}
    for sentence, label_seq in zip(sentences, labels):
        for token, label in zip(sentence, label_seq):
            if label != 'O':  # 非O标签
                if label not in entity_counts:
                    entity_counts[label] = []
                entity_counts[label].append((sentence, label_seq))
    
    # 对少数类实体进行过采样
    balanced_data = []
    max_count = max(len(entities) for entities in entity_counts.values())
    
    for entity_type, entities in entity_counts.items():
        if len(entities) < max_count:
            # 重复采样
            oversampled = np.random.choice(
                entities, 
                size=max_count, 
                replace=True
            )
            balanced_data.extend(oversampled)
        else:
            balanced_data.extend(entities)
    
    return zip(*balanced_data)
```

### 6.3 预训练模型微调

```python
def dynamic_masking_for_imbalanced_data(tokenizer, texts, labels, 
                                       minority_class_boost=0.2):
    """动态掩码比例调整"""
    masked_texts = []
    
    for text, label in zip(texts, labels):
        tokens = tokenizer.tokenize(text)
        
        # 少数类文本增加掩码概率
        if label == 1:  # 假设1是少数类
            mask_prob = 0.15 + minority_class_boost
        else:
            mask_prob = 0.15
        
        # 随机掩码
        masked_tokens = []
        for token in tokens:
            if np.random.random() < mask_prob:
                masked_tokens.append('[MASK]')
            else:
                masked_tokens.append(token)
        
        masked_texts.append(' '.join(masked_tokens))
    
    return masked_texts
```

## 7. 完整Pipeline示例

```python
def complete_imbalanced_classification_pipeline():
    """完整的类别不平衡处理流程"""
    
    # 1. 数据加载和预处理
    # texts, labels = load_data()
    
    # 2. 文本向量化
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(texts)
    
    # 3. 数据重采样
    X_resampled, y_resampled = smote_oversampling(X, labels)
    
    # 4. 模型训练（使用类别权重）
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(class_weight='balanced', random_state=42)
    model.fit(X_resampled, y_resampled)
    
    # 5. 预测和阈值优化
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    optimal_threshold = find_optimal_threshold(y_test, y_pred_proba)
    y_pred = (y_pred_proba > optimal_threshold).astype(int)
    
    # 6. 评估
    evaluate_imbalanced_classification(y_test, y_pred, y_pred_proba)
    
    return model, vectorizer, optimal_threshold

# 使用Pipeline
model, vectorizer, threshold = complete_imbalanced_classification_pipeline()
```

## 8. 方法对比与选择指南

| 方法 | 适用场景 | 优点 | 缺点 |
|------|----------|------|------|
| **SMOTE** | 小规模不平衡文本（<10k条） | 生成语义合理的样本 | 计算成本高 |
| **类别权重** | 大规模数据+深度学习模型 | 无需修改数据分布 | 需调参 |
| **Focal Loss** | 高度不平衡（1:100+） | 自动聚焦难样本 | 可能训练不稳定 |
| **两阶段训练** | 预训练模型（BERT/GPT） | 保留预训练知识 | 需要额外训练步骤 |

## 9. 关键要点总结

1. **轻度不平衡（1:3）**：使用类别权重或阈值移动
2. **中度不平衡（1:10）**：SMOTE + 集成方法
3. **极度不平衡（1:100+）**：Focal Loss + 两阶段训练
4. **评估指标**：优先使用F1-Macro、AUC-ROC、G-Mean
5. **最佳实践**：
   - 优先尝试对模型无损的方法（如损失函数调整）
   - 过采样时确保生成样本的语义合理性
   - 始终用宏平均F1/AUC-ROC替代准确率评估 

