# L2范数与L2范数惩罚详解

## 一、L2范数的数学定义

## 1. 基本概念

L2范数（也称为欧几里得范数）是向量空间中向量长度的度量方式。对于一个n维向量`$w = (w_1, w_2, ..., w_n)$`，其L2范数定义为：

```math
\|w\|_2 = \sqrt{\sum_{i=1}^n w_i^2} = \sqrt{w_1^2 + w_2^2 + \cdots + w_n^2}
```

## 2. 几何解释

在二维空间中，L2范数就是向量的几何长度（从原点到点的直线距离）。在更高维空间中，它保持同样的几何意义。

## 二、L2范数惩罚（权重衰减）

## 1. 定义

L2范数惩罚（也称为权重衰减或Tikhonov正则化）是在损失函数中添加模型权重的L2范数平方作为惩罚项：

```math
J_{reg}(w) = J(w) + \frac{\lambda}{2}\|w\|_2^2 = J(w) + \frac{\lambda}{2}\sum_{i=1}^n w_i^2
```

其中：

*   `$J(w)$`是原始损失函数
*   `$\lambda$`是正则化强度（超参数）
*   `$\frac{1}{2}$`是为了求导后形式简洁而添加的系数

## 2. 作用原理

L2惩罚通过以下方式防止过拟合：

1.  **限制权重幅度**：倾向于让所有权重取较小值
2.  **分散权重贡献**：避免某些权重特别大而主导模型
3.  **改善条件数**：使优化问题更易于求解

## 三、在深度学习中的具体应用

## 1. 实现方式

在PyTorch中可以通过两种方式实现：

### 方式1：优化器直接设置weight\_decay参数

```python
optimizer = torch.optim.SGD(model.parameters(), 
                          lr=0.01, 
                          weight_decay=1e-4)  # λ=0.0001
```

### 方式2：手动添加到损失函数

```python
loss = criterion(outputs, labels)
l2_reg = torch.tensor(0.)
for param in model.parameters():
    l2_reg += torch.norm(param, p=2)**2  # L2范数平方
loss += lambda * l2_reg
```

## 2. 反向传播效果

添加L2惩罚后，权重更新公式变为：

```math
w_{t+1} = w_t - \eta \nabla J(w_t) - \eta \lambda w_t
```

可以看到每次更新时权重会额外衰减`$\eta \lambda w_t$`

## 3. 典型使用场景

*   **全连接层**：通常对所有权重应用L2惩罚
*   **卷积层**：有时只对kernel权重使用
*   **偏置项**：通常不施加L2惩罚（实践中可通过设置不同参数组实现）

## 四、L2惩罚的几何解释

## 1. 优化视角

原始优化问题：

```math
\min_w J(w)
```

加上L2惩罚后：

```math
\min_w J(w) \quad \text{s.t.} \quad \|w\|_2^2 \leq C
```

这相当于限制参数在一个高维球体内

## 2. 解的特性

*   倾向于产生**稠密解**（所有特征都有小量贡献）
*   与L1正则化对比鲜明（L1产生稀疏解）

## 五、超参数λ的选择

## 1. 影响规律

*   λ过大：模型欠拟合（权重被过度压制）
*   λ过小：正则化效果不明显
*   合适范围：通常`$10^{-5}$`到`$10^{-2}$`之间

## 2. 调参建议

1.  从λ=0.001开始尝试
2.  使用对数尺度搜索（如0.0001, 0.001, 0.01, 0.1）
3.  监控训练/验证损失曲线

## 六、与其他技术的配合使用

## 1. 与BatchNorm的关系

*   BatchNorm本身已有正则化效果
*   使用BatchNorm时，可减小L2惩罚强度
*   通常对BN层的γ/β参数不使用L2惩罚

## 2. 与Dropout的组合

*   L2惩罚主要控制权重幅度
*   Dropout主要防止神经元共适应
*   两者互补，常一起使用

## 七、实际案例

## CNN中的典型配置

```python
# 卷积层参数组（应用L2）
conv_params = []
for name, param in model.named_parameters():
    if 'conv' in name and 'weight' in name:
        conv_params.append(param)

# BN层参数组（不应用L2）
bn_params = []
for name, param in model.named_parameters():
    if 'bn' in name:
        bn_params.append(param)

optimizer = torch.optim.SGD([
    {'params': conv_params, 'weight_decay': 1e-4},
    {'params': bn_params, 'weight_decay': 0}
], lr=0.1)
```

## 八、为什么L2惩罚有效？

## 1. 贝叶斯视角

L2惩罚对应于参数的高斯先验分布（最大后验估计MAP）

## 2. 学习理论视角

通过限制假设空间的复杂度，提高泛化能力

## 3. 数值稳定性

防止某些维度权重过大导致的数值问题

需要了解如何在具体网络架构中精细调整L2惩罚，或者想看到不同λ值的效果对比实验吗？
