### **N-gram 模型详解**

**N-gram** 是 NLP 中基于统计的语言模型，用于**捕捉局部词序信息**。它假设一个词的出现概率仅依赖于前 `$n-1$` 个词，属于**马尔可夫模型**的简化形式。

***

## **1. N-gram 的定义与计算**

### **（1）基本概念**

*   **N-gram**：由 `$n$` 个连续词组成的序列。
    *   **Unigram (1-gram)**：单个词（如 "NLP"）
    *   **Bigram (2-gram)**：相邻两词（如 "自然语言"）
    *   **Trigram (3-gram)**：连续三词（如 "自然语言处理"）

### **（2）概率计算**

N-gram 语言模型通过\*\*极大似然估计（MLE）\*\*计算词序列概率：

```math
P(w_i | w_{i-n+1}, ..., w_{i-1}) = \frac{\text{count}(w_{i-n+1}, ..., w_i)}{\text{count}(w_{i-n+1}, ..., w_{i-1})}
```

**示例**：

*   语料库句子："I love NLP" 和 "I love deep learning"
*   Bigram 概率：
    *   `$P(\text{NLP} | \text{love}) = \frac{\text{count}(\text{love NLP})}{\text{count}(\text{love})} = \frac{1}{2}$`

### **（3）平滑技术**

解决零概率问题（未登录词）：

*   **加一平滑（Laplace）**：所有计数 +1
*   **回退（Backoff）**：高阶 N-gram 缺失时使用低阶 N-gram
*   **插值（Interpolation）**：加权组合不同阶 N-gram

***

## **2. N-gram 的应用场景**

### **（1）文本生成**

*   基于历史词预测下一个词（如手机输入法推荐）。\
    **示例**：
    > 输入 "自然语言" → 高概率接 "处理"（Trigram 模型）。

### **（2）拼写纠错**

*   检测并纠正非法的 N-gram 组合（如 "apple the" 概率极低）。

### **（3）机器翻译**

*   早期统计机器翻译（SMT）中计算短语对齐概率。

### **（4）信息检索**

*   扩展查询（如搜索 "New York" 时包含 "NY" 的 Bigram）。

### **（5）语音识别**

*   结合声学模型解码最可能的词序列（如 "recognize speech" vs "wreck a nice beach"）。

***

## **3. N-gram 的优缺点**

### **✅ 优点**

*   **简单高效**：计算复杂度低，适合实时应用。
*   **保留局部词序**：相比 BoW，能捕捉部分上下文。

### **❌ 缺点**

*   **数据稀疏性**：高阶 N-gram 需要极大语料库（如 5-gram 模型）。
*   **长距离依赖缺失**：无法建模超过 `$n$` 个词的依赖（如主语-动词一致性）。
*   **语义盲区**：同义词组合（如 "buy car" 和 "purchase automobile"）被视为不同事件。

***

## **4. 现代 NLP 中的替代方案**

*   **神经语言模型（RNN/LSTM）**：解决长距离依赖，但训练慢。
*   **Transformer（如 GPT）**：自注意力机制全局建模，完全取代 N-gram。

**面试回答建议**：

> "N-gram 是统计语言模型的基石，但在深度学习时代，它主要用于轻量级任务或作为特征补充。例如，GPT 虽基于 Transformer，但在数据预处理阶段仍可能使用 N-gram 进行数据增强。"

如果需要具体代码实现（如 N-gram 生成或平滑技术细节），可进一步探讨！
