### **TF-IDF（词频-逆文档频率）详解**

**TF-IDF**（Term Frequency-Inverse Document Frequency）是一种用于信息检索和文本挖掘的**加权统计方法**，用于衡量一个词在文档中的重要程度。它结合了**词频（TF）和**逆文档频率（IDF），能够降低常见词的权重，提升关键字的区分度。

***

## **1. TF-IDF 的计算方法**

### **（1）词频（TF, Term Frequency）**

衡量一个词在**当前文档**中出现的频率：

```math
\text{TF}(t, d) = \frac{\text{词 } t \text{ 在文档 } d \text{ 中出现的次数}}{\text{文档 } d \text{ 的总词数}}
```

**示例**：

*   文档 `d`："I love NLP and I love deep learning"
*   `love` 的 TF = `2（出现2次）/ 7（总词数） ≈ 0.286`

***

### **（2）逆文档频率（IDF, Inverse Document Frequency）**

衡量一个词在**整个语料库**中的稀有程度，常见词（如 "the", "is"）会被降低权重：

```math
\text{IDF}(t, D) = \log \left( \frac{N}{\text{包含词 } t \text{ 的文档数} + 1} \right)
```

其中：

*   `$N$` = 语料库中文档总数
*   `$\text{df}(t)$` = 包含词 `$t$` 的文档数
*   **+1** 防止分母为 0（平滑处理）

**示例**：

*   假设语料库有 `1000` 篇文档，`"NLP"` 出现在 `50` 篇中，`"the"` 出现在 `900` 篇中。
    *   `IDF("NLP") = log(1000 / (50 + 1)) ≈ 3.0`
    *   `IDF("the") = log(1000 / (900 + 1)) ≈ 0.1`

***

### **（3）TF-IDF 计算**

最终 TF-IDF 值是 TF 和 IDF 的乘积：

```math
\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
```

**示例**：

*   对于词 `"love"` 在某个文档中的 TF=0.286，IDF=1.5（假设），则：
    *   `TF-IDF("love") = 0.286 × 1.5 ≈ 0.429`

***

## **2. TF-IDF 的主要用途**

### **（1）信息检索（Search Engines）**

*   用于计算**查询词与文档的相关性**，排序搜索结果（如 Elasticsearch 默认使用 TF-IDF）。
*   **示例**：搜索 "NLP tutorial"，TF-IDF 会赋予 "NLP" 更高权重（稀有词），降低 "tutorial" 的权重（常见词）。

### **（2）文本分类 & 聚类**

*   作为特征输入到机器学习模型（如 SVM、朴素贝叶斯）。
*   **示例**：在垃圾邮件检测中，"free"、"offer" 可能有高 TF-IDF，帮助分类。

### **（3）关键词提取（Keyword Extraction）**

*   自动提取文档的关键词（如新闻摘要、SEO优化）。
*   **示例**：从一篇 AI 论文中提取 "transformer"、"attention" 等高 TF-IDF 词。

### **（4）文档相似度计算**

*   结合余弦相似度，衡量两篇文档的相似性（如推荐系统）。

***

## **3. TF-IDF 的优缺点**

### **✅ 优点**

*   **简单高效**：计算速度快，适合中小规模数据。
*   **降低噪声**：通过 IDF 抑制常见无意义词（如 "the", "and"）。
*   **可解释性强**：权重直接反映词的重要性。

### **❌ 缺点**

*   **无法捕获语义**：同义词（如 "car" 和 "automobile"）会被视为不同词。
*   **忽略词序**：和 BoW 一样，丢失上下文信息。
*   **长文档偏差**：长文档词频可能被放大，需结合归一化（如 L2 标准化）。

***

## **4. 现代 NLP 中的替代方案**

*   **Word2Vec / GloVe**：稠密向量表示语义关系。
*   **BERT / Transformer**：动态上下文嵌入，彻底超越 TF-IDF。

**面试回答建议**：

> "TF-IDF 是传统NLP的重要特征表示方法，但在深度学习时代，它主要用于轻量级任务或作为基线模型。现代方法如 BERT 能更好地处理语义和上下文，但 TF-IDF 仍适用于低资源场景。"

如果需要具体代码实现（如 Python `sklearn` 示例）或数学优化细节，可以进一步探讨！
