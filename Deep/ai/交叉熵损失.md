# &#x20;交叉熵

交叉熵损失（Cross-Entropy Loss）是机器学习和深度学习中常用的损失函数，尤其适用于分类任务。它通过衡量模型预测概率分布与真实概率分布之间的差异来指导模型优化。

## 核心概念

1.  **熵（Entropy）**\
    表示信息的不确定性，计算公式为：\
    `$H(p) = -\sum p(x) \log p(x)$`\
    其中 ( p(x) ) 是事件 ( x ) 的真实概率。

2.  **KL散度（相对熵）**\
    衡量两个概率分布的差异：\
    `$D_{KL}(p \| q) = \sum p(x) \log \frac{p(x)}{q(x)}$`\
    其中 ( p ) 是真实分布，( q ) 是预测分布。

3.  **交叉熵**\
    交叉熵是熵与KL散度的组合：\
    `$H(p, q) = H(p) + D_{KL}(p \| q) = -\sum p(x) \log q(x)$`\
    当 ( p ) 是固定分布时，最小化交叉熵等价于最小化KL散度。

***

## 分类任务中的形式

1.  **二分类交叉熵损失**\
    `$L = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]$`
    *   ( `$y\_i \in {0,1}  $`)：真实标签
    *   ( `$p\_i \in (0,1) $`)：模型预测为正类的概率

2.  **多分类交叉熵损失**
    *   **Softmax交叉熵**（单标签分类）：\
        `$L = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(p_{i,c})$`
        *   ( `$y\_{i,c} \in {0,1} $`)：样本 ( i ) 是否属于类 ( c )
        *   (`$ p\_{i,c} $`)：模型预测样本 ( i ) 属于类 ( c ) 的概率（通过Softmax归一化）

    *   **多标签分类**时，通常改用Sigmoid输出并逐类计算二分类交叉熵。

***

## 关键特性

1.  **概率校准**\
    鼓励预测概率逼近真实分布（如真实类别的概率趋近1，其他趋近0）。

2.  **梯度性质**
    *   对于Softmax输出，梯度为 ( `$\frac{\partial L}{\partial z\_i} = p\_i - y\_i $` )，直观且易于优化。
    *   误差越大，梯度幅度越大，更新速度越快。

3.  **与对数似然的关系**\
    最小化交叉熵等价于最大化对数似然（MLE）。

***

## 代码实现示例（PyTorch）

```python
import torch.nn as nn

# 二分类任务
loss_fn = nn.BCELoss()  # 输入需经过Sigmoid
# 或直接使用Logits版本（数值稳定）
loss_fn = nn.BCEWithLogitsLoss()

# 多分类任务
loss_fn = nn.CrossEntropyLoss()  # 输入为未归一化的Logits，标签为类别索引
```

***

## 常见问题

1.  **数值稳定性**\
    直接计算 (\log(p)) 可能导致数值溢出，通常实现时使用Log-Sum-Exp技巧（如`nn.CrossEntropyLoss`已内置）。

2.  **类别不平衡**\
    可通过加权交叉熵（Weighted Cross-Entropy）调整类别权重：\
    `$L = -\sum w_c \cdot y_c \log(p_c)$`

3.  **标签平滑（Label Smoothing）**\
    避免模型过度自信，将真实标签从1调整为略小的值（如0.9），提升泛化性。

# Log-Sum-Exp技巧

## 详解

Log-Sum-Exp（LSE）是一种数学技巧，主要用于**数值稳定地计算指数和的对数**，常见于机器学习的概率计算（如Softmax、交叉熵损失）。它的核心作用是**避免数值溢出（overflow）或下溢（underflow）**，同时保持数学等价性。

***

## **1. 基本定义**

给定一个实数向量 `$( \mathbf{z} = [z_1, z_2, ..., z_n] )$`，Log-Sum-Exp 定义为：

```math
\text{LSE}(\mathbf{z}) = \log \left( \sum_{i=1}^n e^{z_i} \right)
```

**关键点**：

*   直接计算 ( e^{z\_i} ) 可能导致数值溢出（( z\_i ) 很大时）或下溢（( z\_i ) 很小时）。
*   LSE 通过**减去最大值**来保持数值稳定性：

```math
 \text{LSE}(\mathbf{z}) = \log \left( \sum_{i=1}^n e^{z_i - \max(\mathbf{z})} \right) + \max(\mathbf{z})
```

这样，所有 `$( e^{z_i - \max(\mathbf{z})} )$` 的值都在 (\[0, 1]) 之间，避免数值问题。

***

## **2. 为什么需要 Log-Sum-Exp？**

## **问题场景**

在计算 **Softmax** 或 **交叉熵损失** 时，我们需要计算：

```math
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
```

```math
\text{CrossEntropy} = -\log \left( \frac{e^{z_y}}{\sum_{j=1}^n e^{z_j}} \right) = -z_y + \log \left( \sum_{j=1}^n e^{z_j} \right)
```

如果 ( z\_i ) 很大（如 1000），直接计算 ( e^{1000} ) 会导致数值溢出（`inf`）；如果 ( z\_i ) 很小（如 -1000），( e^{-1000} ) 会变成 0（下溢）。

## **解决方法：Log-Sum-Exp**

使用 LSE 技巧：

```math
\log \left( \sum_{j=1}^n e^{z_j} \right) = \log \left( \sum_{j=1}^n e^{z_j - \max(\mathbf{z})} \right) + \max(\mathbf{z})
```

这样：

*   `$( e^{z_j - \max(\mathbf{z})} )$` 的最大值为 1（当 `$( z_j = \max(\mathbf{z}) )$`），其余值在 (0, 1] 之间，避免数值问题。
*   最终结果仍然正确，因为：

```math
 \log \left( \sum e^{z_j} \right) = \log \left( e^{\max(\mathbf{z})} \sum e^{z_j - \max(\mathbf{z})} \right) = \max(\mathbf{z}) + \log \left( \sum e^{z_j - \max(\mathbf{z})} \right)
```

***

## **3. 在 Softmax 和交叉熵中的应用**

## **(1) 数值稳定的 Softmax**

原始 Softmax：

```math
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
```

使用 LSE 技巧：

1.  计算 `$( m = \max(z_1, z_2, ..., z_n) )$`。
2.  计算 `$( \text{Softmax}(z_i) = \frac{e^{z_i - m}}{\sum_{j=1}^n e^{z_j - m}} )$`。

**代码示例（Python）**：

```python
import numpy as np

def stable_softmax(z):
    m = np.max(z)
    exp_z = np.exp(z - m)  # 数值稳定
    return exp_z / np.sum(exp_z)
```

## **(2) 数值稳定的交叉熵损失**

原始交叉熵：

```math
L = -\log \left( \frac{e^{z_y}}{\sum_{j=1}^n e^{z_j}} \right) = -z_y + \log \left( \sum_{j=1}^n e^{z_j} \right)
```

使用 LSE 技巧：

```math
L = -z_y + \text{LSE}(\mathbf{z}) = -z_y + \left( \log \left( \sum e^{z_j - m} \right) + m \right)
```

**代码示例（Python）**：

```python
def cross_entropy(z, true_class):
    m = np.max(z)
    log_sum_exp = np.log(np.sum(np.exp(z - m))) + m
    return -z[true_class] + log_sum_exp
```

***

## **4. 总结**

| 概念                    | 作用                                   | 公式                                               |
| --------------------- | ------------------------------------ | ------------------------------------------------ |
| **Log-Sum-Exp (LSE)** | 计算 `$( \log \sum e^{z_i} )$` 的数值稳定方法 | `$( \log \left( \sum e^{z_i - m} \right) + m )$` |
| **Softmax**           | 概率归一化                                | `$( \frac{e^{z_i - m}}{\sum e^{z_j - m}} )$`     |
| **交叉熵损失**             | 分类任务的损失函数                            | `$( -z_y + \text{LSE}(\mathbf{z}) )$`            |

**关键优势**：
✅ 避免数值溢出（大数问题）\
✅ 避免数值下溢（小数问题）\
✅ 保持数学等价性

Log-Sum-Exp 是深度学习（如 Softmax、交叉熵）实现时的**必备技巧**，几乎所有框架（PyTorch、TensorFlow）都内置了该优化。

# &#x20;BCELoss, BCEWithLogitsLoss, CrossEntropyLoss&#x20;

***

## **1. BCELoss（二元交叉熵损失）**

### **原理**

*   **适用任务**：二分类问题（输出概率在 0\~1 之间）。
*   **数学公式**：
    ```math
    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]
    ```
    *   `$( y_i \in {0, 1} )$`：真实标签。
    *   `$( p_i \in (0, 1) )$`：模型预测的概率（需经过 **Sigmoid** 激活）。

### **用法**

```python
import torch.nn as nn

criterion = nn.BCELoss()
predictions = torch.sigmoid(model(inputs))  # 必须手动加 Sigmoid
loss = criterion(predictions, labels.float())
```

### **特点**

*   **输入要求**：预测值需经过 Sigmoid 归一化到 (0, 1)。
*   **缺点**：数值不稳定（log(0) 会导致 NaN）。

### **使用场景**

*   二分类任务（如垃圾邮件检测、医学图像分割）。

***

## **2. BCEWithLogitsLoss（带 Sigmoid 的二元交叉熵）**

### **原理**

*   **改进点**：整合 Sigmoid + BCELoss，**数值更稳定**。
*   **数学公式**：与 BCELoss 相同，但内部自动计算 Sigmoid：
    ```math
    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(\sigma(z_i)) + (1-y_i) \log(1-\sigma(z_i)) \right]
    ```
    *   `$( z_i )$`：模型原始输出（logits）。
    *   `$( \sigma )$`：Sigmoid 函数。

### **用法**

```python
criterion = nn.BCEWithLogitsLoss()
loss = criterion(model(inputs), labels.float())  # 无需手动 Sigmoid
```

### **特点**

*   **输入要求**：直接使用模型原始输出（无需 Sigmoid）。
*   **内置功能**：
    *   自动应用 Sigmoid。
    *   支持 **标签平滑（label smoothing）** 和 **权重调整**。

### **使用场景**

*   二分类任务（推荐替代 BCELoss，避免数值问题）。

***

## **3. CrossEntropyLoss（多分类交叉熵损失）**

### **原理**

*   **适用任务**：多分类问题（输出类别概率分布）。
*   **数学公式**：
    ```math
    \mathcal{L} = -\sum_{i=1}^N y_i \log(p_i)
    ```
    *   `$( y_i )$`：one-hot 编码的真实标签。
    *   `$( p_i )$`：预测的类别概率（需经过 **Softmax** 归一化）。

### **用法**

```python
criterion = nn.CrossEntropyLoss()
loss = criterion(model(inputs), labels)  # labels 是类别索引（非 one-hot）
```

### **特点**

*   **输入要求**：
    *   模型输出为 **原始 logits**（未归一化，内部自动计算 Softmax）。
    *   标签为 **类别索引**（如 `[0, 2, 1]`，不是 one-hot）。
*   **内置功能**：支持类别权重（`weight` 参数）和忽略特定类别（`ignore_index`）。

### **使用场景**

*   多分类任务（如手写数字识别、图像分类）。

***

## **4. 三者的关键区别**

| 特性        | BCELoss     | BCEWithLogitsLoss | CrossEntropyLoss |
| --------- | ----------- | ----------------- | ---------------- |
| **输入要求**  | 需手动 Sigmoid | 原始 logits         | 原始 logits        |
| **输出范围**  | (0, 1)      | (-∞, +∞)          | (-∞, +∞)         |
| **任务类型**  | 二分类         | 二分类               | 多分类              |
| **数值稳定性** | 不稳定（需手动处理）  | 稳定（内置 Sigmoid）    | 稳定（内置 Softmax）   |
| **标签格式**  | Float (0/1) | Float (0/1)       | Long (类别索引)      |

***

## **5. 如何选择？**

1.  **二分类任务**：
    *   优先选 `BCEWithLogitsLoss`（数值稳定，无需手动 Sigmoid）。
    *   若需自定义 Sigmoid 参数，再用 `BCELoss`。
2.  **多分类任务**：
    *   直接用 `CrossEntropyLoss`（内部自动 Softmax）。

***

## **6. 代码对比**

## **二分类任务**

```python
# 方法1：BCELoss（需手动 Sigmoid）
predictions = torch.sigmoid(model(inputs))
loss = nn.BCELoss()(predictions, labels.float())

# 方法2：BCEWithLogitsLoss（推荐）
loss = nn.BCEWithLogitsLoss()(model(inputs), labels.float())
```

## **多分类任务**

```python
# CrossEntropyLoss（标签为类别索引）
loss = nn.CrossEntropyLoss()(model(inputs), labels)  # labels: [0, 2, 1,...]
```

***

## **7. 常见问题**

## **Q1：为什么 `BCEWithLogitsLoss` 更稳定？**

*   内部使用 **Log-Sum-Exp 技巧** 避免数值溢出。
*   示例：
    ```python
    # 不稳定的原始计算（可能 NaN）
    loss = - (y * torch.log(sigmoid(z)) + (1-y) * torch.log(1-sigmoid(z))

    # 稳定的 BCEWithLogitsLoss 计算
    loss = torch.max(z, 0) + torch.log(1 + torch.exp(-torch.abs(z))) - y * z
    ```

## **Q2：多标签分类用什么损失？**

*   若一个样本属于多个类别（如标签 `[1, 0, 1]`），用 `BCEWithLogitsLoss`：
    ```python
    loss = nn.BCEWithLogitsLoss()(outputs, labels.float())  # labels: [[1, 0, 1], ...]
    ```

***

## **总结**

*   **`BCELoss`**：二分类，需手动 Sigmoid，易数值不稳定。
*   **`BCEWithLogitsLoss`**：二分类最佳选择，内置 Sigmoid，稳定高效。
*   **`CrossEntropyLoss`**：多分类任务，自动 Softmax，输入为类别索引。

**一句话选择**：

*   二分类 → `BCEWithLogitsLoss`
*   多分类 → `CrossEntropyLoss`

# &#x20;交叉熵计算过程

我来用一个具体的数值例子解释交叉熵的计算过程：

假设我们有以下数据：

```python
# 预测概率 (batch_size=2, num_classes=3)
y_hat = torch.tensor([[0.1, 0.3, 0.6],  # 样本1的预测概率
                      [0.3, 0.2, 0.5]]) # 样本2的预测概率

# 真实标签 (batch_size=2)
y = torch.tensor([2, 0])  # 样本1的真实类别是2，样本2是0
```

计算步骤分解：

1.  **高级索引获取预测概率**：

```python
# 相当于：
# 样本1取第2个概率值(索引2)：0.6
# 样本2取第0个概率值(索引0)：0.3
selected_probs = y_hat[ [0,1], [2,0] ]  # 得到 tensor([0.6, 0.3])
```

1.  **计算对数概率**：

```python
log_probs = torch.log(selected_probs)  # tensor([-0.5108, -1.2040])
```

1.  **计算负对数似然**：

```python
neg_log_likelihood = -log_probs  # tensor([0.5108, 1.2040])
```

1.  **求平均损失**：

```python
loss = neg_log_likelihood.mean()  # tensor(0.8574)
```

可视化计算过程：

    样本1：
    预测概率: [0.1, 0.3, 0.6]
    真实类别: 2 → 取0.6
    -log(0.6) ≈ 0.5108

    样本2：
    预测概率: [0.3, 0.2, 0.5] 
    真实类别: 0 → 取0.3
    -log(0.3) ≈ 1.2040

    最终损失 = (0.5108 + 1.2040)/2 ≈ 0.8574

这个值越小说明模型预测越准确，当预测概率完全匹配真实标签时，损失为0。

## &#x20;全部代码

```python
def cross_entropy(y_hat, y):
    # # 预测概率 (batch_size=2, num_classes=3)
    # y_hat = torch.tensor([[0.1, 0.3, 0.6],  # 样本1的预测概率
    #                        [0.3, 0.2, 0.5]]) # 样本2的预测概率
    # 步骤1：构建索引元组
    # list(range(len(y_hat))) 生成[0,1,...,batch_size-1]的索引
    # y 是真实标签的tensor，如[2,0,1]表示3个样本的真实类别

    r0 = (list(range(len(y_hat))), y)

    # 样本1取第2个概率值(索引2)：0.6
    # 样本2取第0个概率值(索引0)：0.3
    # selected_probs = y_hat[ [0,1], [2,0] ]  # 得到 tensor([0.6, 0.3])
    #  不好理解的就是这种高级索引方式
    selected_probs = y_hat[r0]

    # 步骤3：计算对数概率
    # 对预测概率取对数（数值稳定性已由softmax保证）
    r2 = torch.log(r1)  # 形状：(batch_size,)

    # 步骤4：计算平均负对数似然
    # 取平均得到交叉熵损失
    r3 = -r2.mean()  # 标量值

    return r3  # 返回最终的损失值
    # return -torch.log(y_hat[list(range(len(y_hat))), y]).mean()
```

