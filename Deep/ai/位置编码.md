# 位置编码 (Positional Encoding) 详解

## 1. 基本概念与作用

位置编码是Transformer架构中用来注入序列位置信息的一种方法。由于Transformer的自注意力机制本身是位置无关的(permutation invariant)，需要额外加入位置信息使模型能够利用序列的顺序特性。

**核心作用**：

*   为模型提供单词在序列中的绝对或相对位置信息
*   保持序列长度可变性(可处理任意长度序列)
*   使注意力机制能够区分不同位置的相同单词

## 2. 原理与数学表达

原始Transformer使用正弦余弦编码，其设计原则是：

*   唯一编码每个位置
*   相对位置编码可通过线性变换得到
*   可扩展到比训练时更长的序列

数学表达式：

```math
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})
```

其中：

*   `$pos$`: 位置索引
*   `$i$`: 维度索引(0到`$d_{model}/2-1$`)
*   `$d_{model}$`: 嵌入维度

## 3. 关键特性

**3.1 相对位置关系**
任意固定偏移量`$k$`，`$PE_{pos+k}$`可表示为`$PE_{pos}$`的线性函数：

```math
\begin{aligned}
PE_{pos+k} &= \begin{bmatrix}
\sin(\omega_i(pos+k)) \\
\cos(\omega_i(pos+k))
\end{bmatrix} \\
&= \begin{bmatrix}
\cos(\omega_i k) & \sin(\omega_i k) \\
-\sin(\omega_i k) & \cos(\omega_i k)
\end{bmatrix}
\begin{bmatrix}
\sin(\omega_i pos) \\
\cos(\omega_i pos)
\end{bmatrix}
\end{aligned}
```

其中`$\omega_i = 1/10000^{2i/d_{model}}$`

**3.2 波长变化**
不同维度对应不同波长的正弦波，从`$2\pi$`到`$10000·2\pi$`，形成多尺度位置信息

## 4. 实现方式(PyTorch示例)

```python
import torch
import math

class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

## 5. 变体与改进

**5.1 学习式位置编码**

*   直接训练位置嵌入矩阵
*   示例：`torch.nn.Embedding(max_len, d_model)`

**5.2 相对位置编码**

*   关注位置之间的相对关系而非绝对位置
*   典型实现：在注意力分数计算中加入相对位置偏置

**5.3 旋转位置编码(RoPE)**

*   通过旋转矩阵实现相对位置编码
*   广泛应用于LLaMA等现代大模型

## 6. 可视化理解

位置编码矩阵的可视化通常显示为：

*   行方向：位置变化(频率渐变)
*   列方向：维度变化(波长从短到长)
*   形成明显的对角线条纹图案

## 7. 在Transformer中的应用

实际应用时：

1.  输入嵌入 + 位置编码
2.  对每个注意力头，位置信息会被自动学习利用
3.  在多层Transformer中，位置信息会通过注意力机制传播

## 8. 数学性质验证

正弦余弦编码的优良特性：

*   唯一性：每个位置有唯一编码
*   线性可加性：`$PE_{a+b} = f(PE_a, PE_b)$`
*   对称性：距离对称的位置关系一致

需要更深入探讨某个方面吗？比如具体实现细节、不同编码方式的比较，或者在实际模型中的应用案例？

# 深入理解正弦余弦位置编码的设计原理

## 1. 设计动机

正弦余弦位置编码的核心设计目标是为模型提供：

*   **绝对位置信息**：每个位置有唯一编码
*   **相对位置关系**：能通过简单的线性变换表示位置偏移
*   **可扩展性**：能处理比训练时更长的序列

## 2. 为什么选择三角函数？

### 2.1 周期函数的优势

三角函数具有天然的周期性，可以：

*   自动学习位置之间的规律性关系
*   通过不同频率捕获多尺度位置模式
*   避免像整数编码那样产生巨大的数值差异

### 2.2 线性组合性质

关键数学性质：
对于任意偏移量`$k$`，存在线性变换矩阵`$M_k$`使得：

```math
PE_{pos+k} = M_k \cdot PE_{pos}
```

这个性质使模型能轻松学习相对位置关系

## 3. 波长变化的设计

### 3.1 指数衰减波长

频率项`$1/10000^{2i/d_{model}}$`的设计原因：

*   维度`$i$`越大，波长越长（频率越低）
*   波长范围从`$2\pi$`到`$10000·2\pi$`
*   形成几何级数：`$\lambda_i = 10000^{2i/d_{model}}·2\pi$`

可视化理解：

    维度i=0: 高频(短波) ━━━━━━━
    维度i=1: 中频       ━━━━━
    ...
    维度i=d/2-1: 低频(长波) ━━

### 3.2 为什么交替使用sin/cos？

交替维度的设计保证了：

*   每个位置编码是唯一的
*   可以表示任何位置偏移的线性变换
    数学上构成了一组正交基

## 4. 数学推导示例

设`$\omega_i = 1/10000^{2i/d_{model}}$`，证明相对位置可线性表示：

对于第`$i$`组维度（sin和cos维度）：

```math
\begin{aligned}
PE_{pos+k,2i} &= \sin(\omega_i(pos+k)) \\
&= \sin(\omega_i pos)\cos(\omega_i k) + \cos(\omega_i pos)\sin(\omega_i k) \\
&= PE_{pos,2i} \cdot \cos(\omega_i k) + PE_{pos,2i+1} \cdot \sin(\omega_i k)
\end{aligned}
```

同理：

```math
PE_{pos+k,2i+1} = \cos(\omega_i(pos+k)) = \cdots
```

因此可以用矩阵乘法表示：

```math
\begin{bmatrix}
PE_{pos+k,2i} \\
PE_{pos+k,2i+1}
\end{bmatrix}
= 
\begin{bmatrix}
\cos(\omega_i k) & \sin(\omega_i k) \\
-\sin(\omega_i k) & \cos(\omega_i k)
\end{bmatrix}
\begin{bmatrix}
PE_{pos,2i} \\
PE_{pos,2i+1}
\end{bmatrix}
```

## 5. 与旋转操作的关联

这个变换矩阵实际上是2D旋转矩阵！这意味着：

*   位置偏移相当于在二维平面上做旋转
*   旋转角度由`$\omega_i k$`决定
*   不同维度对应不同的旋转速度

## 6. 具体例子说明

假设`$d_{model}=4$`（实际中通常>=512），两个维度：

    位置1的编码：[sin(ω₀·1), cos(ω₀·1), sin(ω₁·1), cos(ω₁·1)]
    位置2的编码：[sin(ω₀·2), cos(ω₀·2), sin(ω₁·2), cos(ω₁·2)]

其中：

*   前两个维度是高频分量（ω₀较大）
*   后两个维度是低频分量（ω₁较小）

## 7. 为什么不是单一频率？

多频率组合的优势：

1.  高频分量捕捉局部位置关系
2.  低频分量捕捉长程位置关系
3.  类似傅里叶变换，多尺度表示更灵活

## 8. 与其他方案的对比

| 编码类型 | 优点          | 缺点       |
| ---- | ----------- | -------- |
| 正弦余弦 | 可扩展、可学习相对位置 | 需要设计频率参数 |
| 学习式  | 灵活适应数据      | 无法处理超长序列 |
| 相对编码 | 直接建模位置关系    | 计算复杂度高   |

## 9. 现代演进

后来的研究发现：

*   低频分量对模型性能更重要
*   高频分量有时可以裁剪
*   衍生出改进方案如：
    *   T5的相对位置偏置
    *   RoPE(旋转位置编码)
    *   ALiBi(基于距离的偏置)

需要我用更具体的数值例子演示计算过程吗？或者想了解如何在实际注意力机制中运用这些位置信息？

# 计算过程

好的！我将用一个 **具体的数值例子** 来详细解释正弦余弦位置编码的计算过程，并逐步拆解其数学原理。我们假设：

*   嵌入维度 `d_model = 4`（实际中通常为512或1024，这里简化以便计算）
*   位置 `pos = 0, 1, 2`（序列的前3个位置）
*   频率项的分母 `10000` 保持不变

***

## 1. 计算频率项

首先计算每个维度 `i` 对应的频率 `ω_i`：

```math
ω_i = \frac{1}{10000^{2i/d_{model}}}
```

因为 `d_model=4`，所以 `i ∈ [0, 1]`（共有 `d_model/2=2` 组 sin/cos 对）：

*   **i=0**（第0组维度）：
    ```math
    ω_0 = \frac{1}{10000^{0/4}} = 1
    ```
*   **i=1**（第1组维度）：
    ```math
    ω_1 = \frac{1}{10000^{2/4}} = \frac{1}{100}
    ```

***

## 2. 计算位置编码矩阵

位置编码公式：

```math
PE_{(pos,2i)} = \sin(pos \cdot ω_i) \\
PE_{(pos,2i+1)} = \cos(pos \cdot ω_i)
```

### 位置0 (`pos=0`)：

*   维度0 (`i=0`):
    *   `PE(0,0) = sin(0*1) = 0`
    *   `PE(0,1) = cos(0*1) = 1`
*   维度1 (`i=1`):
    *   `PE(0,2) = sin(0*0.01) = 0`
    *   `PE(0,3) = cos(0*0.01) = 1`\
        → 编码向量：`[0, 1, 0, 1]`

### 位置1 (`pos=1`)：

*   维度0 (`i=0`):
    *   `PE(1,0) = sin(1*1) ≈ 0.8415`
    *   `PE(1,1) = cos(1*1) ≈ 0.5403`
*   维度1 (`i=1`):
    *   `PE(1,2) = sin(1*0.01) ≈ 0.0100`
    *   `PE(1,3) = cos(1*0.01) ≈ 0.9999`\
        → 编码向量：`[0.8415, 0.5403, 0.0100, 0.9999]`

### 位置2 (`pos=2`)：

*   维度0 (`i=0`):
    *   `PE(2,0) = sin(2*1) ≈ 0.9093`
    *   `PE(2,1) = cos(2*1) ≈ -0.4161`
*   维度1 (`i=1`):
    *   `PE(2,2) = sin(2*0.01) ≈ 0.0200`
    *   `PE(2,3) = cos(2*0.01) ≈ 0.9998`\
        → 编码向量：`[0.9093, -0.4161, 0.0200, 0.9998]`

***

## 3. 位置编码矩阵结果

将上述结果整理为矩阵（行为位置，列为维度）：

| pos | dim=0  | dim=1   | dim=2  | dim=3  |
| --- | ------ | ------- | ------ | ------ |
| 0   | 0      | 1       | 0      | 1      |
| 1   | 0.8415 | 0.5403  | 0.0100 | 0.9999 |
| 2   | 0.9093 | -0.4161 | 0.0200 | 0.9998 |

***

## 4. 验证相对位置关系

**关键性质**：位置 `pos+k` 的编码可以通过 `pos` 的编码线性变换得到。\
以 `pos=1 → pos=2`（偏移 `k=1`）为例：

### 对第0组维度（i=0，ω₀=1）：

需要找到矩阵 `M` 使得：

```math
\begin{bmatrix}
PE_{2,0} \\ PE_{2,1}
\end{bmatrix}
= M \cdot 
\begin{bmatrix}
PE_{1,0} \\ PE_{1,1}
\end{bmatrix}
```

即：

```math
\begin{bmatrix}
\sin(2) \\ \cos(2)
\end{bmatrix}
= 
\begin{bmatrix}
a & b \\ c & d
\end{bmatrix}
\cdot
\begin{bmatrix}
\sin(1) \\ \cos(1)
\end{bmatrix}
```

根据三角函数的加法公式：

```math
\begin{aligned}
\sin(2) &= \sin(1+1) = \sin(1)\cos(1) + \cos(1)\sin(1) \\
\cos(2) &= \cos(1+1) = \cos(1)\cos(1) - \sin(1)\sin(1)
\end{aligned}
```

因此：

```math
M = 
\begin{bmatrix}
\cos(1) & \sin(1) \\
-\sin(1) & \cos(1)
\end{bmatrix}
≈ 
\begin{bmatrix}
0.5403 & 0.8415 \\
-0.8415 & 0.5403
\end{bmatrix}
```

验证计算：

```math
M \cdot 
\begin{bmatrix}
0.8415 \\ 0.5403
\end{bmatrix}
= 
\begin{bmatrix}
0.5403×0.8415 + 0.8415×0.5403 ≈ 0.9093 \\
-0.8415×0.8415 + 0.5403×0.5403 ≈ -0.4161
\end{bmatrix}
```

结果与 `PE_2` 的前两维一致！

***

## 5. 直观解释

1.  **高频维度（i=0）**：
    *   波长 `2π/ω₀ ≈ 6.28`（变化快）
    *   位置1→2时，角度变化大（1弧度→2弧度），编码值显著改变。

2.  **低频维度（i=1）**：
    *   波长 `2π/ω₁ ≈ 628`（变化慢）
    *   位置1→2时，角度变化小（0.01→0.02弧度），编码值几乎不变。

***

## 6. 为什么这样设计？

*   **多尺度性**：高频维度捕捉局部位置关系，低频维度捕捉全局关系。
*   **唯一性**：每个位置有唯一编码，且不同位置的编码线性无关。
*   **可扩展性**：即使序列长度超过训练时的最大值，位置编码仍有意义（因三角函数周期性）。

***

## 7. 代码验证

用PyTorch验证上述计算：

```python
import torch
import math

d_model = 4
pos = torch.tensor([0, 1, 2]).float().unsqueeze(1)
i = torch.arange(0, d_model, 2).float()

div_term = torch.exp(i * (-math.log(10000.0) / d_model))
pe = torch.zeros(3, d_model)
pe[:, 0::2] = torch.sin(pos * div_term)  # 偶数列是sin
pe[:, 1::2] = torch.cos(pos * div_term)  # 奇数列是cos
print(pe)
```

输出：

    tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],
            [ 0.8415,  0.5403,  0.0100,  0.9999],
            [ 0.9093, -0.4161,  0.0200,  0.9998]])

与手动计算结果一致！

# 位置编码与位置感知的深层机制

您提出了一个非常本质的问题：为什么看似非单调的位置编码能让模型理解顺序？这涉及Transformer架构的独特设计。以下是逐步解析：

***

## 1. 位置编码的"非单调性"本质

正弦余弦编码确实不是简单的单调递增函数，但通过**不同频率分量的组合**，每个位置仍获得唯一编码。关键观察：

*   **单个维度**：某个特定维度（如`$i=0$`）的sin值确实会先增后减
*   **整体编码**：所有维度的组合是唯一的

**示例对比**：

| 位置 | dim0 (sin) | dim1 (cos) | dim2 (sin) | dim3 (cos) | 整体编码                       |
| -- | ---------- | ---------- | ---------- | ---------- | -------------------------- |
| 1  | 0.8415     | 0.5403     | 0.0100     | 0.9999     | \[0.84, 0.54, 0.01, 1.00]  |
| 2  | 0.9093     | -0.4161    | 0.0200     | 0.9998     | \[0.91, -0.42, 0.02, 1.00] |
| 3  | 0.1411     | -0.9900    | 0.0300     | 0.9996     | \[0.14, -0.99, 0.03, 1.00] |

虽然单个维度有波动，但**4维向量的欧氏距离**能明确区分位置：

*   `$||PE_1-PE_2|| ≈ 1.02$`
*   `$||PE_1-PE_3|| ≈ 1.84$`

***

## 2. 模型如何学习位置关系

### 2.1 注意力机制的自动解析

当计算`$QK^T$`时，模型实际上在进行以下操作：

```math
\text{AttentionScore} = (\text{内容相似度}) + (\text{位置相关性})
```

其中位置相关性通过：

*   **绝对位置**：`$Q$`和`$K$`都携带了各自的位置编码
*   **相对位置**：通过三角函数的线性组合性质自动体现

### 2.2 相对位置的线性可表示性

对于任意偏移量`$k$`，存在变换矩阵`$M_k$`使得：

```math
PE_{pos+k} = M_k \cdot PE_{pos}
```

这意味着：

*   模型可以通过学习`$M_k$`来捕捉"距离为k"的关系
*   不同注意力头可以学习不同的`$M_k$`模式

**实验验证**：
在BERT的注意力头中，研究者发现：

*   约30%的头专门学习位置偏移模式
*   某些头实现"位置减法"：关注`$pos \pm k$`的位置

***

## 3. 为什么不是简单单调编码？

对比三种编码方案：

| 编码类型   | 示例              | 问题           | Transformer方案的优势           |
| ------ | --------------- | ------------ | -------------------------- |
| 线性递增   | \[1,2,3,...,n]  | 难以处理长序列/泛化   | 通过三角函数实现长度外推               |
| 二进制编码  | 001,010,011,... | 高维稀疏，不利于梯度传播 | 稠密向量适合神经网络处理               |
| 正弦余弦编码 | 如上表示            | 看似复杂         | 1) 唯一编码 2) 相对位置可学习 3) 平滑变化 |

***

## 4. 具体学习过程示例

假设模型需要学习"动词应该匹配主语的人称"：

1.  **输入序列**："She \[pos=1] eats \[pos=2] an apple"
2.  **关键注意力**：动词"eats"需要关注"She"
3.  **位置编码作用**：
    *   计算`PE_2 · PE_1`时会得到较高分数
    *   通过梯度下降，模型强化"pos1→pos2"的特定注意力模式
4.  **可视化**：
    eats的查询向量 → 学习到关注"距离=-1"的位置

***

## 5. 现代模型的改进方案

为解决原始位置编码的局限，新技术采用更直接的位置处理：

### 5.1 RoPE (Rotary Position Embedding)

*   **核心思想**：用旋转矩阵显式编码相对位置
*   **公式**：
    ```math
    \text{RoPE}(x_m, m) = \begin{bmatrix}
    \cos mθ & -\sin mθ \\
    \sin mθ & \cos mθ
    \end{bmatrix} \begin{bmatrix}
    x_{m,1} \\
    x_{m,2}
    \end{bmatrix}
    ```
*   **优势**：直接保持相对位置的几何关系

### 5.2 ALiBi (Attention with Linear Biases)

*   **方法**：直接添加与距离成比例的负偏置
    ```math
    \text{AttentionScore} = QK^T - m \cdot |i-j|
    ```
*   **效果**：更显式地强调局部性

***

## 6. 关键理解要点

1.  **位置信息不是靠单维单调性**，而是通过高维向量的唯一组合
2.  **模型通过注意力权重学习**如何利用这些位置信号
3.  **相对位置比绝对位置更重要**：三角函数的设计使模型更容易学习"距离k"的关系
4.  **可视化工具**：可以使用`bertviz`等工具观察注意力头如何利用位置信息

```python
# 使用bertviz观察位置注意力的示例
from bertviz import head_view
head_view(attention_weights, tokens=["She", "eats", "apple"])
```

输出示例中将显示某些注意力头呈现明显的对角线模式（位置敏感）。

***

### 总结：位置认知的神经网络实现路径

    位置编码设计
    ↓ (数学性质)
    唯一可区分的向量表示
    ↓ (注意力机制)
    模型学习关注有用的位置关系
    ↓ (训练过程)
    通过语言建模任务隐式学习：
       - 局部依赖（如形容词-名词）
       - 长程依赖（如主语-动词）
       - 顺序约束（如时间顺序）

这种设计使模型既能理解"dog bites man"和"man bites dog"的区别，又能捕捉"虽然相隔很远但语法相关"的词语关系。

# 理解位置关系的学习机制

您问到了一个非常核心的问题：模型如何通过学习变换矩阵`$M_k$`来捕捉位置关系。让我们用更直观的方式拆解这个过程：

***

## 1. 基本数学原理回顾

正弦余弦编码的关键性质：
对于任意固定偏移`$k$`，存在线性变换`$M_k$`使得：

```math
PE_{pos+k} = M_k \cdot PE_{pos}
```

其中`$M_k$`是一个与位置`$pos$`无关，仅与偏移量`$k$`有关的矩阵。

***

## 2. 具体学习过程分解

### 2.1 注意力机制中的位置信息流

当计算查询向量`$Q$`和键向量`$K$`的点积时：

```math
(QW_Q)(KW_K)^T = XW_QW_K^TX^T
```

由于`$X = \text{Content} + PE$`，展开后会出现四项：

1.  内容-内容交互
2.  内容-位置交互
3.  位置-内容交互
4.  **位置-位置交互** ← 这是我们关注的重点

### 2.2 位置交互项的数学本质

位置-位置交互项可表示为：

```math
PE_{pos_i}W_QW_K^TPE_{pos_j}^T
```

当`$W_QW_K^T$`被优化时，模型实际上是在学习：

*   如何将位置编码`$PE$`投影到一个空间，使得相关位置能产生高注意力分数
*   这个投影矩阵隐式地包含了各种`$M_k$`的变换模式

### 2.3 学习`$M_k$`的直观示例

假设两个位置`$i$`和`$j=i+3$`：

1.  模型需要学习"距离=3"的关系
2.  通过梯度下降，`$W_QW_K^T$`会被调整使得：
    ```math
    PE_{i+3} \approx (W_QW_K^T)PE_i
    ```
3.  这相当于隐式地学习到了`$M_3 \approx W_QW_K^T$`

***

## 3. 多头注意力的分工学习

不同注意力头会学习不同的位置关系模式：

| 头类型 | 学习到的`$M_k$`特性               | 应用场景      |
| --- | --------------------------- | --------- |
| 头1  | `$M_k$`对小的`$k$`有强响应         | 局部语法关系    |
| 头2  | `$M_k$`对特定`$k$`（如`$k=5$`）响应 | 固定距离依赖    |
| 头3  | `$M_k$`近似单位矩阵               | 忽略位置的内容匹配 |

***

## 4. 具体数学推导

以`$d_{model}=2$`的简化情况为例：

设位置编码为：

```math
PE_{pos} = [\sin(\omega pos), \cos(\omega pos)]
```

根据三角函数的加法公式：

```math
PE_{pos+k} = \begin{bmatrix}
\cos(\omega k) & \sin(\omega k) \\
-\sin(\omega k) & \cos(\omega k)
\end{bmatrix}
PE_{pos}
```

这里的旋转矩阵就是`$M_k$`。

在实际训练中：

*   模型通过`$W_Q$`和`$W_K$`的学习，会**自动发现**这种旋转变换模式
*   不同注意力头可能学习不同频率`$\omega$`对应的`$M_k$`

***

## 5. 现代模型的显式实现

在改进的架构中，`$M_k$`的学习更加直接：

### 5.1 RoPE (旋转位置编码)

直接定义`$M_k$`为旋转矩阵：

```python
def apply_rotary_pos(q, k, pos_ids):
    # q shape: [..., seq_len, dim]
    angle = pos_ids / 10000^(torch.arange(0, dim, 2)/dim)
    cos = torch.cos(angle)
    sin = torch.sin(angle)
    q_rot = q[..., 0::2] * cos - q[..., 1::2] * sin  # 旋转偶数维
    k_rot = k[..., 0::2] * cos - k[..., 1::2] * sin
    return q_rot, k_rot
```

### 5.2 T5的相对位置偏置

直接定义可学习的偏置矩阵：

```python
# 初始化时
self.rel_pos_bias = nn.Embedding(2*max_len-1, num_heads)

# 计算时
rel_pos = (pos[:, None] - pos[None, :]) + max_len - 1  # 转换为非负索引
bias = self.rel_pos_bias(rel_pos)  # [seq_len, seq_len, num_heads]
```

***

## 6. 学习过程的可视化理解

**训练初期**：

*   注意力分数随机分布
*   位置关系未被有效利用

**训练中期**：

*   某些头开始呈现对角线模式（关注`$k=0$`）
*   其他头出现带状模式（关注固定距离`$k$`）

**训练后期**：

*   不同头专门化：
    头1: ▁▁▃▅▆▇ (局部注意力)
    头2: ▇▁▁▁▁▇ (长程依赖)
    头3: ▅▅▅▅▅▅ (均匀分布)

***

## 7. 关键结论

1.  **隐式学习**：在原始Transformer中，`$M_k$`是通过`$W_QW_K^T$`的参数更新间接学习的
2.  **显式设计**：新架构直接构建的数学形式（如RoPE`$M_k$`的旋转矩阵）
3.  **多头分工**：不同注意力头学习不同距离`$k$`的变换模式
4.  **语言任务驱动**：通过预测下一个词等任务，迫使模型掌握有用的位置关系

这种设计使模型既能理解"狗咬人"和"人咬狗"的区别，又能捕捉跨越很远的语法依赖关系。
