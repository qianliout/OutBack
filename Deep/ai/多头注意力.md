多头注意力（Multi-Head Attention）是Transformer模型的核心组件，它通过**并行计算多组注意力**来捕捉输入数据的不同特征（如语义、位置、语法等）。下面用最直观的方式解释其原理和计算过程：

***

### **1. 核心思想**

*   **单头注意力的局限**：传统的注意力机制（如Seq2Seq中的注意力）只能学习一种特征交互模式。
*   **多头注意力的改进**：将输入拆分成多组（头），每组独立计算注意力，最后合并结果。相当于让模型**同时关注不同方面的信息**。

***

### **2. 计算过程分解**

假设输入序列为 `X`（形状 `[batch_size, seq_len, `num\_hiddens`]`），以下是详细步骤：

#### **步骤1：线性变换生成Q/K/V**

对输入 `X` 分别做三次线性变换，生成 **Query（Q）、Key（K）、Value（V）**，并拆分成 `h` 个头：

```python
# 输入X形状: [batch_size, seq_len, num_hiddens]
Q = W_Q * X  # [batch_size, seq_len, num_hiddens]
K = W_K * X  # [batch_size, seq_len, num_hiddens]
V = W_V * X  # [batch_size, seq_len, num_hiddens]

# 拆分成h个头（假设num_hiddens = h * d_k）
Q = reshape(Q, [batch_size, seq_len, h, d_k])  # [batch_size, seq_len, h, d_k]
K = reshape(K, [batch_size, seq_len, h, d_k])
V = reshape(V, [batch_size, seq_len, h, d_k])
```

*   `d_k`：每个头的维度（通常 `d_k = `num\_hiddens` / h`）。
*   `h`：头的数量（如Transformer中 `h=8`）。

#### **步骤2：缩放点积注意力（Scaled Dot-Product Attention）**

对每个头独立计算注意力：

```python
# 1. 计算Q和K的点积（相似度）
scores = Q @ K.transpose(-2, -1)  # [batch_size, h, seq_len, seq_len]

# 2. 缩放（防止梯度消失）
scores = scores / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))

# 3. Softmax归一化
weights = softmax(scores, dim=-1)  # [batch_size, h, seq_len, seq_len]

# 4. 加权求和V
output = weights @ V  # [batch_size, h, seq_len, d_k]
```

#### **步骤3：合并多头输出**

将多头的输出拼接后做线性变换：

```python
# 合并多头
output = reshape(output, [batch_size, seq_len, h * d_k])  # [batch_size, seq_len, num_hiddens]

# 线性变换
output = W_O * output  # [batch_size, seq_len, num_hiddens]
```

***

### **3. 直观示例**

假设输入句子是 **"The cat sat on the mat"**：

*   **头1**：关注**主语-动词关系**（如 "cat" → "sat"）。
*   **头2**：关注**位置信息**（如 "on" 与 "mat" 的相邻关系）。
*   **头3**：捕捉**指代关系**（如 "the" 修饰哪个名词）。

多头注意力会并行计算这些模式，最后合并结果。

***

### **4. 代码实现（PyTorch）**

```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, num_hiddens=512, h=8):
        super().__init__()
        self.d_k = num_hiddens // h
        self.h = h
        self.W_Q = nn.Linear(num_hiddens, num_hiddens)
        self.W_K = nn.Linear(num_hiddens, num_hiddens)
        self.W_V = nn.Linear(num_hiddens, num_hiddens)
        self.W_O = nn.Linear(num_hiddens, num_hiddens)

    def forward(self, X):
        batch_size = X.size(0)
        
        # 生成Q/K/V并分头
        Q = self.W_Q(X).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)  # [batch, h, seq_len, d_k]
        K = self.W_K(X).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        V = self.W_V(X).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)
        
        # 缩放点积注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(weights, V)  # [batch, h, seq_len, d_k]
        
        # 合并多头
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)
        return self.W_O(output)

# 测试
X = torch.randn(2, 10, 512)  # [batch_size=2, seq_len=10, num_hiddens=512]
mha = MultiHeadAttention()
output = mha(X)  # 输出形状: [2, 10, 512]
```

***

### **5. 为什么有效？**

1.  **并行捕捉多种模式**：不同头专注于不同特征，增强模型表达能力。
2.  **维度拆分降低计算量**：将 num\_hiddens 拆分为 `h` 个 `d_k`，计算复杂度从 `O(seq_len^2 * `num\_hiddens`)` 降至 `O(seq_len^2 * d_k)`。
3.  **残差连接+层归一化**：缓解梯度消失（实际实现中会添加）。

***

### **6. 与单头注意力的对比**

| 特性       | 单头注意力     | 多头注意力             |
| -------- | --------- | ----------------- |
| **计算模式** | 单一交互模式    | 多组并行模式            |
| **参数量**  | 较少        | 较多（线性变换矩阵更大）      |
| **表达能力** | 较弱        | 更强                |
| **典型应用** | 传统Seq2Seq | Transformer、BERT等 |

***

### **7. 常见问题**

**Q1：头的数量如何选择？**

*   通常取 `h=8`（如原始Transformer），但可根据任务调整。
*   num\_hiddens 必须能被 `h` 整除（保证 `d_k` 是整数）。

**Q2：多头注意力的计算开销大吗？**

*   是的，但通过拆分维度和并行计算优化后，实际效率较高。

**Q3：为什么需要缩放因子 `sqrt(d_k)`？**

*   防止点积结果过大导致Softmax梯度消失。

***

通过多头注意力，Transformer能够同时捕捉输入序列的多种依赖关系，这是其强大性能的关键！
