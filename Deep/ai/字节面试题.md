以下是整理的字节跳动大模型应用开发岗面试题列表，按图片顺序分类呈现：

***

### ​**​基础理论部分​**​

1.  目前主流的开源模型体系有哪些？
2.  Prefix Decoder、Causal Decoder和Encoder-Decoder的区别？
3.  大模型（LLM）的训练目标是什么？
4.  涌现能力的原因是什么？
5.  为何现在的大模型大部分是Decoder-only结构？
6.  简单介绍大模型（LLMs）？
7.  大模型名称后的175B、60B、540B等参数含义？
8.  大模型的优点？
9.  大模型的缺点？

***

### ​**​进阶问题​**​

1.  什么是生成式大模型？
2.  如何让生成的文本丰富而不单调？
3.  LLMs复读机问题

    *   3.1 什么是复读机问题？
    *   3.2 复读机问题出现的原因？
    *   3.3 缓解方法（如Unlikelihood Training、引入噪声、Repetition Penalty等）？
4.  LLaMA输入句子长度是否可以无限长？
5.  如何选择模型（Bert vs. LLaMA/ChatGLM）？
6.  各专业领域是否需要独立大模型？
7.  如何让大模型处理更长文本？

***

### ​**​微调相关​**​

1.  全参数微调所需显存计算？
2.  为什么SFT后模型表现变差？
3.  SFT指令微调数据如何构建？
4.  领域模型预训练数据如何选取？
5.  如何缓解领域训练后的通用能力下降？
6.  预训练阶段如何注入更多知识？
7.  SFT基座模型选Chat还是Base？
8.  领域模型微调的指令和数据格式要求？
9.  领域评测集如何构建？
10. 词表扩增是否必要？
11. 如何训练自己的大模型？
12. 训练中文大模型的经验？
13. 指令微调的好处？
14. 知识注入阶段（预训练 vs. 微调）？
15. 学习领域知识应选择预训练还是微调？
16. 多轮对话任务如何微调？
17. 微调后能力劣化（灾难性遗忘）的原因？

***

### ​**​一面（90分钟）​**​

1.  自我介绍
2.  对DeepSeek-R1的了解？
3.  R1的MLA如何实现KV-Cache节约？
4.  R1在SFT时冷启动的目的？
5.  位置编码（如RoPE）的原理？
6.  14B模型推理和训练的显存需求？
7.  显存占用的影响因素？
8.  灾难性遗忘及解决方法？
9.  BF16、FP16、FP32的区别？
10. Adam/AdamW优化器原理？
11. DeepSpeed的三个阶段？

***

### ​**​二面（60分钟）​**​

1.  手写Attention公式（代码实现）
2.  训练和推理时mask的作用与区别？推理sequence length与训练的关系？
3.  Transformer与传统Seq2Seq模型的区别？
4.  Attention计算中除以√dk的原因？其他Scale处理方式？
5.  Softmax的作用、公式、数值稳定性处理（如减最值）？能否替换其他函数？
6.  数值上溢/下溢问题？
7.  Prompt书写经验及优化方法？
8.  SFT的Loss计算部分及原因？
9.  DeepSpeed的原理？
10. 训练7B模型的显存占用？不同Zero阶段节省显存的效果？
11. 训练使用的卡数及异常问题？

***

整理自图片内容，问题覆盖模型理论、微调实践、工程实现及优化细节。
