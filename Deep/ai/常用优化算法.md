## **深度学习常用优化算法总结**

***

### **1. 梯度下降法（Gradient Descent, GD）**

*   **基本思想**：沿负梯度方向更新参数，最小化损失函数。

*   **参数更新公式**：
    ```math
    \theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t)
    ```
    *   `$\eta$`：学习率
    *   `$\nabla_\theta J(\theta_t)$`：损失函数梯度

*   **特点**：
    *   计算所有样本的梯度（**计算成本高**）。
    *   适用于小数据集或凸优化问题。

***

### **2. 随机梯度下降（Stochastic Gradient Descent, SGD）**

*   **改进点**：每次随机选取**一个样本**计算梯度。

*   **参数更新公式**：
    ```math
    \theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t; x_i, y_i)
    ```

*   **特点**：
    *   更新频繁，收敛快但**波动大**。
    *   可能陷入局部最优。

*   **代码示例（PyTorch）**：
    ```python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    ```

***

### **3. 小批量梯度下降（Mini-batch SGD）**

*   **改进点**：折中方案，每次用**一小批样本**（batch）计算梯度。
*   **特点**：
    *   平衡计算效率和稳定性（**最常用**）。
    *   Batch大小是超参数（通常32/64/128）。

***

### **4. 动量优化（Momentum）**

*   **核心思想**：引入动量项（历史梯度加权平均）加速收敛。

*   **参数更新公式**：
    ```math
    v_{t+1} = \gamma v_t + \eta \nabla_\theta J(\theta_t) \\
    \theta_{t+1} = \theta_t - v_{t+1}
    ```
    *   `$\gamma$`：动量系数（通常0.9）

*   **特点**：
    *   减少震荡，加速收敛（尤其对高曲率区域）。
    *   可逃离局部最优。

*   **代码示例**：
    ```python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    ```

***

### **5. AdaGrad（Adaptive Gradient）**

*   **核心思想**：自适应调整学习率，对稀疏特征加大更新。

*   **参数更新公式**：
    ```math
    G_t = G_{t-1} + (\nabla_\theta J(\theta_t))^2 \\
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta J(\theta_t)
    ```
    *   `$G_t$`：梯度平方的累积
    *   `$\epsilon$`：平滑项（防止除零，如1e-8）

*   **特点**：
    *   适合稀疏数据（如NLP）。
    *   学习率单调下降，可能**过早停止学习**。

***

### **6. RMSProp**

*   **改进点**：解决AdaGrad学习率急剧下降问题，引入衰减系数。

*   **参数更新公式**：
    ```math
    G_t = \beta G_{t-1} + (1-\beta)(\nabla_\theta J(\theta_t))^2 \\
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_\theta J(\theta_t)
    ```
    *   `$\beta$`：衰减率（通常0.9）

*   **特点**：
    *   自适应学习率，适合非平稳目标（如RNN）。

*   **代码示例**：
    ```python
    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)
    ```

***

### **7. Adam（Adaptive Moment Estimation）**

*   **核心思想**：结合动量（一阶矩）和RMSProp（二阶矩）。

*   **参数更新公式**：
    ```math
    m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla_\theta J(\theta_t) \quad \text{(一阶矩)} \\
    v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla_\theta J(\theta_t))^2 \quad \text{(二阶矩)} \\
    \hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \quad \text{(偏差修正)} \\
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
    ```
    *   `$\beta_1=0.9$`, `$\beta_2=0.999$`（默认值）

*   **特点**：
    *   **最常用**，适应不同数据分布。
    *   需调参学习率`$\eta$`。

*   **代码示例**：
    ```python
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
    ```

***

### **8. 其他优化算法**

| 算法           | 核心思想                 | 适用场景        |
| ------------ | -------------------- | ----------- |
| **AdaDelta** | 改进RMSProp，无需初始学习率    | 超参数敏感任务     |
| **Nadam**    | Adam + Nesterov动量    | 需要更稳定收敛的任务  |
| **Lion**     | 符号动量+自适应学习率（2023新算法） | 大模型训练（如LLM） |

***

## **总结与选择建议**

| 算法               | 优点            | 缺点             | 推荐场景      |
| ---------------- | ------------- | -------------- | --------- |
| **SGD**          | 简单，理论收敛性好     | 需手动调学习率，易震荡    | 凸优化、小数据集  |
| **SGD+Momentum** | 加速收敛，减少震荡     | 需调动量系数         | 深度网络通用    |
| **Adam**         | 自适应学习率，默认表现好  | 可能过拟合          | 大多数深度学习任务 |
| **RMSProp**      | 适合非平稳目标（如RNN） | 对稀疏数据不如AdaGrad | 循环神经网络    |

**实际建议**：

*   **默认尝试Adam**（快速收敛，调参简单）。
*   **追求极致性能**：用SGD+Momentum配合学习率调度器。
*   **大模型训练**：尝试Lion或AdaFactor。

**代码模板（PyTorch）**：

```python
# 常用优化器初始化
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 配合学习率调度器
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
```

以下是深度学习中 **10 个核心优化算法** 的详细解析，包含公式拆解、应用场景、优缺点对比及代码示例，适合系统学习：

***

## 1. **随机梯度下降 (SGD)**

*   **公式**：
    ```math
    \theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta_t)
    ```
    *   `$(\theta_t)$`: 第 (t) 步的参数
    *   (\eta): 学习率
    *   (\nabla_\theta J(\theta_t)): 损失函数梯度
*   **特点**：
    *   **优点**：实现简单，内存占用低
    *   **缺点**：学习率固定，易陷入局部最优
*   **适用场景**：
    *   小型数据集训练
    *   需要精细调参的任务（如SVM）
*   **图示**：\
    ![SGD](https://miro.medium.com/v2/resize\:fit:1400/1*_kY-7KQKZ4tQ8V7U6Bz5Zw.gif)

***

## 2. **带动量的SGD (Momentum)**

*   **公式**：
    ```math
    v_{t+1} = \gamma v_t + \eta \cdot \nabla_\theta J(\theta_t) \\
    \theta_{t+1} = \theta_t - v_{t+1}
    ```
    *   (\gamma): 动量系数（通常0.9）
    *   (v_t): 累积梯度动量
*   **特点**：
    *   **优点**：加速收敛，减少震荡
    *   **缺点**：需手动调整动量参数
*   **适用场景**：
    *   高维非凸优化（如ResNet训练）
*   **物理类比**：\
    如同小球滚下山坡，动量使其越过局部洼地

***

## 3. **Nesterov加速梯度 (NAG)**

*   **公式**：
    ```math
    v_{t+1} = \gamma v_t + \eta \cdot \nabla_\theta J(\theta_t - \gamma v_t) \\
    \theta_{t+1} = \theta_t - v_{t+1}
    ```
*   **关键改进**：\
    在动量方向提前计算梯度（"向前看"）
*   **优点**：比Momentum更精准收敛
*   **缺点**：计算复杂度略高
*   **适用场景**：\
    GAN训练、对抗样本生成

***

## 4. **AdaGrad**

*   **公式**：
    ```math
    G_t = G_{t-1} + (\nabla_\theta J(\theta_t))^2 \\
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot \nabla_\theta J(\theta_t)
    ```
    *   (G_t): 历史梯度平方和
    *   (\epsilon): 平滑项（通常1e-8）

*   **特点**：
    *   **优点**：自动调整学习率，适合稀疏数据
    *   **缺点**：学习率单调递减导致早停
*   **适用场景**：\
    NLP任务（如Word2Vec）

***

## 5. **RMSProp**

*   **公式**：
    ```math
    E[g^2]_t = \beta E[g^2]_{t-1} + (1-\beta)(\nabla_\theta J(\theta_t))^2 
    ```
    ```math
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \cdot \nabla_\theta J(\theta_t)
    ```
    *   (\beta): 衰减率（通常0.9）

*   **改进点**：\
    引入指数加权平均，解决AdaGrad激进衰减问题
*   **适用场景**：\
    RNN/LSTM时序模型训练

***

## 6. **Adam**

*   **公式**：
    ```math
    m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla_\theta J(\theta_t) \quad \text{(一阶矩)} \\
    v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla_\theta J(\theta_t))^2 \quad \text{(二阶矩)} \\
    \hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \\
    \theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
    ```
    *   (\beta_1=0.9, \beta_2=0.999)

*   **特点**：
    *   **优点**：自适应学习率，默认参数效果好
    *   **缺点**：可能收敛到次优点
*   **适用场景**：\
    绝大多数CNN/Transformer任务

***

## 7. **AdamW**

*   **公式**：
    ```math
    \theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)
    ```
    *   (\lambda): 权重衰减系数
*   **改进点**：\
    解耦权重衰减与梯度更新
*   **适用场景**：\
    图像分类（如ViT）、大模型预训练

***

## 8. **Nadam**

*   **公式**：\
    Adam + Nesterov动量\\
    ```math
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} (\beta_1 \hat{m}_t + \frac{(1-\beta_1)\nabla_\theta J(\theta_t)}{1-\beta_1^t})
    ```
*   **优点**：结合Adam自适应与NAG快速收敛
*   **适用场景**：\
    需要快速收敛的任务（如目标检测）

***

## 9. **Lion (Google 2023)**

*   **公式**：
    ```math
    m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla_\theta J(\theta_t) \\
    \theta_{t+1} = \theta_t - \eta \cdot \text{sign}(m_t)
    ```
*   **创新点**：\
    用符号函数替代动量更新
*   **优点**：内存占用更低
*   **适用场景**：\
    LLM预训练（如PaLM 2）

***

## 10. **Lookahead**

*   **公式**：
    ```math
    \text{快速权重更新：} \theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t) \\
    \text{慢速权重更新：} \Theta_{t+k} = \Theta_t + \alpha (\theta_{t+k} - \Theta_t)
    ```
    *   (\alpha): 慢速权重更新系数

*   **特点**：\
    双循环机制提升稳定性
*   **适用场景**：\
    小批量数据下的鲁棒训练

***

## **对比总结表**

| 优化器      | 自适应学习率 | 动量 | 内存占用 | 适用场景     |
| -------- | ------ | -- | ---- | -------- |
| SGD      | ❌      | ❌  | 低    | 小数据集     |
| Momentum | ❌      | ✔️ | 中    | CNN/RNN  |
| Adam     | ✔️     | ✔️ | 高    | 通用任务     |
| AdamW    | ✔️     | ✔️ | 高    | 图像分类/大模型 |
| Lion     | ❌      | ✔️ | 低    | 大规模预训练   |

***

## **代码示例（PyTorch）**

```python
# 常用优化器初始化
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))
# 或使用AdamW（推荐）
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)

# 配合学习率调度器
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
```

***

## **学习建议**

1.  **默认选择**：从AdamW开始尝试
2.  **调参重点**：学习率 > 动量参数 > 其他
3.  **进阶技巧**：
    *   配合Warmup策略（前10%训练逐步提高学习率）
    *   使用梯度裁剪（`clip_grad_norm_`）避免爆炸

通过理解这些优化器的数学本质，可以更灵活地应对不同训练场景！
