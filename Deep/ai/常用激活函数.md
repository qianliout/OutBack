# 深度学习中的激活函数

## 激活函数概述

激活函数是神经网络中的非线性变换，为模型引入非线性能力，使其可以学习复杂模式。数学表示为：

```math
y = f(z)
```

其中 `$z$` 是输入信号（通常是线性组合 `$\sum w_i x_i + b$`）

## 常用激活函数详解

## 1. Sigmoid（Logistic函数）

### 数学表达式

```math
\sigma(z) = \frac{1}{1 + e^{-z}}
```

### 特点

*   将输入压缩到 (0,1) 区间
*   早期神经网络常用

### 优点

1.  输出可解释为概率
2.  平滑梯度

### 缺点

1.  容易导致梯度消失（当 `$|z|$` 较大时梯度接近0）
2.  输出不以0为中心
3.  指数运算计算成本较高

### 使用场景

*   二分类问题的输出层
*   需要概率解释的场合

## 2. Tanh（双曲正切函数）

### 数学表达式

```math
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
```

### 特点

*   输出范围 (-1,1)
*   比sigmoid更常用

### 优点

1.  输出以0为中心
2.  梯度比sigmoid更强

### 缺点

1.  仍然存在梯度消失问题

### 使用场景

*   RNN/LSTM等循环网络
*   隐藏层的激活

## 3. ReLU（Rectified Linear Unit）

### 数学表达式

```math
\text{ReLU}(z) = \max(0, z)
```

### 特点

*   当前最常用的激活函数
*   计算简单高效

### 优点

1.  计算效率高（无需指数运算）
2.  缓解梯度消失问题（正区间梯度为1）
3.  促进稀疏激活

### 缺点

1.  "Dying ReLU"问题（负输入时梯度为0）
2.  输出不以0为中心

### 使用场景

*   CNN和大多数前馈神经网络的隐藏层
*   计算机视觉任务

## 4. Leaky ReLU

### 数学表达式

```math
\text{LeakyReLU}(z) = \begin{cases} 
z & \text{if } z > 0 \\
\alpha z & \text{if } z \leq 0 
\end{cases}
```

（通常 `$\alpha=0.01$`）

### 特点

*   ReLU的改进版

### 优点

1.  解决Dying ReLU问题
2.  保持ReLU的大部分优点

### 缺点

1.  需要手动设置或学习 `$\alpha$` 参数

### 使用场景

*   当担心ReLU神经元"死亡"时
*   GAN等对抗性网络

## 5. Softmax

### 数学表达式

```math
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \quad \text{对于} j=1,...,K
```

### 特点

*   多类分类的标准选择

### 优点

1.  输出可解释为概率分布
2.  适合多分类问题

### 缺点

1.  对极端值敏感
2.  计算成本较高

### 使用场景

*   多分类问题的输出层

## 选择建议

1.  **隐藏层**：优先尝试ReLU或其变体（Leaky ReLU等）
2.  **二分类输出层**：Sigmoid
3.  **多分类输出层**：Softmax
4.  **RNN/LSTM**：Tanh或Sigmoid

## 高级变体（补充）

## Swish（Google提出）

```math
\text{Swish}(z) = z \cdot \sigma(\beta z)
```

*   结合了ReLU和Sigmoid的特性
*   在深层网络中表现优于ReLU

## GELU（高斯误差线性单元）

```math
\text{GELU}(z) = z \Phi(z)
```

其中 `$\Phi(z)$` 是标准正态分布的累积分布函数

*   被BERT、GPT等Transformer模型采用

需要了解哪个激活函数的更多细节，或者具体应用场景的选择建议吗？
