在 RAG（Retrieval-Augmented Generation）系统中，**文档的分割（Chunking）是影响检索效果的关键步骤**。合理的分割能平衡上下文完整性与检索精度。以下是详细的分割方法、算法及实践策略：

***

## **1. 文档分割的核心目标**

*   **保留语义完整性**：确保每个分块包含独立语义单元（如完整段落）。
*   **控制分块大小**：适配向量模型的上下文窗口（如 OpenAI 的 text-embedding-ada-002 支持 8192 tokens）。
*   **避免信息割裂**：防止关键信息被截断（如问答对分离）。

***

## **2. 常用分割算法与策略**

### **(1) 固定长度分割（Fixed-Size Chunking）**

*   **原理**：按固定 token 数或字符数分割，简单高效。
*   **适用场景**：结构化文档（如代码、日志）或长度均匀的文本。
*   **实现示例**：
    ```python
    from langchain.text_splitter import RecursiveCharacterTextSplitter

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # 每块最大字符数
        chunk_overlap=50,  # 块间重叠字符数
        separators=["\n\n", "\n", "。", "？", "！", "；", "，", " "]  # 中文优先分割符
    )
    chunks = splitter.split_text(document)
    ```

### **(2) 基于语义的分割（Semantic Chunking）**

*   **原理**：利用 NLP 模型检测语义边界（如句子重要性、主题变化）。
*   **算法**：
    *   **TextTiling**：通过词频变化检测主题边界。
    *   **BERTopic**：聚类相似句子，按主题分块。
*   **工具示例**：
    ```python
    from semantic_text_splitter import TextSplitter

    splitter = TextSplitter.from_huggingface_tokenizer("bert-base-chinese", max_tokens=500)
    chunks = splitter.chunks(document)
    ```

### **(3) 递归分割（Recursive Splitting）**

*   **原理**：按分隔符优先级递归分割（先按段落，再按句子）。
*   **优势**：适应非结构化文本（如混合中英文的网页内容）。
*   **LangChain 实现**：
    ```python
    splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
        tokenizer,
        chunk_size=300,
        chunk_overlap=30,
        separators=["\n\n", "\n", "。", "？", "！", "；", "，", " "]
    )
    ```

### **(4) 基于标记的分割（Marker-Based Chunking）**

*   **原理**：根据特定标记（如 HTML 标签、Markdown 标题）分割。
*   **适用场景**：格式化文档（如维基百科、Jupyter Notebook）。
*   **示例**：
    ```python
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html_doc, 'html.parser')
    chunks = [p.get_text() for p in soup.find_all('p')]  # 按HTML段落分割
    ```

***

## **3. 中文文本分割的特殊处理**

### **(1) 分词增强**

结合 jieba 分词确定分割边界：

```python
import jieba
def chinese_aware_split(text, max_len=300):
    words = list(jieba.cut(text))
    chunks = []
    current_chunk = []
    current_len = 0
    
    for word in words:
        if current_len + len(word) > max_len:
            chunks.append("".join(current_chunk))
            current_chunk = [word]
            current_len = len(word)
        else:
            current_chunk.append(word)
            current_len += len(word)
    
    if current_chunk:
        chunks.append("".join(current_chunk))
    return chunks
```

### **(2) 标点优先级**

中文优先分割符：`["\n\n", "\n", "。", "？", "！", "；", "……", "，"]`

***

## **4. 分块大小与重叠的优化建议**

| **场景** | **推荐分块大小**      | **重叠大小** | **理由**      |
| ------ | --------------- | -------- | ----------- |
| 技术文档   | 500-800 tokens  | 50-100   | 保持完整代码段/公式  |
| 新闻/百科  | 300-500 tokens  | 30-50    | 段落独立性高      |
| 对话记录   | 200-300 tokens  | 20-30    | 单轮对话较短      |
| 法律/合同  | 600-1000 tokens | 100-150  | 长句多，需保留完整条款 |

***

## **5. 分块质量评估方法**

### **(1) 人工检查**

*   随机抽样分块，验证语义完整性。
*   **检查点**：
    *   是否截断核心实体（如人名、术语）？
    *   分块是否包含完整问答对？

### **(2) 向量相似度测试**

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

def evaluate_chunk_continuity(chunks):
    embeddings = model.encode(chunks)
    scores = []
    for i in range(len(chunks)-1):
        score = np.dot(embeddings[i], embeddings[i+1])
        scores.append(score)
    return np.mean(scores)  # 越高表示连续性越好
```

### **(3) 检索效果指标**

*   **Chunk Hit Rate**：测试查询命中相关分块的比例。
*   **Answer Coverage**：生成答案对分块内容的覆盖度。

***

## **6. 高级优化技巧**

### **(1) 动态分块（Dynamic Chunking）**

*   **原理**：根据内容复杂度调整分块大小。
*   **实现**：
    ```python
    def dynamic_chunk(text, min_size=200, max_size=600):
        sentences = text.split('。')
        chunks = []
        current_chunk = []
        current_len = 0
        
        for sent in sentences:
            sent_len = len(sent)
            if current_len + sent_len > max_size and current_len >= min_size:
                chunks.append("。".join(current_chunk) + "。")
                current_chunk = [sent]
                current_len = sent_len
            else:
                current_chunk.append(sent)
                current_len += sent_len
        
        if current_chunk:
            chunks.append("。".join(current_chunk))
        return chunks
    ```

### **(2) 分块元数据增强**

为每个分块添加上下文信息：

```python
chunks_with_meta = [{
    "text": chunk,
    "source": "doc1.pdf",
    "section": "第三章",
    "prev_chunk": "...",  # 前一块的摘要
    "next_chunk": "..."   # 后一块的摘要
} for chunk in chunks]
```

***

## **7. 工具推荐**

| **工具**                     | **特点**                     | **适用场景**       |
| -------------------------- | -------------------------- | -------------- |
| **LangChain**              | 内置多种分割器，支持递归分割             | 快速原型开发         |
| **Semantic Text Splitter** | 基于语义边界检测                   | 学术论文/技术文档      |
| **BeautifulSoup**          | HTML/XML 结构化分割             | 网页内容提取         |
| **Tiktoken**               | 精准计算 token 数（适配 OpenAI 模型） | 需要严格控制 token 数 |

***

## **总结：最佳实践流程**

1.  **预处理**：清洗文档（去噪音、标准化格式）。
2.  **分割实验**：尝试不同算法和参数，评估分块质量。
3.  **向量化测试**：检查分块嵌入的语义一致性。
4.  **检索验证**：通过真实查询测试命中率。
5.  **迭代优化**：根据反馈调整分割策略。

通过合理分割，RAG 系统能更精准地检索到相关上下文，显著提升生成答案的质量。
