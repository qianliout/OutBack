# 1. Transformer 中的标签平滑损失函数（Label Smoothing）

在 Transformer 模型中，标签平滑（Label Smoothing）通常应用于**交叉熵损失函数**（Cross-Entropy Loss），目的是缓解模型对训练标签的过度自信（over-confidence），提高泛化能力。

## 1.1 数学定义

标准交叉熵损失函数（无标签平滑）：

```math
L_{\text{CE}} = -\sum_{i=1}^C y_i \log(p_i)
```

其中：

*   `$C$` 是类别数，
*   `$y_i$` 是真实标签的 one-hot 编码（`$y_i=1$` 对目标类，其余为 0），
*   `$p_i$` 是模型预测的类别概率。

**加入标签平滑后**，真实标签被调整为软标签：

```math
y_i^{\text{smooth}} = 
\begin{cases} 
1 - \epsilon & \text{如果 } i \text{ 是目标类}, \\
\frac{\epsilon}{C-1} & \text{否则},
\end{cases}
```

其中 `$\epsilon$` 是平滑系数（通常取 0.1 或 0.2）。损失函数变为：

```math
L_{\text{LS}} = -\sum_{i=1}^C y_i^{\text{smooth}} \log(p_i)
```

***

## 1.2 原理

1.  **防止过度自信**：\
    标准交叉熵会迫使模型将目标类的概率预测为接近 1，导致对错误标注或模糊样本过于敏感。标签平滑通过分配少量概率给非目标类，避免模型输出极端概率。

2.  **正则化效果**：\
    相当于对模型预测的分布加入噪声，鼓励参数更新更平滑，类似 L2 正则化。

3.  **校准模型置信度**：\
    在测试时，模型输出的概率会更接近真实置信度（例如，预测概率 0.8 的实际准确率也接近 80%）。

***

## 1.3 在 Transformer 中的作用

1.  **机器翻译（如原始论文应用）**：\
    Transformer 的 decoder 需要预测下一个词，标签平滑可缓解对高频词的过拟合，提升对低频词的泛化能力。

2.  **自回归生成任务**：\
    避免模型因过度自信而陷入重复生成或错误累积（exposure bias）。

3.  **多分类任务（如文本分类）**：\
    对相似类别（如情感极性中的 "中性" 和 "略微正面"）的边界更鲁棒。

***

## 1.4 代码示例（PyTorch）

```python
import torch.nn as nn

# 使用 PyTorch 内置的标签平滑交叉熵
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

# 手动实现
def label_smoothing_loss(pred, target, epsilon=0.1):
    C = pred.size(-1)
    log_pred = torch.log_softmax(pred, dim=-1)
    smooth_target = torch.full_like(pred, epsilon/(C-1))
    smooth_target.scatter_(1, target.unsqueeze(1), 1-epsilon)
    loss = (-smooth_target * log_pred).sum(dim=-1).mean()
    return loss
```

***

# 2. 标签平滑的优缺点

## 2.1 优点

*   提升模型泛化性（尤其在数据噪声较多时）。
*   减少过拟合风险（类似 dropout 的效果）。
*   使模型概率输出更合理（校准性）。

## 2.2 缺点

*   可能降低训练速度（需要更多 epoch 收敛）。
*   对某些任务（如对抗训练）可能不适用。

***

## 3. 扩展思考

*   **与 KL 散度的关系**：标签平滑等价于最小化预测分布与均匀分布之间的 KL 散度。
*   **参数选择**：`$\epsilon$` 通常取 0.1，但需根据任务调整（数据噪声大时可增大）。

如果需要更具体的 Transformer 应用细节（如 BERT、GPT 中的实现差异），可以进一步讨论！
