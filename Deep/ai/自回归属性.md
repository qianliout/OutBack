自回归（Autoregressive）是序列生成模型（如Transformer的解码器部分）的核心属性，指模型在生成序列时**逐步**、**单向**地产生输出，且每一步的预测依赖于之前已生成的输出。具体定义和特点如下：

***

# 1. 数学定义

对于一个序列 `$y = (y_1, y_2, ..., y_T)$`，自回归模型将联合概率分解为**条件概率的链式乘积**：

```math
p(y) = \prod_{t=1}^T p(y_t \mid y_1, y_2, ..., y_{t-1})
```

*   每一步 `$y_t$` 的生成依赖于前面所有步骤的输出 `$y_{<t}$`。
*   在Transformer中，这一特性通过**掩码注意力**（Masked Attention）实现。

***

# 2. 关键特点

*   **单向性**：生成时只能从左到右（或从右到左），无法同时访问未来信息。
    *   例如：GPT 是典型的自回归模型。
*   **逐步生成**：类似人类逐字写作的过程，每一步基于历史生成一个新 token。
*   **依赖掩码**：通过注意力掩码（如 `masked_fill`）阻止解码器看到未来位置：
    ```math
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
    ```
    其中 `$M$` 是下三角掩码矩阵（`$M_{ij} = -\infty \text{ if } i < j$`）。

***

# 3. 与非自回归模型（Non-Autoregressive）对比

| 特性   | 自回归 (AR)  | 非自回归 (NAR)                                 |
| ---- | --------- | ------------------------------------------ |
| 生成方式 | 逐步生成      | 并行生成所有输出                                   |
| 速度   | 慢（需串行）    | 快（一次完成）                                    |
| 依赖关系 | 强（显式建模依赖） | 弱（假设输出间独立）                                 |
| 典型模型 | GPT, LSTM | BERT, NAT (Non-Autoregressive Transformer) |

***

### 4. Transformer 中的实现

*   **编码器**：非自回归（可双向处理输入，如 BERT）。
*   **解码器**：自回归（如 GPT 或机器翻译的解码器），通过以下机制实现：
    1.  **训练时**：使用教师强制（Teacher Forcing），输入是真实序列的右移版本（`shifted right`）。
    2.  **推理时**：迭代生成，每一步将当前输出作为下一步输入。

***

# 5. 优缺点

*   **优点**：生成质量高（显式建模依赖关系），适合长文本生成。
*   **缺点**：生成速度慢（无法并行），错误会累积（Exposure Bias）。

# **自回归属性如何通过掩码注意力（Masked Attention）实现**

好的！这里的关键在于理解\*\*“自回归属性如何通过掩码注意力（Masked Attention）实现”\*\*。我们结合Transformer解码器的具体机制分步说明：

***

## 1. 自回归的核心要求

自回归生成必须满足：\
**“生成第`$t$`个token时，只能看到`$1$`到`$t-1$`的token，不能看到`$t$`及之后的token”**。\
例如生成序列`["我", "爱", "你"]`时：

*   预测`"爱"`时只能使用`"我"`；
*   预测`"你"`时只能使用`"我", "爱"`。

***

## 2. 掩码注意力的作用

Transformer的解码器通过**掩码注意力**强制实现这一限制。具体步骤：

### （1）注意力分数的计算

在标准注意力中，查询`$Q$`和键`$K$`的点积计算所有位置的相似度：

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
```

**问题**：这样会同时计算当前token与所有未来token的关联（违反自回归）。

### （2）加入掩码矩阵`$M$`

通过一个下三角掩码矩阵`$M$`（元素值定义如下）：

```math
M_{ij} = \begin{cases} 
0, & \text{if } i \geq j \ (允许访问过去或当前位置) \\
-\infty, & \text{if } i < j \ (禁止访问未来位置)
\end{cases}
```

将`$M$`加到注意力分数上：

```math
\text{MaskedAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
```

*   **效果**：未来位置的注意力权重被压制成`$0$`（因为`$e^{-\infty}=0$`），当前token只能关注过去token。

***

### 3. 具体例子

假设正在生成序列`["A", "B", "C"]`，解码器的自回归过程如下：

| 生成步骤    | 已生成序列             | 可访问的位置  | 掩码矩阵`$M$`（3×3）                                                                                                        |
| ------- | ----------------- | ------- | --------------------------------------------------------------------------------------------------------------------- |
| `$t=1$` | `["A"]`           | 仅位置1    | `$\begin{bmatrix} 0 & -\infty & -\infty \\ -\infty & -\infty & -\infty \\ -\infty & -\infty & -\infty \end{bmatrix}$` |
| `$t=2$` | `["A", "B"]`      | 位置1和2   | `$\begin{bmatrix} 0 & -\infty & -\infty \\ 0 & 0 & -\infty \\ -\infty & -\infty & -\infty \end{bmatrix}$`             |
| `$t=3$` | `["A", "B", "C"]` | 位置1,2,3 | `$\begin{bmatrix} 0 & -\infty & -\infty \\ 0 & 0 & -\infty \\ 0 & 0 & 0 \end{bmatrix}$`                               |

***

## 4. 可视化说明

以生成第3个token (`"C"`)为例：

*   **未掩码的注意力**：可以同时看到`"A"`, `"B"`, `"C"`（违反自回归）。
*   **掩码后的注意力**：只能看到`"A"`和`"B"`，对`"C"`本身的注意力权重为`$0$`。

![Masked Attention](https://i.imgur.com/6Hj5F4W.png)\
（图中灰色部分表示被掩码的未来位置）

***

### 5. 代码模拟

用PyTorch风格的伪代码说明掩码的实现：

```python
# 假设seq_len=3, 注意力分数矩阵scores (3x3)
scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
mask = torch.tril(torch.ones(seq_len, seq_len))  # 下三角矩阵
mask = mask.masked_fill(mask == 0, float('-inf'))  # 下三角外设为-inf
scores = scores + mask  # 未来位置被掩码
weights = softmax(scores, dim=-1)  # 未来位置的权重=0
output = torch.matmul(weights, V)
```

***

### 总结

*   **掩码注意力**是Transformer实现自回归的关键：通过数学手段（`$-\infty$`掩码）强制模型在生成时仅依赖历史信息。
*   **类比**：类似人类写作时的“只能看到已经写出的内容，不能提前看到未写的内容”。



