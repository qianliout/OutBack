## 随机梯度下降计算过程

在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法sgd来更新模型参数&#x20;

以下是具体理解

好的！我将结合具体数据和代码步骤，详细解释深度学习训练中的一个完整迭代过程（以线性回归为例）。你会看到**数据如何流动、梯度如何计算、参数如何更新**。

***

### **1. 问题设定**

假设我们有一个简单的线性回归模型：

```math
\hat{y} = w x + b
```

其中：

*   真实数据生成规则：`$y = 2x + 1 + \epsilon$`（`$\epsilon$` 是噪声）
*   目标：通过训练学习参数 `$w$` 和 `$b$`，使其逼近真实值 `$w=2$`, `$b=1$`。

***

### **2. 模拟数据**

我们生成一组训练数据（共100样本，批量大小=10）：

```python
import torch

# 真实数据生成
X = torch.rand(100, 1)  # 输入特征 (100 samples)
y = 2 * X + 1 + 0.1 * torch.randn(100, 1)  # 真实标签 (带噪声)

# 初始化模型参数
w = torch.randn(1, requires_grad=True)  # 随机初始化权重 (需要梯度)
b = torch.zeros(1, requires_grad=True)  # 初始偏置
```

***

### **3. 单次迭代的详细步骤**

#### **(1) 读取一小批量数据（Batch）**

```python
batch_size = 10
idx = torch.randint(0, 100, (batch_size,))  # 随机选10个样本
X_batch, y_batch = X[idx], y[idx]
```

*   **数据示例**： X\_batch = tensor(\[\[0.12], \[0.45], ..., \[0.78]])  # shape (10,1) y\_batch = tensor(\[\[1.21], \[1.93], ..., \[2.51]])   # 对应的真实值

#### **(2) 前向传播（获得预测）**

```python
y_pred = w * X_batch + b  # 模型预测
```

*   **计算示例**：
    *   假设当前 `$w=1.5$`, `$b=0.3$`
    *   第一个样本预测值：`$1.5 \times 0.12 + 0.3 = 0.48$`（与真实值 `$1.21$` 相差较远）

#### **(3) 计算损失（Loss）**

使用均方误差（MSE）：

```python
loss = ((y_pred - y_batch) ** 2).mean()  # MSE损失
```

*   **计算过程**：
    *   对每个样本计算 `$(预测值 - 真实值)^2$`，然后取平均。
    *   例如第一个样本损失：`$(0.48 - 1.21)^2 = 0.5329$`

#### **(4) 反向传播（计算梯度）**

```python
loss.backward()  # 自动计算梯度，这里计算的是 w,b 的梯度
```

*   **梯度结果**：
    *   `$\frac{\partial \text{loss}}{\partial w}$` 和 `$\frac{\partial \text{loss}}{\partial b}$` 被存储在 `w.grad` 和 `b.grad` 中。
    *   例如： w\.grad = tensor(\[-0.82])  # 当前batch对w的梯度&#x20;
    *   b.grad = tensor(\[-0.91])  # 对b的梯度

#### **(5) 参数更新（SGD优化器）**

```python
learning_rate = 0.01
with torch.no_grad():  # 禁用梯度跟踪（因为更新参数是数学操作，不需要梯度）
    w -= learning_rate * w.grad
    b -= learning_rate * b.grad
    w.grad.zero_()    # 清空梯度，防止累加
    b.grad.zero_()
```

*   **更新后的参数**：
    *   新 `$w = 1.5 - 0.01 \times (-0.82) = 1.5082$`
    *   新 `$b = 0.3 - 0.01 \times (-0.91) = 0.3091$`
    *   **趋势**：梯度为负时，参数会增加（向损失减小的方向移动）。

***

### **4. 完整训练循环代码**

```python
for epoch in range(100):  # 训练100轮
    for i in range(0, 100, batch_size):
        # 1. 读取批量数据
        X_batch, y_batch = X[i:i+batch_size], y[i:i+batch_size]
        
        # 2. 前向传播
        y_pred = w * X_batch + b
        
        # 3. 计算损失
        loss = ((y_pred - y_batch) ** 2).mean()
        
        # 4. 反向传播，自动计算梯度
        loss.backward()
        
        # 5. 参数更新 (SGD)
        with torch.no_grad():
            w -= learning_rate * w.grad
            b -= learning_rate * b.grad
            w.grad.zero_()
            b.grad.zero_()
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')

# 训练后结果
print(f'Trained w: {w.item():.3f}, b: {b.item():.3f}')
# 理想输出：w≈2.0, b≈1.0
```

***

### **5. 关键点总结**

| 步骤       | 输入                        | 操作                                                      | 输出                                                                      |
| -------- | ------------------------- | ------------------------------------------------------- | ----------------------------------------------------------------------- |
| **前向传播** | 数据 `$X$`, 参数 `$w$`, `$b$` | `$wX + b$`                                              | 预测值 `$\hat{y}$`                                                         |
| **计算损失** | `$\hat{y}$`, 真实值 `$y$`    | `$(\hat{y}-y)^2$` 取平均（可以是其他方式）                          | 标量损失值 `$L$`                                                             |
| **反向传播** | `$L$`, 计算图                | `loss.backward()`                                       | 梯度 `$\frac{\partial L}{\partial w}$`, `$\frac{\partial L}{\partial b}$` |
| **参数更新** | 梯度, 学习率 `$\eta$`          | `$w \leftarrow w - \eta \frac{\partial L}{\partial w}$` | 新的 `$w$`, `$b$`                                                         |

***

### **6. 可视化理解**

    初始参数 (w=1.5, b=0.3)
       ↓
    预测值 y_pred = 1.5*X + 0.3 → 计算损失 L=(y_pred - y)^2
       ↓
    反向传播：计算梯度 dL/dw = -0.82, dL/db = -0.91
       ↓
    更新参数：w = 1.5 - 0.01*(-0.82) = 1.5082
             b = 0.3 - 0.01*(-0.91) = 0.3091
       ↓
    下一轮迭代...

通过多次迭代，`$w$` 和 `$b$` 会逐步逼近真实值 `$2$` 和 `$1$`，损失逐渐减小。这就是深度学习的核心训练过程！

在深度学习中，**计算图（Computational Graph）** 是在 **前向传播（Forward Pass）** 过程中动态生成的，并在反向传播时用于梯度计算。以下是详细的步骤解析：

***

## 计算图

### **1. 计算图的生成时机**

计算图是在模型执行前向传播时 **自动构建** 的，具体发生在以下步骤中：

1.  **前向传播开始时**：当输入数据经过模型的每一层时，PyTorch/TensorFlow 会记录所有张量操作（如矩阵乘法、激活函数等）。
2.  **操作记录**：每个涉及 `requires_grad=True` 的张量操作会被加入计算图中，形成节点（Nodes）和边（Edges）。
3.  **图构建完成**：当前向传播结束时，计算图会完整保存从输入到损失的所有计算路径。

***

### **2. 具体过程示例**

以简单的线性回归为例：

```python
import torch

# 初始化需要梯度的参数（叶子节点）
w = torch.tensor([1.5], requires_grad=True)
b = torch.tensor([0.3], requires_grad=True)

# 输入数据
x = torch.tensor([0.5])  # 假设单个样本
y_true = torch.tensor([2.0])  # 真实值

# 前向传播（计算图在此过程中构建）
y_pred = w * x + b       # 操作1: 乘法 -> 操作2: 加法
loss = (y_pred - y_true) ** 2  # 操作3: 平方
```

此时的计算图如下：

        w (leaf)     x (无梯度)
          \         /
           Multiply (操作1)
              |
              a (中间结果)
               \ 
                Add (操作2) <- b (leaf)
                  |
               y_pred
                  |
               Subtract <- y_true
                  |
               Square (操作3)
                  |
               loss

***

### **3. 计算图的关键特性**

*   **动态图（Dynamic Graph）**：PyTorch 每次前向传播会重新构建计算图（与 TensorFlow 1.x 的静态图不同）。
*   **叶子节点（Leaf Nodes）**：直接创建的张量（如 `w`, `b`）是叶子节点，其梯度由反向传播计算。
*   **中间节点（Intermediate Nodes）**：由操作生成的张量（如 `y_pred`, `a`）会自动记录操作历史。

***

### **4. 反向传播时如何使用计算图？**

当调用 `loss.backward()` 时：

1.  **从损失节点开始**：反向遍历计算图，按链式法则计算梯度。
    *   例如：`$\frac{\partial \text{loss}}{\partial w} = \frac{\partial \text{loss}}{\partial y_{\text{pred}}} \cdot \frac{\partial y_{\text{pred}}}{\partial w}$`
2.  **梯度累积**：梯度会存储在叶子节点的 `.grad` 属性中（如 `w.grad`, `b.grad`）。
3.  **图释放**：默认情况下，计算图在反向传播后会被销毁（除非指定 `retain_graph=True`）。

***

### **5. 验证计算图的存在**

可以通过 `grad_fn` 属性查看张量的操作来源：

```python
print(loss.grad_fn)  # 输出: <PowBackward0 at 0x7f8b1c1b2a90>
print(y_pred.grad_fn) # 输出: <AddBackward0 at 0x7f8b1c1b2b50>
print((w * x).grad_fn) # 输出: <MulBackward0 at 0x7f8b1c1b2c10>
```

这些 `*Backward` 对象就是计算图中节点的反向计算函数。

***

### **6. 重要注意事项**

*   **非叶子节点的梯度**：默认情况下，中间节点的梯度不会被保留（节省内存），除非显式调用 `.retain_grad()`。

*   **禁止原地操作**：修改 `requires_grad=True` 的叶子张量会破坏计算图，引发错误:

    RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.

*   **with torch.no\_grad()**：在该上下文中操作时，PyTorch 不会构建计算图。

***

### **总结**

| 步骤       | 计算图的角色                         |
| -------- | ------------------------------ |
| **前向传播** | 动态生成计算图，记录所有操作（如乘、加、激活函数等）。    |
| **反向传播** | 依据计算图，通过链式法则从损失节点回溯计算梯度。       |
| **参数更新** | 使用叶子节点的 `.grad` 更新参数，计算图默认被释放。 |

理解计算图的生成和释放机制，是掌握深度学习框架自动微分（Autograd）的关键！
