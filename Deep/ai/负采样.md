# 负采样（Negative Sampling）详解

## 1. 负采样的基本概念

负采样是一种用于优化训练效率的技术，主要应用于**自然语言处理**（尤其是词嵌入模型如Word2Vec）和**推荐系统**等领域。

`$核心思想$`：在训练过程中，对于每个正样本（真实存在的样本对），随机采样少量负样本（不存在的样本对）进行对比学习，而不是使用所有可能的负样本。

## 2. 解决的问题

负采样主要解决了两个关键问题：

1.  **计算效率问题**：
    *   传统softmax需要计算所有类别（如词汇表中所有单词）的概率分布
    *   计算复杂度为`$O(|V|)$`（V是词汇表大小），当`$|V|$`很大时（如百万量级），计算非常昂贵

2.  **类别不平衡问题**：
    *   真实数据中正样本（如实际出现的单词对）远少于可能的负样本
    *   负采样通过有选择地训练部分负样本，改善了这种不平衡

## 3. 数学原理

## 原始softmax目标函数：

```math
P(w_o|w_i) = \frac{\exp(v_{w_o}^T v_{w_i})}{\sum_{w=1}^V \exp(v_w^T v_{w_i})}
```

## 负采样改进后的目标函数：

```math
\log \sigma(v_{w_o}^T v_{w_i}) + \sum_{k=1}^K \mathbb{E}_{w_k \sim P_n(w)} [\log \sigma(-v_{w_k}^T v_{w_i})]
```

其中：

*   `$\sigma(x)$`是sigmoid函数：`$\sigma(x) = \frac{1}{1+e^{-x}}$`
*   `$w_o$`是目标词（正样本）
*   `$w_i$`是输入词
*   `$w_k$`是采样的负样本
*   `$K$`是负样本数量（通常5-20个）
*   `$P_n(w)$`是负采样分布

## 4. 负采样分布

常用的负采样分布是**修正后的unigram分布**：

```math
P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=1}^V f(w_j)^{3/4}}
```

其中`$f(w_i)$`是单词`$w_i$`的频率。3/4次方的调整使得：

*   低频词被采样概率增大
*   高频词被采样概率减小（但仍是主要负样本来源）

## 5. 实现步骤

1.  对每个训练样本`$(w_i, w_o)$`：
    *   保持正样本`$(w_i, w_o)$`的梯度更新
    *   随机采样K个负样本`$(w_i, w_k)$`，`$w_k \sim P_n(w)$`
2.  更新参数时：
    *   最大化正样本的相似度
    *   最小化负样本的相似度

## 6. 优势与局限

**优势**：

*   训练速度大幅提升（尤其在大词汇表场景）
*   实际效果与原始softmax相当甚至更好
*   可以灵活控制负样本数量

**局限**：

*   采样分布的选择影响模型性能
*   可能引入采样偏差
*   需要调整负样本数量K（超参数）

## 7. 应用场景

*   Word2Vec中的Skip-gram模型
*   推荐系统中的物品/用户嵌入
*   任何需要处理大规模输出空间的分类问题

好的！我用一个**具体的词向量训练例子**和**推荐系统例子**来说明负采样是如何工作的，包括具体数据和计算步骤。

***

# 一、自然语言处理中的负采样（Word2Vec示例）

## 1. 场景设定

**训练目标**：学习单词的向量表示（如"king"→\[0.3, -0.2, 0.5]）\
**输入数据**：句子 = \["cat", "climb", "tree"]\
**参数**：

*   向量维度=3
*   负样本数K=2
*   当前训练的中心词`$w_i$`="climb"
*   真实上下文词（正样本）`$w_o$`="tree"

## 2. 具体数据

假设当前向量值（随机初始化）：

    $v_{climb} = [0.1, -0.3, 0.5]$  
    $v_{tree} = [0.4, 0.2, -0.1]$  
    $v_{apple} = [-0.2, 0.6, 0.3]$  （采样的负样本1）  
    $v_{book} = [0.5, -0.1, -0.4]$   （采样的负样本2）

## 3. 计算过程

### (1) 正样本得分

```math
\text{score}_+ = v_{tree} \cdot v_{climb} = 0.4*0.1 + 0.2*(-0.3) + (-0.1)*0.5 = -0.13
```

使用sigmoid激活：

```math
\sigma(\text{score}_+) = \frac{1}{1+e^{0.13}} \approx 0.467
```

### (2) 负样本得分

*   对"apple"：
    ```math
    \text{score}_-^1 = v_{apple} \cdot v_{climb} = -0.2*0.1 + 0.6*(-0.3) + 0.3*0.5 = -0.05  
    \sigma(-\text{score}_-^1) = \frac{1}{1+e^{0.05}} \approx 0.488
    ```
*   对"book"：
    ```math
    \text{score}_-^2 = v_{book} \cdot v_{climb} = 0.5*0.1 + (-0.1)*(-0.3) + (-0.4)*0.5 = -0.12  
    \sigma(-\text{score}_-^2) = \frac{1}{1+e^{0.12}} \approx 0.470
    ```

### (3) 损失函数

```math
\mathcal{L} = -\log(0.467) - \log(0.488) - \log(0.470) \approx 0.76
```

### (4) 参数更新

通过反向传播调整`$v_{climb}$`、`$v_{tree}$`、`$v_{apple}$`、`$v_{book}$`的值（具体略）。

***

# 二、推荐系统中的负采样（电影推荐示例）

## 1. 场景设定

**目标**：预测用户是否会喜欢某部电影\
**数据**：

*   用户A的真实点击（正样本）：\["泰坦尼克号", "阿凡达"]
*   所有电影：\["泰坦尼克号", "阿凡达", "变形金刚", "肖申克的救赎", "复仇者联盟"]
*   负样本数K=3

## 2. 负采样过程

从用户A**未点击**的电影中随机采样3部（概率与流行度相关）：

1.  "变形金刚"（高频电影，采样概率高）
2.  "复仇者联盟"（高频电影）
3.  "肖申克的救赎"（低频电影，但因3/4次方调整仍有概率被选中）

## 3. 模型训练

对于用户A的向量`$u_A$`和电影向量`$v_i$`：

*   最大化：`$\sigma(u_A \cdot v_{泰坦尼克号})$` 和 `$\sigma(u_A \cdot v_{阿凡达})$`
*   最小化：`$\sigma(u_A \cdot v_{变形金刚})$`、`$\sigma(u_A \cdot v_{复仇者联盟})$`、`$\sigma(u_A \cdot v_{肖申克的救赎})$`

***

## 三、为什么这样有效？——实例分析

假设初始随机向量：

    $u_A = [0.2, -0.1]$  
    $v_{泰坦尼克号} = [0.3, 0.4]$  
    $v_{变形金刚} = [-0.2, 0.5]$

1.  **正样本更新**：\
    `$u_A \cdot v_{泰坦尼克号} = 0.02$` → 值较小\
    梯度更新会使`$u_A$`靠近`$v_{泰坦尼克号}$`（如变为\[0.25, 0.05]）

2.  **负样本更新**：\
    `$u_A \cdot v_{变形金刚} = -0.09$` → 但希望更负\
    梯度更新会使`$u_A$`远离`$v_{变形金刚}$`（如变为\[0.18, -0.15]）

***

## 四、对比传统方法

若用传统softmax：

*   需计算用户A与**所有5万部电影**的相似度
*   而负采样只需计算1个正样本+3个负样本=4次计算

***

需要我用其他例子补充说明吗？或是某个计算步骤需要更详细展开？
