好的，我会严格使用 Markdown 的 LaTeX 语法（用 `$$` 包围公式）重新解释 GRU 重置门的计算过程，并配合具体数值示例。

***

### **重置门（Reset Gate）计算公式**

重置门 `$\mathbf{R}_t$` 的数学定义为：

```math
\mathbf{R}_t = \sigma\left(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r\right)
```

其中：

*   `$\mathbf{X}_t$` 是当前时间步的输入向量
*   `$\mathbf{H}_{t-1}$` 是前一时间步的隐藏状态
*   `$\mathbf{W}_{xr}, \mathbf{W}_{hr}$` 是权重矩阵
*   `$\mathbf{b}_r$` 是偏置项
*   `$\sigma$` 是 sigmoid 函数：`$\sigma(x) = \frac{1}{1+e^{-x}}$`

***

### **具体数值示例**

#### **1. 定义输入数据**

```math
\mathbf{X}_t = \begin{bmatrix} 1.0 \\ -0.5 \end{bmatrix}, \quad 
\mathbf{H}_{t-1} = \begin{bmatrix} 0.5 \\ 1.0 \\ -1.0 \end{bmatrix}
```

#### **2. 定义权重和偏置**

```math
\mathbf{W}_{xr} = \begin{bmatrix} 0.1 & -0.2 & 0.3 \\ 0.4 & 0.5 & -0.6 \end{bmatrix}, \quad 
\mathbf{W}_{hr} = \begin{bmatrix} 0.2 & 0.0 & -0.1 \\ -0.3 & 0.4 & 0.5 \\ 0.1 & -0.2 & 0.3 \end{bmatrix}, \quad 
\mathbf{b}_r = \begin{bmatrix} 0.1 \\ -0.1 \\ 0.2 \end{bmatrix}
```

#### **3. 计算步骤**

**(1) 计算 `$\mathbf{X}_t \mathbf{W}_{xr}$`**

```math
\begin{aligned}
\mathbf{X}_t \mathbf{W}_{xr} &= \begin{bmatrix} 1.0 & -0.5 \end{bmatrix} \begin{bmatrix} 0.1 & -0.2 & 0.3 \\ 0.4 & 0.5 & -0.6 \end{bmatrix} \\
&= \begin{bmatrix} 
(1.0 \times 0.1) + (-0.5 \times 0.4) \\
(1.0 \times -0.2) + (-0.5 \times 0.5) \\
(1.0 \times 0.3) + (-0.5 \times -0.6)
\end{bmatrix}^\top \\
&= \begin{bmatrix} -0.1 & -0.45 & 0.6 \end{bmatrix}^\top
\end{aligned}
```

**(2) 计算 `$\mathbf{H}_{t-1} \mathbf{W}_{hr}$`**

```math
\begin{aligned}
\mathbf{H}_{t-1} \mathbf{W}_{hr} &= \begin{bmatrix} 0.5 & 1.0 & -1.0 \end{bmatrix} \begin{bmatrix} 0.2 & 0.0 & -0.1 \\ -0.3 & 0.4 & 0.5 \\ 0.1 & -0.2 & 0.3 \end{bmatrix} \\
&= \begin{bmatrix} 
(0.5 \times 0.2) + (1.0 \times -0.3) + (-1.0 \times 0.1) \\
(0.5 \times 0.0) + (1.0 \times 0.4) + (-1.0 \times -0.2) \\
(0.5 \times -0.1) + (1.0 \times 0.5) + (-1.0 \times 0.3)
\end{bmatrix}^\top \\
&= \begin{bmatrix} -0.3 & 0.6 & 0.15 \end{bmatrix}^\top
\end{aligned}
```

**(3) 合并结果并加偏置**

```math
\begin{bmatrix} -0.1 \\ -0.45 \\ 0.6 \end{bmatrix} + \begin{bmatrix} -0.3 \\ 0.6 \\ 0.15 \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.1 \\ 0.2 \end{bmatrix} = \begin{bmatrix} -0.3 \\ 0.05 \\ 0.95 \end{bmatrix}
```

**(4) 应用 Sigmoid 激活**

```math
\mathbf{R}_t = \sigma\left(\begin{bmatrix} -0.3 \\ 0.05 \\ 0.95 \end{bmatrix}\right) = \begin{bmatrix} 
\frac{1}{1+e^{0.3}} \\ 
\frac{1}{1+e^{-0.05}} \\ 
\frac{1}{1+e^{-0.95}} 
\end{bmatrix} \approx \begin{bmatrix} 0.425 \\ 0.512 \\ 0.721 \end{bmatrix}
```

***

### **最终输出**

```math
\mathbf{R}_t \approx \begin{bmatrix} 0.425 \\ 0.512 \\ 0.721 \end{bmatrix}
```

### **关键说明**

1.  所有公式严格用 `$$` 包围，符合 Markdown 的 LaTeX 语法标准。
2.  计算步骤完整展示矩阵乘法和逐元素操作。
3.  Sigmoid 函数将数值压缩到 `$[0,1]$` 区间，形成门控信号。

在GRU（门控循环单元）中，权重矩阵 `$\mathbf{W}_{xr}$` 和 `$\mathbf{W}_{hr}$` 分别表示 **输入到重置门** 和 **隐藏状态到重置门** 的线性变换权重。以下是详细说明：

***

### **1. `$\mathbf{W}_{xr}$`：输入到重置门的权重**

#### **数学定义**

```math
\mathbf{W}_{xr} \in \mathbb{R}^{d \times h}
```

*   **输入维度**：`$d$`（输入向量 `$\mathbf{X}_t$` 的维度）
*   **输出维度**：`$h$`（隐藏状态 `$\mathbf{H}_t$` 的维度）

#### **物理意义**

*   将当前时间步的输入 `$\mathbf{X}_t$` 映射到重置门 `$\mathbf{R}_t$` 的计算空间中。
*   **作用**：决定当前输入 `$\mathbf{X}_t$` 对重置门的贡献程度。

#### **计算示例**

若输入 `$\mathbf{X}_t = [x_1, x_2]^\top$`，权重 `$\mathbf{W}_{xr}$` 的每一列对应隐藏状态的某一维度：

```math
\mathbf{X}_t \mathbf{W}_{xr} = \begin{bmatrix} x_1 w_{11} + x_2 w_{21} \\ x_1 w_{12} + x_2 w_{22} \\ x_1 w_{13} + x_2 w_{23} \end{bmatrix}^\top
```

***

### **2. `$\mathbf{W}_{hr}$`：隐藏状态到重置门的权重**

#### **数学定义**

```math
\mathbf{W}_{hr} \in \mathbb{R}^{h \times h}
```

*   **输入维度**：`$h$`（前一隐藏状态 `$\mathbf{H}_{t-1}$` 的维度）
*   **输出维度**：`$h$`（与隐藏状态维度相同）

#### **物理意义**

*   将前一隐藏状态 `$\mathbf{H}_{t-1}$` 映射到重置门 `$\mathbf{R}_t$` 的计算空间中。
*   **作用**：决定历史信息 `$\mathbf{H}_{t-1}$` 对重置门的贡献程度。

#### **计算示例**

若隐藏状态 `$\mathbf{H}_{t-1} = [h_1, h_2, h_3]^\top$`，权重 `$\mathbf{W}_{hr}$` 的每一列对应重置门的某一维度：

```math
\mathbf{H}_{t-1} \mathbf{W}_{hr} = \begin{bmatrix} h_1 w_{11} + h_2 w_{21} + h_3 w_{31} \\ h_1 w_{12} + h_2 w_{22} + h_3 w_{32} \\ h_1 w_{13} + h_2 w_{23} + h_3 w_{33} \end{bmatrix}^\top
```

***

### **3. 联合作用**

重置门的完整计算：

```math
\mathbf{R}_t = \sigma\left( \underbrace{\mathbf{X}_t \mathbf{W}_{xr}}_{\text{输入贡献}} + \underbrace{\mathbf{H}_{t-1} \mathbf{W}_{hr}}_{\text{历史贡献}} + \mathbf{b}_r \right)
```

*   **`$\mathbf{W}_{xr}$`**：学习输入数据中哪些特征对“重置历史信息”更重要。
*   **`$\mathbf{W}_{hr}$`**：学习历史信息中哪些维度需要被保留或遗忘。

***

### **4. 类比说明**

假设你在阅读一段文本：

*   **`$\mathbf{W}_{xr}$`**：决定当前单词（如句号）是否触发“重置历史记忆”。
*   **`$\mathbf{W}_{hr}$`**：决定前一时刻的隐藏状态（如上下文情感）如何影响当前重置决策。

***

### **5. 参数学习**

*   这些权重通过反向传播（BPTT）自动学习。
*   例如：
    *   若模型需要频繁忽略历史信息（如段落边界），`$\mathbf{W}_{xr}$` 会对标点符号赋予高权重。
    *   若模型需长期依赖历史（如语法结构），`$\mathbf{W}_{hr}$` 会强化隐藏状态的传递。

***

### **总结表**

| 权重矩阵                | 作用                              | 输入 → 输出维度                   | 功能类比          |
| ------------------- | ------------------------------- | --------------------------- | ------------- |
| `$\mathbf{W}_{xr}$` | 输入 `$\mathbf{X}_t$` → 重置门       | `$\mathbb{R}^{d \times h}$` | “当前信息的重要性过滤器” |
| `$\mathbf{W}_{hr}$` | 隐藏状态 `$\mathbf{H}_{t-1}$` → 重置门 | `$\mathbb{R}^{h \times h}$` | “历史记忆的遗忘控制器”  |

通过这种方式，GRU能够动态决定何时忽略历史信息、何时保留它，从而有效处理序列数据的长期依赖关系。
