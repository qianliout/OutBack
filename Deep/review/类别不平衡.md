在NLP任务中，**类别不平衡问题**（如情感分析中负面样本仅占5%）会导致模型偏向多数类。以下是系统化的解决方案、原理说明及实践指南：

***

## **1. 数据层面的解决方法**

### **(1) 过采样（Oversampling）**

*   **原理**：增加少数类样本的副本或生成新样本，平衡类别分布。
*   **方法**：
    *   **随机过采样**：直接复制少数类样本（易过拟合）。
    *   **SMOTE-NLP**：通过近邻插值生成新文本。
        ```python
        from imblearn.over_sampling import SMOTE
        from sklearn.feature_extraction.text import TfidfVectorizer

        tfidf = TfidfVectorizer()
        X = tfidf.fit_transform(texts)
        smote = SMOTE()
        X_res, y_res = smote.fit_resample(X, labels)  # 生成合成样本
        ```
    *   **回译增强**：将文本翻译成其他语言再译回（如“好电影”→英文→“good movie”→中文→“不错的电影”）。

### **(2) 欠采样（Undersampling）**

*   **原理**：减少多数类样本数量，可能丢失重要信息。
*   **方法**：
    *   **随机欠采样**：随机丢弃多数类样本。
    *   **NearMiss**：保留与少数类边界相关的多数类样本。
    *   **Tomek Links**：移除边界附近的多数类样本。
        ```python
        from imblearn.under_sampling import TomekLinks
        tl = TomekLinks()
        X_res, y_res = tl.fit_resample(X, labels)
        ```

### **(3) 混合采样**

*   **原理**：结合过采样和欠采样。
*   **示例**：
    ```python
    from imblearn.combine import SMOTETomek
    smote_tomek = SMOTETomek()
    X_res, y_res = smote_tomek.fit_resample(X, labels)
    ```

***

## **2. 算法层面的解决方法**

### **(1) 类别权重调整**

*   **原理**：损失函数中给少数类更高权重。
*   **实现**：
    ```python
    from sklearn.svm import LinearSVC
    model = LinearSVC(class_weight={0:1, 1:10})  # 少数类权重=10

    # BERT微调中的权重设置
    from transformers import Trainer
    trainer = Trainer(
        model,
        args,
        compute_metrics=compute_metrics,
        class_weights=torch.tensor([1.0, 5.0])  # 少数类权重5倍
    )
    ```

### **(2) 损失函数改进**

*   **Focal Loss**：降低易分类样本的权重，聚焦难样本。
    ```python
    class FocalLoss(nn.Module):
        def __init__(self, alpha=0.25, gamma=2):
            super().__init__()
            self.alpha = alpha
            self.gamma = gamma

        def forward(self, inputs, targets):
            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
            pt = torch.exp(-BCE_loss)
            loss = self.alpha * (1-pt)**self.gamma * BCE_loss
            return loss.mean()
    ```

### **(3) 阈值移动（Threshold Moving）**

*   **原理**：调整分类阈值（默认0.5），如改为少数类的先验概率。
    ```python
    from sklearn.metrics import precision_recall_curve
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)
    optimal_threshold = thresholds[np.argmax(recall - precision)]  # 最大化F1
    ```

***

## **3. 模型架构的改进**

### **(1) 集成方法**

*   **原理**：组合多个模型的预测，降低方差。
*   **方法**：
    *   **EasyEnsemble**：对多数类欠采样多次，训练多个子模型。
        ```python
        from imblearn.ensemble import EasyEnsembleClassifier
        eec = EasyEnsembleClassifier(n_estimators=10)
        eec.fit(X_train, y_train)
        ```
    *   **BalanceCascade**：迭代地移除被正确分类的多数类样本。

### **(2) 两阶段训练**

1.  **第一阶段**：用全部数据训练特征提取器。
2.  **第二阶段**：对分类层使用平衡数据微调。

***

## **4. 评估指标的调整**

| 指标                   | 公式                                           | 适用场景          |
| -------------------- | -------------------------------------------- | ------------- |
| **F1-Score (Macro)** | `$ F1 = 2 \times \frac{P \times R}{P + R} $` | 通用不平衡分类       |
| **AUC-ROC**          | ROC曲线下面积                                     | 关注排序性能（如推荐系统） |
| **G-Mean**           | `$ \sqrt{Recall_0 \times Recall_1} $`        | 要求各类召回率均衡     |

**代码实现**：

```python
from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred, target_names=['class0', 'class1']))
```

***

## **5. 领域特定解决方案**

### **(1) 文本生成任务**

*   **温度采样（Temperature Scaling）**：\
    调整生成时的softmax温度，增加少数类词汇概率。
    ```python
    generator(text, temperature=0.7)  # 更高温度增加多样性
    ```

### **(2) 命名实体识别（NER）**

*   **实体级别过采样**：\
    复制包含少数类实体的句子（如“肝癌”实体出现次数翻倍）。

### **(3) 预训练模型微调**

*   **动态掩码比例**：\
    对少数类相关词增加掩码概率（如医疗实体词掩码概率提高20%）。

***

## **6. 方法对比与选择指南**

| 方法             | 适用场景            | 优点        | 缺点       |
| -------------- | --------------- | --------- | -------- |
| **SMOTE-NLP**  | 小规模不平衡文本（<10k条） | 生成语义合理的样本 | 计算成本高    |
| **类别权重**       | 大规模数据+深度学习模型    | 无需修改数据分布  | 需调参      |
| **Focal Loss** | 高度不平衡（1:100+）   | 自动聚焦难样本   | 可能训练不稳定  |
| **两阶段训练**      | 预训练模型（BERT/GPT） | 保留预训练知识   | 需要额外训练步骤 |

***

## **7. 完整Pipeline示例（医疗文本分类）**

```python
from imblearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

# 定义处理流程
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),
    ('smote', SMOTE(random_state=42)),  # 过采样
    ('clf', RandomForestClassifier(class_weight='balanced'))
])

# 评估
from sklearn.model_selection import cross_val_score
scores = cross_val_score(pipeline, texts, labels, cv=5, scoring='f1_macro')
print(f"平均F1: {scores.mean():.3f}")
```

***

## **总结**

*   **轻度不平衡（1:3）**：使用类别权重或阈值移动。
*   **中度不平衡（1:10）**：SMOTE-NLP + 集成方法。
*   **极度不平衡（1:100+）**：Focal Loss + 两阶段训练。

> **关键原则**：
>
> 1.  优先尝试对模型无损的方法（如损失函数调整）
> 2.  过采样时确保生成样本的语义合理性
> 3.  始终用宏平均F1/AUC-ROC替代准确率评估

**推荐工具库**：

*   [imbalanced-learn](https://imbalanced-learn.org/)
*   [TextAugment](https://github.com/dsfsi/textaugment)（数据增强）
*   [Transformer-based Focal Loss](https://huggingface.co/docs/transformers/tasks/sequence_classification#imbalanced-data)

