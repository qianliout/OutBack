# 位置编码 (Positional Encoding)

## 1. 实现原理

自注意力机制有一个固有的“缺陷”：它本身是无法感知序列中单词的顺序的。无论你如何打乱输入句子中单词的顺序，自注意力的计算结果都是完全一样的。这对于需要理解语序的自然语言任务来说是致命的。

为了解决这个问题，Transformer 的提出者引入了**位置编码 (Positional Encoding)**。其核心思想是：为输入序列中的每一个位置（或 token）都生成一个独特的、能够代表其绝对或相对位置信息的向量，然后将这个**位置向量**与该位置的**词嵌入向量**相加。这样，每个单词的表示就同时包含了其语义信息（来自词嵌入）和位置信息（来自位置编码）。

**a. 正弦/余弦位置编码 (Sinusoidal Positional Encoding):**

在原始的 Transformer 论文中，作者提出了一种使用正弦和余弦函数来生成位置编码的方法。这种方法不需要训练，可以直接计算得出，并且能够很好地泛化到比训练集中句子更长的序列。

其计算公式如下：

对于位置 `pos` 和维度 `i`：

*   当 `i` 是偶数时: `PE(pos, 2i) = sin(pos / 10000^(2i / d_model))`
*   当 `i` 是奇数时: `PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))`

其中：
*   `pos` 是单词在序列中的位置 (0, 1, 2, ...)
*   `i` 是位置编码向量中的维度索引 (0, 1, 2, ...)
*   `d_model` 是词嵌入的维度

这个公式的巧妙之处在于：

1.  **唯一性:** 每个位置都有一个独一无二的位置编码。
2.  **固定偏移:** 对于任意固定的偏移量 `k`，`PE(pos+k)` 都可以表示为 `PE(pos)` 的一个线性函数。这意味着模型可以很容易地学习到单词之间的相对位置关系。
3.  **周期性:** 正弦和余弦函数的周期性使得模型能够推断出比训练时遇到的序列更长的位置信息。

**b. 可学习的位置编码 (Learned Positional Encoding):**

除了固定的正弦/余弦编码外，另一种常见的方法是让模型自己学习位置编码。具体做法是：创建一个与最大序列长度和词嵌入维度相匹配的位置编码矩阵，并将其作为模型参数的一部分，在训练过程中不断更新。

这种方法更加灵活，允许模型根据具体任务学习到最优的位置表示。BERT 和很多后续模型都采用了这种方式。

---

## 2. 所解决的问题

位置编码的核心目标是解决**自注意力机制无法处理序列顺序**的问题。

*   **为模型注入顺序信息:** 它为模型提供了关于单词在序列中绝对位置或相对位置的关键信息，使得模型能够区分 “A B” 和 “B A” 这样语序不同但单词相同的序列。
*   **保持排列不变性模型的优势:** 通过将位置信息作为额外输入“添加”进去，而不是改变模型的核心结构，Transformer 依然保留了自注意力机制强大的并行计算能力和对长距离依赖的捕捉能力。

---

## 3. 核心代码

下面是一个使用 PyTorch 实现的、基于正弦/余弦函数的位置编码模块。

```python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        """
        Args:
            d_model (int): 词嵌入的维度.
            max_len (int): 句子的最大长度.
        """
        super(PositionalEncoding, self).__init__()

        # 创建一个足够大的位置编码矩阵
        pe = torch.zeros(max_len, d_model)
        
        # 创建一个代表位置的张量 [max_len, 1]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # 计算分母部分
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        # 计算偶数维度的 sin 值
        pe[:, 0::2] = torch.sin(position * div_term)
        
        # 计算奇数维度的 cos 值
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # 增加一个 batch 维度，并注册为 buffer (不参与模型训练)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        """
        Args:
            x: 输入张量, shape [batch_size, seq_len, d_model]
        """
        # 将 x 与其位置编码相加
        # x.size(1) 是当前输入的序列长度
        x = x + self.pe[:, :x.size(1)]
        return x

```

---

## 4. 实际工程中的应用

位置编码是所有 Transformer 及其变体模型中必不可少的一部分。

*   **原始 Transformer 模型:** 使用固定的正弦/余弦位置编码。
*   **BERT:** 使用可学习的绝对位置编码。它会为每个输入位置学习一个特定的向量。
*   **GPT-2/GPT-3:** 同样使用可学习的绝对位置编码。
*   **Transformer-XL:** 为了处理超长序列，引入了相对位置编码的概念，只关注单词之间的相对距离，而不是绝对位置。
*   **RoPE (Rotary Position Embedding):** 一种新颖的相对位置编码方法，通过旋转 Q 和 K 向量来注入位置信息，在许多最新的大语言模型（如 LLaMA）中被广泛采用，因为它在处理长文本时表现出色。

在实际应用中，选择哪种位置编码方式取决于具体的任务和模型架构。对于大多数标准长度的序列任务，可学习的绝对位置编码表现良好。对于需要处理非常长序列的任务，相对位置编码（如 RoPE）通常是更好的选择。
