# 低秩适配 (LoRA: Low-Rank Adaptation)

## 1. 实现原理

LoRA（Low-Rank Adaptation of Large Language Models）是一种**参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）**技术。其核心思想是：**在微调大型预训练模型（如 LLM）时，冻结住原始模型的绝大部分参数，只为模型中的特定层（通常是注意力层中的权重矩阵）注入少量、可训练的“适配器”模块。**

这个“适配器”的设计，基于一个关键的假设：**模型在适应新任务时，其参数的“变化量”是低秩的。** 也就是说，尽管原始的权重矩阵 `W` 是一个巨大的、高秩的矩阵，但为了适应新任务而需要学习到的“更新量” `ΔW`，可以用两个更小的、低秩的矩阵的乘积来近似模拟。

**实现流程：**

1.  **冻结预训练权重:** 首先，将预训练模型（例如，一个 70 亿参数的 LLaMA 模型）的所有原始权重 `W` 设置为不可训练状态（`requires_grad = False`）。

2.  **注入低秩适配器:**
    *   选择要进行适配的层。在 Transformer 中，通常选择对 Query 和 Value 的线性投影层（`W_q` 和 `W_v`）进行适配，因为它们对注意力的计算至关重要。
    *   对于一个原始的、被冻结的权重矩阵 `W`（维度为 `d x k`），我们在其旁边并联一个“低秩适配器”。这个适配器由两个小矩阵组成：
        *   矩阵 `A`，维度为 `d x r`
        *   矩阵 `B`，维度为 `r x k`
    *   其中，`r` 是这个适配器的**“秩”（rank）**，它是一个远小于 `d` 和 `k` 的超参数（例如，`r` 可以是 4, 8, 16）。`r` 越小，引入的新参数就越少。

3.  **修改前向传播:**
    *   在微调时，模型的输出 `y` 不再仅仅是 `y = x * W`，而是变成了：
        `y = x * W + x * A * B`
    *   其中，`x` 是输入，`W` 是被冻结的原始权重，而 `A` 和 `B` 是新添加的、可训练的权重。`A` 通常用随机高斯分布初始化，而 `B` 用零初始化，这样在训练开始时，适配器 `A*B` 的输出为零，不会对原始模型产生干扰。

4.  **只训练适配器:**
    *   在反向传播时，只有矩阵 `A` 和 `B` 的梯度会被计算和更新，而庞大的 `W` 矩阵保持不变。

通过这种方式，需要训练的参数数量从数亿甚至数十亿，急剧减少到了区区几百万甚至几十万，降幅可达上千倍。

---

## 2. 所解决的问题

LoRA 主要解决了**全量微调（Full Fine-tuning）大型语言模型时遇到的巨大挑战**：

1.  **高昂的计算和存储成本:** 全量微调一个百亿参数的 LLM 需要大量的、高端的 GPU 资源和数百 GB 的存储空间（用于存储模型副本、梯度、优化器状态等）。LoRA 使得在单个消费级 GPU 上微调大型模型成为可能。

2.  **灾难性遗忘 (Catastrophic Forgetting):** 全量微调会更新模型的所有参数，这有可能会破坏模型在预训练阶段学到的通用知识。LoRA 只在旁边增加“补丁”，不触动原始知识，从而更好地保留了模型的泛化能力。

3.  **部署和管理困难:** 如果为每一个下游任务都保存一个完整的、微调后的模型副本，存储成本将是巨大的。使用 LoRA，我们只需要保存一份共享的、巨大的基础模型，然后为每个任务只保存一个极小的（通常只有几 MB）适配器权重（`A` 和 `B`）。在部署时，可以根据任务需求，动态地加载和切换不同的适配器。

---

## 3. 核心代码

在工程实践中，我们很少手动实现 LoRA，而是使用像 Hugging Face 的 `peft` (Parameter-Efficient Fine-Tuning) 这样的库，它可以非常方便地将 LoRA 应用到任意的 `transformers` 模型上。

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import get_peft_model, LoraConfig, TaskType

# 1. 加载预训练模型
model_id = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(model_id)

# 2. 定义 LoRA 配置
lora_config = LoraConfig(
    r=8,  # 秩 (rank)
    lora_alpha=32, # LoRA a scaling factor
    target_modules=["q_proj", "v_proj"], # 选择要适配的模块
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM # 任务类型
)

# 3. 使用 get_peft_model 将 LoRA 应用到模型上
peft_model = get_peft_model(model, lora_config)

# 4. 打印可训练参数的数量
# 你会发现可训练参数相比总参数，占比极小 (e.g., 0.08%)
peft_model.print_trainable_parameters()

# 5. 像往常一样进行模型训练
# trainer = Trainer(model=peft_model, ...)
# trainer.train()

# 6. 保存适配器权重
# 只会保存很小的适配器权重文件，而不是整个模型
peft_model.save_pretrained("./my-lora-adapter")

# 7. 加载和合并权重进行推理
# from peft import PeftModel
# base_model = AutoModelForCausalLM.from_pretrained(model_id)
# lora_model = PeftModel.from_pretrained(base_model, "./my-lora-adapter")
# merged_model = lora_model.merge_and_unoad() # 可以选择合并权重以加速推理
```

---

## 4. 实际工程中的应用

LoRA 已经成为当前**微调大型语言模型（LLM）社区和行业的标准和首选方案**。

*   **开源社区的繁荣:** LoRA 极大地降低了个人开发者和研究者参与 LLM 微调的门槛，催生了 Hugging Face Hub 上成千上万个针对特定任务、特定风格的微调模型。用户可以下载基础模型和各种 LoRA 适配器，轻松定制自己的专属模型。
*   **AI 生成内容 (AIGC):** 在文生图领域，LoRA 被广泛用于微调 Stable Diffusion 等模型，以生成特定角色、画风或物体的图像。一个 LoRA 适配器就可以封装一个“画风”或一个“人物”的知识。
*   **企业级应用:** 企业可以利用 LoRA，使用自己的私有数据，经济高效地微调通用大模型，使其适应企业内部的特定任务（如客服、代码辅助、文档分析），而无需从头训练或维护庞大的模型副本。
*   **多任务服务:** 在需要提供多种定制化服务的场景下，可以在后台维护一个基础模型，并根据用户请求动态加载不同的 LoRA 适配器，实现高效的多任务服务。

LoRA 的变体，如 **QLoRA**，通过将 LoRA 与 4 位量化技术相结合，进一步降低了微调的硬件门槛，是目前在单张消费级 GPU 上微调大型模型的 SOTA (State-of-the-art) 方法。
