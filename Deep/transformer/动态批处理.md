# 动态批处理 (Dynamic Batching)

## 1. 实现原理

动态批处理（Dynamic Batching），也称为可变批处理（Variable Batching）或自适应批处理（Adaptive Batching），是一种在深度学习模型推理阶段常用的优化技术。其核心思想是：**根据当前待处理请求的实际情况（特别是序列长度），动态地构建批次（Batch），而不是使用固定大小的批次。**

**背景：固定批处理的局限性**

在深度学习中，为了充分利用 GPU 等并行计算设备的性能，通常会将多个输入样本组成一个批次进行处理。然而，对于像 Transformer 这样处理序列数据的模型，如果序列长度不一，固定批处理会带来效率问题：

1.  **填充（Padding）浪费：** 为了使批次内的所有序列长度一致，短序列需要被填充（pad）到批次中最长序列的长度。这些填充的 token 不包含实际信息，但仍然会参与模型的计算，导致计算资源的浪费。
2.  **GPU 利用率低：** 如果批次中大部分是短序列，而为了少数几个长序列而将整个批次填充到很长，那么 GPU 的大部分计算单元可能都在处理无用的填充数据，导致实际利用率不高。

**动态批处理的实现流程：**

动态批处理旨在解决上述问题，其实现通常涉及以下步骤：

1.  **请求收集与队列：**
    *   所有到达的推理请求（例如，用户输入的文本）会被放入一个等待队列中。

2.  **长度排序/分组：**
    *   队列中的请求可以根据其序列长度进行排序，或者将长度相近的请求进行分组。
    *   **目的：** 确保在同一个批次内的序列长度尽可能接近，从而最小化填充。

3.  **动态批次构建：**
    *   推理服务会根据当前的 GPU 负载、内存限制以及队列中请求的长度分布，动态地从队列中选择请求来构建批次。
    *   **策略：** 可以是“最大化批次内 token 总数”（在不超过内存限制的前提下），或者是“最大化批次大小”（在不超过最大序列长度的前提下）。
    *   **示例：** 如果队列中有长度为 100, 105, 200, 500 的请求，动态批处理可能会将 100 和 105 组成一个批次（填充到 105），而不是将它们与 500 组成一个批次（填充到 500）。

4.  **填充与掩码：**
    *   一旦批次确定，批次内的所有序列会被填充到该批次中最长序列的长度。
    *   同时，生成相应的注意力掩码（Attention Mask），以确保模型在计算时忽略填充部分。

5.  **模型推理：**
    *   将构建好的批次送入模型进行推理。

6.  **结果返回：**
    *   推理完成后，将结果返回给对应的请求发起者。

## 2. 所解决的问题

动态批处理主要解决了深度学习模型（特别是序列模型）在推理阶段的**效率和资源利用率**问题：

1.  **提高 GPU 利用率：** 通过减少不必要的填充计算，使得 GPU 能够更专注于处理实际数据，从而提高其计算资源的利用效率。
2.  **降低推理延迟：** 尤其对于短序列请求，它们不再需要等待批次中所有长序列的计算完成，可以更快地被处理，从而降低了平均推理延迟。
3.  **提升吞吐量：** 在相同硬件资源下，更高效的批次处理意味着单位时间内可以处理更多的请求，从而提升了模型的吞吐量。
4.  **优化内存使用：** 减少填充也意味着减少了中间激活值所需的内存，这对于大型模型尤其重要。

## 3. 核心代码

动态批处理通常是在推理服务框架层面实现的，而不是在模型代码内部。主流的推理服务框架（如 NVIDIA Triton Inference Server, TensorFlow Serving, TorchServe）都提供了对动态批处理的支持。

在模型层面，您只需要确保您的模型能够正确处理带有填充和注意力掩码的输入即可。以下是一个概念性的伪代码，展示了推理服务如何构建动态批次：

```python
import time
import random
from collections import deque

# 假设这是模拟的请求队列
request_queue = deque()

def simulate_incoming_requests(num_requests=100, max_len=512):
    for i in range(num_requests):
        # 模拟不同长度的请求
        seq_len = random.randint(10, max_len)
        request_queue.append({"id": i, "data": [0]*seq_len, "len": seq_len})
        time.sleep(random.uniform(0.01, 0.1)) # 模拟请求到达间隔

def dynamic_batching_inference(model, max_batch_size=16, max_batch_tokens=2048):
    processed_requests = []
    current_batch = []

    while request_queue or current_batch:
        # 尝试从队列中获取请求
        if request_queue:
            # 简单策略：按长度排序，尝试组成批次
            # 实际中可能更复杂，例如等待一段时间，或者有更智能的调度器
            request_queue_sorted = sorted(list(request_queue), key=lambda x: x["len"])
            request_queue.clear()
            request_queue.extend(request_queue_sorted)

            while request_queue and len(current_batch) < max_batch_size:
                next_req = request_queue[0]
                # 检查加入新请求后是否超出最大 token 数限制
                potential_max_len = max([r["len"] for r in current_batch + [next_req]]) if current_batch else next_req["len"]
                potential_total_tokens = potential_max_len * (len(current_batch) + 1)

                if potential_total_tokens <= max_batch_tokens:
                    current_batch.append(request_queue.popleft())
                else:
                    break # 无法再加入更多请求

        if not current_batch and not request_queue: # 没有更多请求了
            break

        if current_batch:
            # 1. 确定当前批次的最大长度
            batch_max_len = max(r["len"] for r in current_batch)

            # 2. 对数据进行填充和生成掩码
            padded_data = []
            attention_masks = []
            for req in current_batch:
                padded_data.append(req["data"] + [0] * (batch_max_len - req["len"]))
                attention_masks.append([1]*req["len"] + [0]*(batch_max_len - req["len"]))
            
            # 转换为 tensor
            input_tensor = torch.tensor(padded_data)
            mask_tensor = torch.tensor(attention_masks)

            # 3. 模拟模型推理
            # output = model(input_tensor, mask_tensor) # 实际的模型调用
            print(f"Processing batch of size {len(current_batch)}, max_len {batch_max_len}, total_tokens {len(current_batch)*batch_max_len}")
            time.sleep(0.5) # 模拟推理时间

            processed_requests.extend([r["id"] for r in current_batch])
            current_batch = [] # 清空当前批次
        else:
            time.sleep(0.1) # 等待新请求

    return processed_requests

# 模拟运行
# import threading
# req_thread = threading.Thread(target=simulate_incoming_requests, args=(50, 256))
# req_thread.start()
# 
# # 假设 model 是一个 PyTorch nn.Module
# class DummyModel(nn.Module):
#     def forward(self, x, mask):
#         return x # 简单返回输入
# model = DummyModel()
# 
# processed_ids = dynamic_batching_inference(model)
# print(f"Processed {len(processed_ids)} requests.")
```

## 4. 实际工程中的应用

动态批处理是部署高性能深度学习推理服务（尤其是 LLM 服务）的**标准实践**。

*   **大型语言模型 (LLM) 推理：** LLM 的输入序列长度差异巨大，动态批处理是提高 LLM 服务吞吐量和降低成本的关键技术。例如，NVIDIA 的 FasterTransformer 和 Triton Inference Server 都深度支持动态批处理。
*   **语音识别：** 语音输入长度不一，动态批处理可以有效提高语音模型的推理效率。
*   **推荐系统：** 用户行为序列长度各异，动态批处理有助于优化推荐模型的服务性能。
*   **云服务提供商：** 几乎所有提供 AI 推理服务的云平台都会在底层使用动态批处理来优化资源分配和成本。

**挑战：**

*   **调度复杂性：** 实现高效的动态批处理需要复杂的调度逻辑，以平衡批次大小、序列长度、等待时间、吞吐量和延迟。
*   **内存碎片：** 动态批次可能导致 GPU 内存碎片化，需要更精细的内存管理。
*   **请求延迟：** 短请求可能需要等待其他请求到达以组成批次，从而增加其自身的延迟。需要权衡平均延迟和最大延迟。

尽管存在这些挑战，动态批处理对于实现高效、低成本的深度学习推理服务来说是不可或缺的。
