# 因果掩码 (Causal Mask)

## 1. 实现原理

因果掩码（Causal Mask），也称为“后续掩码”（Subsequent Mask），是 Transformer Decoder 在进行自注意力计算时使用的一种特殊掩码。它的核心作用是：**确保在预测当前位置的 token 时，模型只能关注到当前位置及之前的所有 token，而不能“看到”未来的 token。**

这个机制是至关重要的，因为它保证了 Decoder 在生成序列时的**自回归（Autoregressive）**特性。自回归的意思是，模型的每一步输出都只能依赖于它之前的输出，就像我们说话写字一样，是一个一个词顺序生成的。

**实现原理如下：**

1.  在 Decoder 的自注意力层中，我们首先像往常一样计算 Query (Q) 和 Key (K) 矩阵的点积，得到一个注意力分数矩阵 (Attention Score Matrix)。这个矩阵的形状是 `[seq_len, seq_len]`，其中 `score[i, j]` 代表第 `i` 个位置的 token 对第 `j` 个位置的 token 的关注度。

2.  接下来，我们创建一个与注意力分数矩阵同样大小的**上三角矩阵**作为掩码。这个矩阵的对角线及以下部分为 `True` (或 `1`)，表示允许关注；而对角线以上的部分为 `False` (或 `0`)，表示需要被屏蔽。

    例如，对于一个长度为 4 的序列，因果掩码看起来像这样：
    ```
    [[True, False, False, False],
     [True, True,  False, False],
     [True, True,  True,  False],
     [True, True,  True,  True]]
    ```

3.  我们将这个掩码应用到注意力分数矩阵上。具体做法是，将掩码中为 `False` 的位置对应的注意力分数替换成一个非常小的负数（例如 `-1e9` 或 `-infinity`）。

4.  最后，对被掩码处理过的注意力分数矩阵应用 Softmax 函数。由于那些被替换为极大负数的位置在经过 Softmax 后其概率会变得几乎为零，这就等效于模型在计算加权和时完全忽略了这些“未来”的位置。

通过这种方式，当模型在预测第 `i` 个 token 时，它的注意力权重只能分布在第 `0` 到 `i` 个 token 上，从而保证了信息的流动是单向的、符合因果关系的。

---

## 2. 所解决的问题

因果掩码主要解决了以下问题：

1.  **防止信息泄露 (Information Leakage):** 在训练序列生成任务时，我们通常会将整个目标序列一次性输入到 Decoder 中（配合 Teacher Forcing）。如果没有因果掩码，Decoder 在预测第 `i` 个词时，就能通过自注意力机制直接“偷看”到标准答案中的第 `i+1`、`i+2`... 个词。这会让模型学习到一个毫无用处的“复制”策略，而不是真正学习语言的生成规律。

2.  **保证模型的自回归能力:** 因果掩码是实现自回归解码的必要条件。它确保了模型在推理（inference）时，能够像人类一样，基于已经生成的内容，一步一步地预测未来的内容。

**与填充掩码 (Padding Mask) 的区别:**
*   **作用对象不同:** 因果掩码用于 Decoder 的自注意力层，防止看到未来信息。填充掩码用于 Encoder 和 Decoder，防止模型关注到输入序列中为了对齐长度而填充的无效 `<pad>` token。
*   **形状不同:** 因果掩码是一个上三角矩阵。填充掩码的形状则取决于输入序列中 `<pad>` token 的位置。
*   在 Decoder 的自注意力层中，这两种掩码通常会**同时使用**。

---

## 3. 核心代码

在 PyTorch 中，我们可以很方便地生成一个因果掩码并应用它。

```python
import torch
import torch.nn as nn

# 假设在多头注意力模块的 forward 方法中
# energy shape: (N, heads, query_len, key_len)

def forward(self, values, keys, query, mask): # mask 这里通常指 padding_mask
    ...
    energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])

    # --- 因果掩码的实现 ---
    N, num_heads, query_len, key_len = energy.shape
    device = energy.device

    # 创建一个上三角矩阵，对角线以上为 1 (True)
    causal_mask = torch.triu(torch.ones((query_len, key_len)), diagonal=1).bool().to(device)
    
    # 将 causal_mask 中为 True 的位置 (即未来位置) 的 energy 值设为一个极大负数
    energy.masked_fill_(causal_mask, float("-1e20"))

    # (如果存在 padding_mask，也需要在这里应用)
    if mask is not None:
        energy = energy.masked_fill(mask == 0, float("-1e20"))

    attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)
    ...
```

`torch.triu(..., diagonal=1)` 是关键，它能生成一个上三角矩阵，`diagonal=1` 表示从主对角线的上一条对角线开始填充。

---

## 4. 实际工程中的应用

因果掩码是所有基于 Transformer 的**自回归生成模型**的必备组件。

*   **GPT 系列模型 (GPT, GPT-2, GPT-3, ChatGPT):** 作为纯 Decoder-only 架构，因果掩码是其核心机制，使其能够进行大规模的语言建模和文本生成。
*   **标准 Transformer 的 Decoder:** 在机器翻译、文本摘要等 Encoder-Decoder 任务中，Decoder 部分必须使用因果掩码。
*   **多模态生成模型 (DALL-E, Imagen):** 在生成图像或视频时，如果生成过程是自回归的（例如，一个像素块接一个像素块地生成），那么同样需要使用因果掩码来确保生成顺序的合理性。

在任何需要模型按顺序、一步一步生成序列的场景中，因果掩码都是保证模型以正确方式学习和工作的基本前提。
