# 层归一化 (Layer Normalization)

## 1. 实现原理

层归一化（Layer Normalization, LayerNorm）是一种归一化技术，旨在稳定深度神经网络的训练过程。与批量归一化（Batch Normalization, BatchNorm）不同，LayerNorm 是在**单个样本**的**特征维度**上进行归一化，而不是在整个批次（batch）的样本上进行。

具体来说，对于一个给定的输入向量 `x`（在 Transformer 中，这通常是某个位置的词嵌入向量或某个子层的输出向量），LayerNorm 的计算过程如下：

1.  **计算均值 (Mean):** 计算该向量 `x` 中所有元素（即所有特征维度）的均值 `μ`。

    `μ = (1/H) * Σ(x_i)`  (其中 H 是特征维度的大小)

2.  **计算方差 (Variance):** 计算该向量 `x` 中所有元素的方差 `σ²`。

    `σ² = (1/H) * Σ((x_i - μ)²)`

3.  **归一化 (Normalize):** 使用计算出的均值和方差对向量 `x` 进行归一化，使其均值为0，方差为1。为了防止分母为零，通常会加上一个很小的常数 `ε` (epsilon)。

    `x_normalized = (x - μ) / √(σ² + ε)`

4.  **缩放与平移 (Scale and Shift):** 为了保持模型的表达能力，归一化后的向量会通过两个可学习的参数——**增益 (gain, `γ`)** 和 **偏置 (bias, `β`)**——进行缩放和平移。

    `Output = γ * x_normalized + β`

    这两个参数 `γ` 和 `β` 的维度与特征维度相同，它们在模型训练过程中被学习。初始时，`γ` 通常被初始化为1，`β` 被初始化为0。这允许模型在需要时“撤销”归一化操作，或者学习到更适合任务的分布。

在 Transformer 的结构中，LayerNorm 通常紧跟在残差连接之后，用于对每个子层（如多头注意力和前馈网络）的输出进行处理。

---

## 2. 所解决的问题

层归一化主要解决了深度网络训练中的“内部协变量偏移”（Internal Covariate Shift）问题，并带来了以下好处，尤其是在处理序列数据时：

1.  **加速模型收敛:** 通过将每一层的输入都维持在一个相对稳定、均值为0方差为1的分布范围内，LayerNorm 使得损失曲面更加平滑，从而允许使用更大的学习率，加速模型的收敛过程。

2.  **独立于批次大小 (Batch Size):** 这是 LayerNorm 相对于 BatchNorm 的最大优势。BatchNorm 需要在整个批次上计算均值和方差，因此其效果严重依赖于批次大小。当批次很小（比如只有1或2）时，BatchNorm 的统计量会非常不稳定，效果急剧下降。而 LayerNorm 完全在单个样本内部进行计算，与批次大小无关，因此在处理长度不一的序列数据、小批次训练或在线学习等场景中表现得更加稳定和高效。这正是它被用于 Transformer 的一个关键原因。

3.  **适用于序列数据 (RNN/Transformer):** 对于变长的序列数据，在不同时间步上应用 BatchNorm 是很困难且不自然的。而 LayerNorm 对每个时间步的输出向量独立进行归一化，完美地适应了序列数据的处理模式。

---

## 3. 核心代码

下面是一个使用 PyTorch 实现的简化版 LayerNorm，以帮助理解其内部计算逻辑。当然，在实际应用中，我们通常直接使用 `torch.nn.LayerNorm`。

```python
import torch
import torch.nn as nn

class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        """
        Args:
            features (int): 特征维度 (即 d_model).
            eps (float): 防止除以零的小常数.
        """
        super(LayerNorm, self).__init__()
        # 初始化可学习的缩放参数 gamma
        self.gamma = nn.Parameter(torch.ones(features))
        # 初始化可学习的平移参数 beta
        self.beta = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        # x shape: [batch_size, seq_len, d_model]
        
        # 沿最后一个维度 (特征维度) 计算均值和方差
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        
        # 归一化
        normalized = (x - mean) / (std + self.eps)
        
        # 缩放和平移
        # self.gamma 和 self.beta 的维度是 [d_model]
        # PyTorch 的广播机制会自动处理维度的匹配
        out = self.gamma * normalized + self.beta
        return out

```

---

## 4. 实际工程中的应用

层归一化是 Transformer 及其所有变体模型中不可或缺的组件。

*   **NLP 模型:** BERT, GPT, T5 等所有主流预训练语言模型都使用 LayerNorm 来稳定训练。它通常与残差连接配合使用，形成 `Add & Norm` 模块。
*   **计算机视觉:** 虽然 BatchNorm 在 CV 领域仍占主导地位，但随着 Vision Transformer (ViT) 等模型的兴起，LayerNorm 在视觉任务中的应用也越来越广泛。
*   **语音处理:** 在基于 Transformer 的语音识别和语音合成模型中，LayerNorm 同样是标准配置。

**为什么 Transformer 用 LayerNorm 而不是 BatchNorm？**

这是 Transformer 的一个经典面试题。主要原因就是上面提到的第二点：**LayerNorm 独立于批次大小，更适合处理变长的序列数据**。在 NLP 任务中，每个批次中的句子长度往往是不同的，需要进行填充（padding）。如果使用 BatchNorm，这些填充位会影响整个批次的统计量计算，带来噪声。而 LayerNorm 在每个样本内部独立计算，完全不受其他样本和填充位的影响，因此是更自然、更稳健的选择。
