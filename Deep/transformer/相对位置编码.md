# 相对位置编码 (Relative Position Encoding)

## 1. 实现原理

相对位置编码（Relative Position Encoding）是对原始 Transformer 中使用的绝对位置编码（Absolute Positional Encoding）的一种重要改进。其核心思想是：**在计算注意力分数时，不直接使用每个 token 的绝对位置，而是显式地考虑两个 token 之间的相对距离。**

**背景：绝对位置编码的局限性**

原始的绝对位置编码（无论是正弦函数式还是可学习式）是将位置信息直接“注入”到词嵌入向量中。这意味着，`Attention(pos_A, pos_B)` 和 `Attention(pos_A+k, pos_B+k)` 的计算方式是不同的，尽管两对 token 之间的相对距离是完全一样的。直觉上，相距为 2 的两个词之间的关系，不应该因为它们出现在句子的开头还是结尾而有本质的不同。

相对位置编码正是为了解决这个问题，让模型能更好地泛化到不同的位置组合。

**实现方式：**

相对位置编码有多种实现方式，但其思路都是在计算注意力分数时引入一个与相对位置相关的偏置项（bias）。

一个经典且直观的实现（来自论文《Self-Attention with Relative Position Representations》）如下：

标准的注意力分数计算是：
`score(i, j) = (W_q * x_i)^T * (W_k * x_j)`

其中 `x_i` 和 `x_j` 是加入了绝对位置编码的输入向量。

**相对位置编码的修改：**

1.  **移除输入中的绝对位置编码:** 输入向量 `x_i` 和 `x_j` 不再包含位置信息。

2.  **在注意力计算中引入相对位置项:** 将注意力分数的计算公式修改为两部分：一部分是内容的交互，另一部分是位置的交互。

    `score(i, j) = (W_q * x_i)^T * (W_k * x_j) + (W_q * x_i)^T * a_{ij}`

    这里的 `a_{ij}` 就是一个可学习的**相对位置嵌入向量**，它代表了从位置 `i` “看”向位置 `j` 的相对关系。例如，`a_{-1}` 代表“前一个词”的关系，`a_{+2}` 代表“后第二个词”的关系。

3.  **更精细的分解 (Transformer-XL):**
    后续的工作（如 Transformer-XL）对这个思想进行了扩展，将 Key 向量的计算也分解，使得相对位置的表示更加精细和高效。

**RoPE (旋转位置编码 - Rotary Position Embedding):**

这是目前最流行、效果最好的相对位置编码方法之一，被 LLaMA, PaLM 等众多先进的 LLM 所采用。

*   **核心思想:** 不再通过加法将位置信息融入，而是通过**向量旋转**。它将绝对位置 `m` 和 `n` 的信息，通过数学变换，巧妙地只保留了相对位置 `m-n` 的信息。
*   **实现方式:** 对于 Query 向量 `q` 和 Key 向量 `k`，在它们送入注意力计算之前，分别乘以一个与它们各自的绝对位置相关的旋转矩阵。

    `q'_m = R_m * q_m`
    `k'_n = R_n * k_n`

    神奇之处在于，`q'_m` 和 `k'_n` 的点积 `(q'_m)^T * k'_n`，其结果只与 `q_m`, `k_n` 以及它们的相对位置 `m-n` 有关，而与绝对位置 `m` 和 `n` 无关。这使得模型天然地具有了对相对位置的建模能力。

---

## 2. 所解决的问题

相对位置编码主要解决了以下问题：

1.  **提升模型对位置关系的泛化能力:** 让模型学习到与绝对位置无关的、更通用的相对位置规律（例如，“形容词通常出现在它修饰的名词前面”），从而在不同的句子结构中表现得更好。

2.  **更好地处理长序列:** 对于非常长的文本，绝对位置编码的泛化能力会下降。因为在训练时，模型可能没有见过那么靠后的绝对位置。而相对位置编码只关心有限范围内的相对距离（例如，-512 到 +512），因此可以更好地外推到比训练序列更长的文本上。

3.  **在某些架构中是必需的:** 对于像 Transformer-XL 这样需要处理超长序列并使用循环机制的模型，绝对位置编码是不可行的，必须使用相对位置编码。

---

## 3. 核心代码

实现 RoPE 相对位置编码相对复杂，涉及到对向量进行特定的复数或旋转操作。下面是一个简化的伪代码，以展示其核心思想。

```python
import torch

# 伪代码，示意 RoPE 的核心逻辑
def get_rotary_embedding(seq_len, embed_dim):
    # 1. 根据论文中的公式，计算出每个位置、每个维度的旋转角度 a
    # a(pos, i) = pos / (10000^(2i / embed_dim))
    ...
    return a

def apply_rotary_pos_emb(q, k, rotary_emb):
    # q, k shape: [batch, seq_len, num_heads, head_dim]
    
    # 2. 将 q, k 的最后一个维度看作复数 (real, imag, real, imag, ...)
    q_complex = q.float().reshape(*q.shape[:-1], -1, 2)
    k_complex = k.float().reshape(*k.shape[:-1], -1, 2)
    
    # 3. 将旋转角度 a 转换为复数形式 (cos(a) + i*sin(a))
    rotary_emb_complex = ... # shape [seq_len, -1, 2]
    
    # 4. 进行复数乘法，实现旋转
    q_rotated = q_complex * rotary_emb_complex
    k_rotated = k_complex * rotary_emb_complex
    
    # 5. 将旋转后的向量转换回原始形状
    q_out = q_rotated.flatten(3)
    k_out = k_rotated.flatten(3)
    
    return q_out, k_out

# 在注意力计算之前调用
# rotary_emb = get_rotary_embedding(...)
# q_rotated, k_rotated = apply_rotary_pos_emb(q, k, rotary_emb)
# scores = torch.matmul(q_rotated, k_rotated.transpose(-1, -2))
```

在实际应用中，我们通常会直接使用实现了 RoPE 的库，例如 Hugging Face `transformers` 中 LLaMA 模型的实现。

---

## 4. 实际工程中的应用

相对位置编码，特别是 RoPE，已经成为现代高性能大型语言模型（LLM）的**标配**。

*   **T5, DeBERTa:** 采用了早期的、基于偏置项的相对位置编码方法，并取得了比 BERT 更好的性能。
*   **LLaMA, LLaMA 2, Mistral:** 这些当前最先进的开源 LLM 都使用了 RoPE 作为其位置编码方案，这被认为是它们能够有效处理长上下文的关键因素之一。
*   **PaLM (Google):** 同样采用了 RoPE。
*   **多模态模型:** 在需要对序列化数据（如视频帧）进行建模时，相对位置编码也同样适用。

在需要模型处理长序列、并且对 token 间相对位置关系有较高要求的场景下，相对位置编码（尤其是 RoPE）相比于传统的绝对位置编码具有明显的优势。它是当前长上下文建模技术栈中的一个核心组件。
