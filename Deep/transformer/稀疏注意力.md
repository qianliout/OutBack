# 稀疏注意力 (Sparse Attention)

## 1. 实现原理

稀疏注意力（Sparse Attention）是一系列旨在降低标准自注意力机制计算复杂度的优化方法的总称。它的核心思想是：**放弃计算完整的、所有 token 对所有 token 的注意力分数，而是有选择地只计算一部分“重要”的 token 对之间的注意力。**

**背景：标准自注意力的瓶颈**

标准（或称为“密集”）自注意力机制需要计算一个 `N x N` 的注意力分数矩阵，其中 `N` 是序列长度。这导致其计算复杂度和内存占用都是 `O(N²)` 级别的。当序列长度 `N` 变得非常大时（例如，几千甚至上万），`N²` 会成为一个天文数字，使得模型无法在现有硬件上运行。这极大地限制了 Transformer 处理长文档、高分辨率图像等长序列任务的能力。

**稀疏注意力的基本思路：**

稀疏注意力的目标，就是将 `O(N²)` 的复杂度降低到接近线性的 `O(N log N)` 或 `O(N)`。

虽然具体的实现方法有很多种，但它们大多遵循一些共同的模式，通过组合不同的“稀疏”注意力模式来近似模拟密集的全局注意力。其中，**Longformer** 提出的组合模式是最具代表性的一种：

**a. 滑动窗口注意力 (Sliding Window Attention):**

*   **原理:** 假设每个 token 只与它在序列中的邻近 token 有强相关性。因此，对于每个 token，我们只计算它与左边 `w/2` 个和右边 `w/2` 个 token 的注意力分数（`w` 是窗口大小）。
*   **效果:** 这就像卷积神经网络（CNN）中的“局部感受野”。它能有效地捕捉局部上下文信息。其计算复杂度为 `O(N * w)`，当 `w` 是一个常数时，复杂度就是线性的 `O(N)`。

**b. 膨胀/空洞滑动窗口 (Dilated Sliding Window):**

*   **原理:** 为了在不增加计算量的情况下扩大感受野，可以引入“空洞”（Dilation）。即在计算注意力时，邻近的窗口位置之间会跳过一些 token。例如，一个空洞大小为 2 的窗口会关注位置 `i-4, i-2, i, i+2, i+4`。
*   **效果:** 这使得上层的注意力头可以关注到更远距离的依赖关系。

**c. 全局注意力 (Global Attention):**

*   **原理:** 仅仅有局部注意力是不够的，因为序列中可能存在一些特殊的、需要与所有其他 token 交互的“明星” token（例如，BERT 中的 `[CLS]` token，或者任务中最重要的关键词）。因此，我们会选择一小部分 token，让它们可以关注序列中的所有其他 token，同时所有其他 token 也可以关注它们。
*   **效果:** 这弥补了滑动窗口注意力无法捕捉长距离依赖的缺陷，通过少数几个“全局节点”来汇总和分发信息。

**Longformer 的组合策略：**

Longformer 将**滑动窗口注意力**和**全局注意力**结合在一起。对于大部分 token，使用滑动窗口注意力；对于少数预先选定的重要 token，使用全局注意力。通过这种组合，Longformer 在保持线性计算复杂度的同时，有效地模拟了全局的注意力覆盖范围。

---

## 2. 所解决的问题

稀疏注意力主要解决了**标准 Transformer 在处理长序列时的计算效率和内存占用问题**。

*   **降低计算复杂度:** 将 `O(N²)` 的复杂度降低到 `O(N)` 或 `O(N log N)`，使得处理长序列成为可能。
*   **减少内存占用:** 避免了实例化一个巨大的 `N x N` 注意力矩阵，从而大幅节省了显存。
*   **扩展 Transformer 的应用范围:** 使 Transformer 能够被应用于长文档理解、长篇问答、基因组数据分析、高分辨率图像处理等以前无法涉足的领域。

---

## 3. 核心代码

实现稀疏注意力通常比标准注意力要复杂得多，因为它涉及到对注意力矩阵进行复杂的索引和掩码操作。幸运的是，Hugging Face `transformers` 库已经为一些支持稀疏注意力的模型（如 Longformer）提供了内置实现。

下面是使用 Longformer 的一个示例，展示了如何配置全局注意力。

```python
import torch
from transformers import LongformerTokenizer, LongformerModel

# 1. 加载 Tokenizer 和模型
tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
model = LongformerModel.from_pretrained('allenai/longformer-base-4096')

# 2. 准备一个长文本输入
long_text = "..." # 一个非常长的文本字符串，长度可以达到 4096

# 3. 编码文本
encoding = tokenizer(long_text, return_tensors="pt")
input_ids = encoding.input_ids

# 4. 设置全局注意力
# Longformer 要求我们明确指定哪些 token 需要拥有全局注意力。
# 通常，我们会给序列的第一个 token ([CLS]) 设置全局注意力。
attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)

global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device)
# 将第一个 token ([CLS]) 的全局注意力掩码设置为 1
global_attention_mask[:, 0] = 1

# 5. 将输入和掩码传入模型
with torch.no_grad():
    outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)

# 6. 获取输出
sequence_output = outputs.last_hidden_state

print(sequence_output.shape)
```

在这个例子中，`attention_mask` 是标准的填充掩码，而 `global_attention_mask` 是 Longformer 特有的，用于指定哪些位置需要应用全局注意力。

---

## 4. 实际工程中的应用

稀疏注意力是长序列建模领域的一个核心研究方向，并已在多个场景中得到应用。

*   **长文档处理:** Longformer, BigBird, ETC 等模型被广泛用于需要理解整篇长文档的任务，如法律文件分析、科学文献阅读、长篇小说摘要等。
*   **多模态任务:** 在处理高分辨率图像或长视频时，也需要类似稀疏注意力的机制来管理计算量。
*   **生物信息学:** 分析长链 DNA 或蛋白质序列。

**其他稀疏注意力方法：**

除了 Longformer 的模式外，还有许多其他类型的稀疏注意力，例如：
*   **BigBird:** 结合了随机注意力、窗口注意力和全局注意力，提供了更强的理论保证。
*   **Reformer:** 使用局部敏感哈希（LSH）来选择性地计算注意力，将复杂度降低到 `O(N log N)`。
*   **Routing Transformer:** 通过聚类的方式，让每个 query 只关注与之最相似的 key。

随着对超长上下文窗口需求的增加，稀疏注意力和各种近似注意力（Approximated Attention）的方法，是当前大型语言模型发展的一个关键技术前沿。
