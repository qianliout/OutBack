# 自回归生成 (Autoregressive Generation)

## 1. 实现原理

自回归（Autoregressive, AR）生成是一种序列生成模型的工作方式。其核心思想是：**序列中的每一个元素都是由模型根据之前已经生成的所有元素来预测的。** 换句话说，模型在生成第 `t` 个 token 时，会将第 `1` 到 `t-1` 个 token 作为条件输入。

这个过程可以用条件概率来描述：

`p(y_1, ..., y_n | x) = p(y_1 | x) * p(y_2 | y_1, x) * ... * p(y_n | y_1, ..., y_{n-1}, x)`

其中：
*   `x` 是输入序列（例如，在机器翻译中是源语言句子）。
*   `y_i` 是要生成的目标序列中的第 `i` 个 token。

**在 Transformer Decoder 中的实现流程如下：**

1.  **起始步骤 (t=1):**
    *   将源语言句子的编码结果（Encoder 的输出）和目标序列的起始符 `<SOS>` (Start of Sentence) 输入到 Decoder 中。
    *   Decoder 通过前向传播，在其输出层（通常是一个大的线性层接 Softmax）上生成一个覆盖整个词汇表的概率分布。
    *   从这个概率分布中选择一个词作为第一个生成的词 `y_1`（例如，通过选择概率最高的词，即贪心搜索）。

2.  **循环步骤 (t > 1):**
    *   将上一步生成的词 `y_{t-1}` 添加到已生成的序列末尾。
    *   将这个新的、更长的序列（`<SOS>`, `y_1`, ..., `y_{t-1}`）作为 Decoder 的新输入。
    *   Decoder 再次进行前向传播，生成一个新的概率分布，用于预测下一个词 `y_t`。
    *   再次从概率分布中选择一个词 `y_t`。

3.  **终止步骤:**
    *   重复步骤2，直到模型生成一个特殊的结束符 `<EOS>` (End of Sentence)，或者达到了预设的最大生成长度。

在这个过程中，**因果掩码 (Causal Mask)** 扮演着至关重要的角色，它确保了在每一步计算中，Decoder 都无法“偷看”到未来的信息。同时，为了提高效率，通常会使用 **KV Cache** 技术来缓存之前时间步的 Key 和 Value 向量，避免重复计算。

---

## 2. 所解决的问题

自回归生成是解决**序列生成任务**的一种基本且自然的方式。它旨在让机器像人类一样，循序渐进、有条不紊地创造内容。

*   **建模序列的联合概率分布:** 自回归模型通过链式法则将复杂的联合概率分布分解为一系列简单的条件概率，使得建模和计算成为可能。
*   **适用于任意长度的序列生成:** 理论上，只要模型不生成结束符，自回归过程可以无限进行下去，从而生成任意长度的序列。
*   **保证生成内容的连贯性:** 由于每一步的生成都依赖于之前的所有内容，模型可以维持生成序列的上下文连贯性和逻辑流畅性。

所有需要“无中生有”创造新文本的任务，都离不开自回归生成机制。

---

## 3. 核心代码

下面是一个简化的伪代码，展示了在推理（inference）阶段，如何使用 Transformer Decoder 进行自回归生成（以贪心搜索为例）。

```python
import torch
import torch.nn.functional as F

def greedy_decode(model, src, src_mask, max_len, start_symbol_idx, end_symbol_idx):
    """
    Args:
        model: 训练好的 Encoder-Decoder Transformer 模型.
        src: 输入序列的索引, shape: [1, src_len].
        src_mask: 输入序列的填充掩码.
        max_len: 最大生成长度.
        start_symbol_idx: 起始符 <SOS> 的索引.
        end_symbol_idx: 结束符 <EOS> 的索引.
    """
    model.eval() # 切换到评估模式

    # 1. 对输入序列进行编码
    encoder_output = model.encode(src, src_mask)

    # 2. 初始化 Decoder 的输入，只包含一个 <SOS> token
    ys = torch.ones(1, 1).fill_(start_symbol_idx).type_as(src.data)

    # 3. 循环生成
    for i in range(max_len - 1):
        # 创建 Decoder 的因果掩码
        tgt_mask = model.generate_square_subsequent_mask(ys.size(1)).type_as(src.data)
        
        # 前向传播，得到下一个词的 logits
        # 注意：这里没有使用 Teacher Forcing，输入 ys 是模型自己生成的
        out = model.decode(encoder_output, src_mask, ys, tgt_mask)
        prob = model.generator(out[:, -1]) # 只取最后一个时间步的输出
        
        # 4. 选择概率最高的词
        _, next_word_idx = torch.max(prob, dim=1)
        next_word_idx = next_word_idx.item()

        # 5. 将新生成的词拼接到序列末尾
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word_idx)], dim=1)

        # 6. 检查是否生成结束符
        if next_word_idx == end_symbol_idx:
            break
            
    return ys

```

---

## 4. 实际工程中的应用

自回归生成是所有现代生成式 AI 模型的核心工作模式。

*   **大型语言模型 (LLM):** ChatGPT, LLaMA, Claude 等所有 LLM 都是自回归模型。你输入的提示（Prompt）作为初始条件，模型在此基础上逐词（或 token）生成回复。
*   **机器翻译:** 将源语言句子翻译成目标语言。
*   **文本摘要:** 阅读一篇文章，并自回归地生成其摘要。
*   **代码生成:** 如 GitHub Copilot，根据代码注释和上下文，自回归地生成代码片段。
*   **图像生成 (部分模型):** 像 PixelCNN/PixelRNN 或 VQ-VAE 的第二阶段，会自回归地生成图像中的像素点或离散的图像标记。
*   **语音合成 (Text-to-Speech):** 根据输入的文本，自回归地生成音频波形或声谱图。

**自回归生成的缺点:**

*   **生成速度慢:** 因为生成过程是串行的，必须生成完第 `t` 个词才能开始生成第 `t+1` 个词，所以无法并行化，导致推理速度较慢。这是其在工程应用中的主要瓶颈。
*   **误差累积:** 与训练时的 Teacher Forcing 不同，推理时的任何一个错误都可能影响后续所有内容的生成质量。

为了克服这些缺点，研究者们提出了非自回归（Non-Autoregressive）生成模型，但目前生成质量和通用性仍以自回归模型为主流。
