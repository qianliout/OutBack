# 自注意力机制 (Self-Attention)

## 1. 实现原理

自注意力（Self-Attention）机制是 Transformer 模型的核心，它允许模型在处理序列数据（如文本）时，动态地衡量不同单词之间的重要性。简单来说，就是模型在编码某个单词时，会关注输入序列中所有其他的单词，并根据相关性计算出一个加权的上下文表示。

这个过程可以分解为以下几个步骤：

**a. Q, K, V 的创建:**

首先，对于输入序列中的每一个单词（或称为 token），我们都会通过三个不同的、可学习的权重矩阵（`W_q`, `W_k`, `W_v`）将其原始词嵌入向量（embedding）分别线性变换为三个新的向量：

*   **查询向量 (Query, Q):** 代表当前单词，用于去“查询”序列中其他单词与自己的相关性。
*   **键向量 (Key, K):** 代表序列中被查询的单词，用于和 Q 进行匹配。
*   **值向量 (Value, V):** 代表序列中被查询单词的实际内容。

你可以把这个过程想象成在图书馆查资料：你的**查询 (Q)** 是你想要查找的主题，书架上每本书的**键 (K)** 是书的标题或标签，而书的**值 (V)** 则是书本的实际内容。你用你的 Q 去和所有书的 K 进行匹配，匹配度越高，那本书的 V 对你就越重要。

**b. 计算注意力分数 (Attention Score):**

为了计算当前单词与序列中其他所有单词的相关性，我们将当前单词的 Q 向量与所有单词的 K 向量进行点积运算。这个分数衡量了两个单词之间的“匹配度”。

```
Score(q, k_i) = q · k_i
```

**c. 分数缩放 (Scaling):**

点积的结果可能会变得非常大，这会导致后续的 Softmax 函数进入梯度非常小的区域，不利于模型训练。为了缓解这个问题，需要将分数除以一个缩放因子，通常是 K 向量维度的平方根 (`√d_k`)。

```
Scaled Score = Score / √d_k
```

**d. Softmax 归一化:**

接下来，对所有缩放后的分数应用 Softmax 函数，将其转换为一个概率分布，所有分数的总和为 1。这个概率分布就是**注意力权重 (Attention Weights)**，它代表了在编码当前单词时，应该给予序列中其他每个单词多少“关注度”。

```
Attention Weights = softmax(Scaled Scores)
```

**e. 加权求和:**

最后，将得到的注意力权重与每个单词对应的 V 向量相乘，然后将所有结果加权求和。这样就得到了当前单词的最终上下文表示 (Context Vector)。这个向量包含了整个序列的加权信息，而不仅仅是当前单词本身。

```
Context Vector = Σ (Attention Weight_i * V_i)
```

**总结公式:**

将以上步骤合并，就得到了自注意力的核心公式：

```
Attention(Q, K, V) = softmax( (Q * K^T) / √d_k ) * V
```

---

## 2. 所解决的问题

自注意力机制主要解决了传统序列模型（如 RNN、LSTM）在处理长序列时遇到的两个核心问题：

*   **长距离依赖问题 (Long-Range Dependencies):** RNN 是一种顺序处理模型，信息需要通过时间步一步步传递。当序列很长时，早期的信息很容易在传递过程中丢失或被稀释，导致模型难以捕捉相距很远的单词之间的依赖关系。自注意力机制通过直接计算序列中任意两个单词之间的关联，使得任意两个位置之间的距离都为 1，从而完美地解决了长距离依赖问题。
*   **计算效率问题:** RNN 的计算是串行的，必须等待前一个时间步计算完成后才能开始下一个，这限制了模型的并行计算能力。自注意力机制中，每个单词的计算都可以独立进行，不依赖于前一个单词的计算结果，因此可以大规模并行化，极大地提高了训练效率。

---

## 3. 核心代码

下面是一个使用 PyTorch 实现的简化版自注意力模块，以帮助你理解其内部工作原理。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        """
        Args:
            embed_size (int): 输入词嵌入的维度.
            heads (int): 注意力头的数量 (在多头注意力中会用到).
                         这里为了简化，我们先只关注单个头的情况.
        """
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        # 定义 Q, K, V 的线性变换层
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        
        # 输出的线性层
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values, keys, query, mask):
        # 获取输入序列的样本数
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # 将输入拆分到多个头
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)

        # 通过线性层得到 Q, K, V
        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)

        # 计算 Q, K 的点积 (energy)
        # queries shape: (N, query_len, heads, head_dim)
        # keys shape: (N, key_len, heads, head_dim)
        # energy shape: (N, heads, query_len, key_len)
        energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])

        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # 计算注意力权重
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

        # 加权求和
        # attention shape: (N, heads, query_len, key_len)
        # values shape: (N, value_len, heads, head_dim)
        # out shape: (N, query_len, heads, head_dim)
        out = torch.einsum("nhql,nlhd->nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # 通过输出线性层
        out = self.fc_out(out)
        return out

```

---

## 4. 实际工程中的应用

自注意力机制是现代深度学习模型（尤其是自然语言处理领域）的基石，其应用非常广泛：

*   **机器翻译:** 在谷歌的 Transformer 模型中首次被提出，用于提升翻译的准确性和流畅度。
*   **文本摘要:** 模型能够理解全文的重点，并生成概括性的摘要。
*   **情感分析:** 通过理解句子中各个词语之间的关系，更准确地判断文本的情感倾向。
*   **问答系统:** 模型可以理解问题和上下文之间的关联，从而找到最相关的答案。
*   **代码生成:** 如 GitHub Copilot，通过理解代码上下文，自动生成代码片段。
*   **蛋白质结构预测:** AlphaFold 2 也借鉴了注意力机制的思想来分析氨基酸序列之间的关系。
