以下是针对面试准备的**30个Transformer核心知识点**和**20道高频面试题**，涵盖原理、实现、优化和前沿扩展，帮助你系统掌握Transformer技术。

---

### **一、30个Transformer核心知识点**  
#### **1. 基础架构**  
1. **自注意力机制（Self-Attention）**：计算输入序列的加权表示，公式：  
   $$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$  
2. **多头注意力（Multi-Head Attention）**：并行多个注意力头，增强模型捕捉不同特征的能力。  
3. **位置编码（Positional Encoding）**：注入序列位置信息，常用正弦/余弦函数或可学习参数。  
4. **残差连接（Residual Connection）**：缓解梯度消失，公式：$y = x + \text{Layer}(x)$。  
5. **层归一化（LayerNorm）**：对特征维度归一化，加速收敛。  

#### **2. 训练与优化**  
6. **Teacher Forcing**：训练时使用真实标签作为解码器输入。  
7. **标签平滑（Label Smoothing）**：防止模型对标签过度自信。  
8. **学习率预热（Warmup）**：初始阶段逐步增大学习率，避免早期震荡。  
9. **梯度裁剪（Gradient Clipping）**：防止梯度爆炸。  
10. **Dropout应用**：在注意力权重和FFN层中使用。  

#### **3. 解码器特性**  
11. **因果掩码（Causal Mask）**：确保解码器只能看到当前位置及之前的token。  
12. **自回归生成（Autoregressive）**：推理时逐token生成，依赖前序输出。  
13. **KV Cache**：推理时缓存历史Key-Value，降低计算复杂度。  

#### **4. 变体与扩展**  
14. **BERT**：仅编码器，使用MLM（掩码语言模型）预训练。  
15. **GPT**：仅解码器，自回归语言模型。  
16. **T5**：统一文本到文本框架，编码器-解码器结构。  
17. **稀疏注意力**：如Longformer的局部+全局注意力，处理长序列。  
18. **相对位置编码**：如RoPE（旋转位置编码），更好处理长文本。  

#### **5. 效率与部署**  
19. **模型量化**：降低权重精度（如FP32→INT8）以减少内存占用。  
20. **知识蒸馏**：用小模型模拟大模型行为。  
21. **剪枝（Pruning）**：移除冗余权重或注意力头。  

#### **6. 数学细节**  
22. **注意力分数缩放**：除以$\sqrt{d_k}$防止softmax梯度消失。  
23. **QKV矩阵计算**：同一输入线性变换得到Query、Key、Value。  
24. **FFN层**：两层全连接（通常中间维度扩大4倍）。  

#### **7. 前沿方向**  
25. **视觉Transformer（ViT）**：将图像分块输入Transformer。  
26. **跨模态模型**：如CLIP（文本-图像对齐）。  
27. **MoE架构**：如Switch Transformer，动态激活专家模块。  
28. **低秩适配（LoRA）**：微调时仅训练低秩矩阵增量。  
29. **RetNet**：结合递归和注意力，提升推理效率。  
30. **Mamba**：状态空间模型+选择性注意力，替代Transformer？  

---

  

如果需要更详细的答案或代码示例，可以告诉我具体题目！

### **二、20道Transformer面试题**  
#### **基础原理**  
1. 为什么Transformer比RNN更适合处理长序列？  
2. 自注意力机制的时间复杂度是多少？如何优化？  
3. 为什么需要位置编码？可学习的位置编码和正弦编码各有什么优缺点？  
4. 解释多头注意力的作用，为什么“多头”比“单头”好？  

#### **训练与推理**  
5. 训练时Decoder的输入和推理时有什么区别？  
6. 为什么推理时Decoder需要KV Cache？如何实现？  
7. 如何处理推理时生成的序列长度超过训练时的最大长度？  

#### **数学与实现**  
8. 推导自注意力机制的梯度（$\frac{\partial L}{\partial W_Q}$）。  
9. LayerNorm和BatchNorm的区别？为什么Transformer用LayerNorm？  
10. 为什么注意力分数要除以$\sqrt{d_k}$？  

#### **模型变种**  
11. BERT的MLM任务具体如何实现？和自回归语言模型（如GPT）有什么区别？  
12. 解释T5的“Text-to-Text”框架如何统一不同任务。  
13. Longformer如何将注意力复杂度从$O(n^2)$降到$O(n)$？  

#### **优化与部署**  
14. 如何对Transformer模型进行动态批处理（Dynamic Batching）？  
15. 知识蒸馏在Transformer中如何应用？举例说明。  
16. 模型量化会带来哪些性能损失？如何缓解？  

#### **开放问题**  
17. 如果让你设计一个更高效的Transformer，你会从哪些方面改进？  
18. Transformer在视觉任务（如检测、分割）中的局限性是什么？  
19. 为什么ChatGPT/LLM普遍基于Decoder-only架构？  
20. 你认为Transformer会被其他架构（如Mamba）取代吗？为什么？  

---

### **三、回答技巧**  
- **结合公式**：如解释注意力机制时写出数学公式。  
- **对比分析**：如比较RNN/Transformer/Mamba的优缺点。  
- **举例说明**：提到具体模型（如BERT、GPT-3）的实现细节。  
- **强调工程经验**：如部署时的KV Cache优化、量化实践。