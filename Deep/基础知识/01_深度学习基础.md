# 01_深度学习基础

## 正则化方法详解

# 深度学习中的正则化方法详解

## 概述

正则化技术用于防止模型过拟合，提高泛化能力，主要通过对模型训练过程施加约束或引入噪声来实现。

## 1. Dropout

### 解决的问题

1. 神经元之间的复杂共适应(co-adaptation)
2. 过拟合问题
3. 模型对特定特征的过度依赖

### 实现原理

训练时以概率 $p$ 随机"关闭"神经元：

$$y = \begin{cases}
\frac{x}{1-p} & \text{概率}1-p \\
0 & \text{概率}p
\end{cases}$$

### PyTorch实现

```python
import torch.nn as nn

# 定义带Dropout的网络
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 512)
        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(512, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)  # 训练时随机置零
        x = self.fc2(x)
        return x

# 训练时启用dropout
model.train()
# 测试时关闭dropout
model.eval()
```

### 优点

1. 实现简单高效
2. 相当于隐式地集成(ensemble)多个子网络
3. 适用于各种网络结构
4. 不需要修改损失函数

### 缺点

1. 训练时间可能延长(需要更多迭代)
2. 需要调整dropout率 $p$
3. 与BatchNorm同时使用时可能需要调整

### 典型应用

- 全连接层(效果最显著)
- CNN(通常放在最后全连接层)
- 防止过拟合的标准选择

## 2. L1/L2正则化(权重衰减)

### 解决的问题

1. 控制权重幅度
2. 防止参数过度增长
3. L1可产生稀疏解

### 数学表达式

**L2正则化**：
$$J_{L2} = J_0 + \lambda \sum_{i} w_i^2$$

**L1正则化**：
$$J_{L1} = J_0 + \lambda \sum_{i} |w_i|$$

其中 $\lambda$ 是正则化系数

### PyTorch实现

```python
# L2正则化（权重衰减）
optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)

# L1正则化（手动实现）
def l1_regularization(model, lambda_l1=1e-4):
    l1_loss = 0
    for param in model.parameters():
        l1_loss += torch.sum(torch.abs(param))
    return lambda_l1 * l1_loss

# 在训练循环中使用
loss = criterion(outputs, targets) + l1_regularization(model)
```

### 优点

1. L2使权重平滑分布
2. L1能自动进行特征选择
3. 数学理论完备
4. 实现简单

### 缺点

1. 需要仔细调整 $\lambda$
2. L1在深度学习中使用较少(因网络本身有冗余性)
3. 可能干扰优化过程

## 3. 早停(Early Stopping)

### 解决的问题

1. 防止训练过程过拟合
2. 自动确定最佳训练轮数

### 实现方式

1. 在验证集上监控性能
2. 当性能不再提升时停止训练
3. 保留验证集最佳参数的副本

### PyTorch实现

```python
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
    
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        
        return self.counter >= self.patience

# 使用示例
early_stopping = EarlyStopping(patience=10)
for epoch in range(max_epochs):
    train_loss = train_epoch()
    val_loss = validate_epoch()
    
    if early_stopping(val_loss):
        print("Early stopping triggered")
        break
```

### 优点

1. 不需要修改模型结构
2. 计算开销小
3. 可以与其他正则化方法结合

### 缺点

1. 需要验证集
2. 可能过早停止(如果验证指标波动)
3. 浪费了部分训练数据(验证集)

## 4. 数据增强(Data Augmentation)

### 解决的问题

1. 训练数据不足
2. 提高模型对输入变化的鲁棒性

### 实现方式

对输入数据应用随机变换：

- **图像**: 旋转/翻转/裁剪/颜色抖动
- **文本**: 同义词替换/随机删除
- **音频**: 变速/加噪声

### PyTorch实现

```python
import torchvision.transforms as transforms

# 图像数据增强
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])
])

# 文本数据增强示例
def text_augmentation(text):
    # 随机删除单词
    words = text.split()
    if len(words) > 3:
        idx = random.randint(0, len(words)-1)
        words.pop(idx)
    return ' '.join(words)
```

### 优点

1. 不增加推理计算量
2. 物理意义明确
3. 效果通常非常显著

### 缺点

1. 需要领域知识设计增强策略
2. 某些增强可能破坏原始信息
3. 计算开销较大

## 5. 标签平滑(Label Smoothing)

### 解决的问题

1. 防止模型对标签过度自信
2. 缓解错误标注的影响

### 数学表达式

$$y_{LS} = (1 - \epsilon) \cdot y + \epsilon \cdot u(k)$$

其中：
- $y$：原始的 one-hot 编码标签
- $\epsilon$：平滑系数（通常 0.1 ~ 0.2）
- $u(k)$：均匀分布向量（每个元素值为 $1/k$）
- $y_{LS}$：平滑后的标签

### PyTorch实现

```python
# 使用内置的标签平滑
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

# 手动实现
def label_smoothing_loss(predictions, targets, epsilon=0.1, num_classes=10):
    # 创建平滑标签
    smooth_targets = torch.zeros_like(predictions)
    smooth_targets.fill_(epsilon / (num_classes - 1))
    smooth_targets.scatter_(1, targets.unsqueeze(1), 1 - epsilon)
    
    # 计算交叉熵
    return -torch.sum(smooth_targets * torch.log_softmax(predictions, dim=1), dim=1).mean()
```

## 6. Dropout与BatchNorm同时使用

### 注意事项

1. **训练顺序**：通常先BatchNorm再Dropout
2. **测试模式**：BatchNorm使用运行时统计，Dropout关闭
3. **学习率调整**：可能需要降低学习率

### 最佳实践

```python
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(512, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)  # 先BatchNorm
        x = torch.relu(x)
        x = self.dropout(x)  # 再Dropout
        x = self.fc2(x)
        return x
```

## 正则化方法选择指南

| 场景 | 推荐方法 | 说明 |
|------|----------|------|
| 全连接层过拟合 | Dropout | 最有效的正则化方法 |
| 权重过大 | L2正则化 | 控制权重幅度 |
| 特征选择 | L1正则化 | 产生稀疏解 |
| 训练时间过长 | 早停 | 自动确定最佳轮数 |
| 数据不足 | 数据增强 | 增加训练样本 |
| 过度自信 | 标签平滑 | 提高泛化能力 |

## 组合使用建议

1. **基础组合**：L2正则化 + Dropout + 早停
2. **数据增强**：根据任务类型选择合适的增强策略
3. **标签平滑**：在分类任务中配合使用
4. **BatchNorm**：与Dropout配合时注意顺序 

## 损失函数详解

# 深度学习中的损失函数详解

## 概述

损失函数(Loss Function)是深度学习的核心组件，它量化模型预测与真实值之间的差异，指导模型优化方向。

### 损失函数特性

- **非负性**：$L(\hat{y}, y) \geq 0$
- **一致性**：预测越准确，损失越小
- **可导性**：便于梯度下降优化

## 分类任务损失函数

### 1. 交叉熵损失(Cross-Entropy Loss)

#### 数学原理

$$L = -\frac{1}{N}\sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(p_{i,c})$$

其中：
- $N$: 样本数量
- $C$: 类别数量
- $y_{i,c}$: 样本i的真实类别c的指示器(0或1)
- $p_{i,c}$: 样本i属于类别c的预测概率

#### 二分类交叉熵损失

$$L = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]$$

其中 $y_i \in \{0,1\}$ 是真实标签，$p_i \in (0,1)$ 是模型预测为正类的概率。

#### PyTorch实现

```python
import torch.nn as nn

# 多分类任务
criterion = nn.CrossEntropyLoss()  # 内置softmax
loss = criterion(outputs, labels)  # labels形状：(N,)

# 二分类任务
criterion = nn.BCELoss()  # 输入需经过sigmoid
# 或直接使用Logits版本（数值稳定）
criterion = nn.BCEWithLogitsLoss()  # 内置sigmoid
loss = criterion(logits, targets)
```

#### 使用场景
- 多分类问题
- 二分类问题
- 输出层配合Softmax/Sigmoid使用

### 2. 负对数似然损失(NLL Loss)

#### 数学原理

$$L = -\frac{1}{N}\sum_{i=1}^N \log(p_{i,y_i})$$

#### PyTorch实现

```python
criterion = nn.NLLLoss()
loss = criterion(log_probs, labels)  # labels形状：(N,)
```

#### 使用场景
- 需要自定义log softmax时
- 与LogSoftmax层配合使用
- 语言模型中更灵活的概率处理

## 回归任务损失函数

### 1. 均方误差损失(MSE Loss)

#### 数学原理

$$L = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2$$

#### PyTorch实现

```python
criterion = nn.MSELoss()
loss = criterion(predictions, targets)
```

#### 使用场景
- 连续值预测
- 对异常值敏感
- 例如：房价预测、温度预测

### 2. 平均绝对误差损失(MAE/L1 Loss)

#### 数学原理

$$L = \frac{1}{N}\sum_{i=1}^N |y_i - \hat{y}_i|$$

#### PyTorch实现

```python
criterion = nn.L1Loss()
loss = criterion(predictions, targets)
```

#### 使用场景
- 对异常值更鲁棒
- 需要稀疏梯度时
- 例如：图像重建、金融预测

### 3. Huber损失(Smooth L1 Loss)

#### 数学原理

$$L = \begin{cases} 
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y-\hat{y}| < \delta \\
\delta|y-\hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}$$

#### PyTorch实现

```python
criterion = nn.SmoothL1Loss(beta=1.0)  # beta即delta
loss = criterion(predictions, targets)
```

#### 使用场景
- 结合MSE和MAE优点
- 目标检测中的边界框回归
- 例如：Faster R-CNN

## 特殊任务损失函数

### 1. 对比损失(Contrastive Loss)

#### 数学原理

$$L = \frac{1}{2N}\sum_{i=1}^N [y_i d_i^2 + (1-y_i)\max(0, m - d_i)^2]$$

其中 $d_i$ 是样本对的距离，$m$ 是边界margin

#### PyTorch实现

```python
def contrastive_loss(distance, label, margin=1.0):
    loss = label * torch.pow(distance, 2) + \
           (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2)
    return torch.mean(loss)
```

#### 使用场景
- 度量学习
- 人脸识别
- 相似度学习

### 2. 标签平滑损失(Label Smoothing)

#### 数学原理

将真实标签从1调整为略小的值（如0.9），避免模型过度自信：

$$y_{smooth} = (1 - \alpha) \cdot y + \alpha / K$$

其中 $\alpha$ 是平滑参数，$K$ 是类别数。

#### PyTorch实现

```python
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
loss = criterion(outputs, labels)
```

## 数值稳定性技巧

### Log-Sum-Exp技巧

在计算Softmax或交叉熵时，直接计算 $e^{z_i}$ 可能导致数值溢出。使用LSE技巧：

$$\text{LSE}(\mathbf{z}) = \log \left( \sum_{i=1}^n e^{z_i} \right) = \log \left( \sum_{i=1}^n e^{z_i - \max(\mathbf{z})} \right) + \max(\mathbf{z})$$

#### 数值稳定的Softmax

```python
def stable_softmax(z):
    m = np.max(z)
    exp_z = np.exp(z - m)  # 数值稳定
    return exp_z / np.sum(exp_z)
```

## 损失函数选择指南

| 任务类型 | 推荐损失函数 | 说明 |
|---------|-------------|------|
| 二分类 | BCEWithLogitsLoss | 数值稳定，内置sigmoid |
| 多分类 | CrossEntropyLoss | 内置softmax，最常用 |
| 回归 | MSELoss | 标准回归损失 |
| 回归(鲁棒) | SmoothL1Loss | 对异常值更鲁棒 |
| 度量学习 | ContrastiveLoss | 学习相似度 |

## 常见问题与解决方案

### 1. 类别不平衡
使用加权交叉熵：
$$L = -\sum w_c \cdot y_c \log(p_c)$$

### 2. 数值稳定性
- 使用Log-Sum-Exp技巧
- 选择带Logits版本的损失函数
- 适当的学习率调整

### 3. 梯度消失/爆炸
- 使用合适的激活函数
- 梯度裁剪
- 批量归一化 

## 激活函数详解

# 深度学习中的激活函数详解

## 概述

激活函数是神经网络中的非线性变换，为模型引入非线性能力，使其可以学习复杂模式。数学表示为：

$$y = f(z)$$

其中 $z$ 是输入信号（通常是线性组合 $\sum w_i x_i + b$）

## 常用激活函数

### 1. Sigmoid（Logistic函数）

#### 数学表达式

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

#### 特点
- 将输入压缩到 (0,1) 区间
- 早期神经网络常用

#### 优点
1. 输出可解释为概率
2. 平滑梯度

#### 缺点
1. 容易导致梯度消失（当 $|z|$ 较大时梯度接近0）
2. 输出不以0为中心
3. 指数运算计算成本较高

#### 使用场景
- 二分类问题的输出层
- 需要概率解释的场合

### 2. Tanh（双曲正切函数）

#### 数学表达式

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

#### 特点
- 输出范围 (-1,1)
- 比sigmoid更常用

#### 优点
1. 输出以0为中心
2. 梯度比sigmoid更强

#### 缺点
1. 仍然存在梯度消失问题

#### 使用场景
- RNN/LSTM等循环网络
- 隐藏层的激活

### 3. ReLU（Rectified Linear Unit）

#### 数学表达式

$$\text{ReLU}(z) = \max(0, z)$$

#### 特点
- 当前最常用的激活函数
- 计算简单高效

#### 优点
1. 计算效率高（无需指数运算）
2. 缓解梯度消失问题（正区间梯度为1）
3. 促进稀疏激活

#### 缺点
1. "Dying ReLU"问题（负输入时梯度为0）
2. 输出不以0为中心

#### 使用场景
- CNN和大多数前馈神经网络的隐藏层
- 计算机视觉任务

### 4. Leaky ReLU

#### 数学表达式

$$\text{LeakyReLU}(z) = \begin{cases} 
z & \text{if } z > 0 \\
\alpha z & \text{if } z \leq 0 
\end{cases}$$

（通常 $\alpha=0.01$）

#### 特点
- ReLU的改进版

#### 优点
1. 解决Dying ReLU问题
2. 保持ReLU的大部分优点

#### 缺点
1. 需要手动设置或学习 $\alpha$ 参数

#### 使用场景
- 当担心ReLU神经元"死亡"时
- GAN等对抗性网络

### 5. Softmax

#### 数学表达式

$$\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \quad \text{对于} j=1,...,K$$

#### 特点
- 多类分类的标准选择

#### 优点
1. 输出可解释为概率分布
2. 适合多分类问题

#### 缺点
1. 对极端值敏感
2. 计算成本较高

#### 使用场景
- 多分类问题的输出层

## 高级激活函数

### Swish（Google提出）

$$\text{Swish}(z) = z \cdot \sigma(\beta z)$$

- 结合了ReLU和Sigmoid的特性
- 在深层网络中表现优于ReLU

### GELU（高斯误差线性单元）

$$\text{GELU}(z) = z \Phi(z)$$

其中 $\Phi(z)$ 是标准正态分布的累积分布函数

- 被BERT、GPT等Transformer模型采用

## 选择建议

1. **隐藏层**：优先尝试ReLU或其变体（Leaky ReLU等）
2. **二分类输出层**：Sigmoid
3. **多分类输出层**：Softmax
4. **RNN/LSTM**：Tanh或Sigmoid

## PyTorch实现示例

```python
import torch
import torch.nn as nn

# 定义激活函数
relu = nn.ReLU()
sigmoid = nn.Sigmoid()
tanh = nn.Tanh()
softmax = nn.Softmax(dim=1)
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
gelu = nn.GELU()

# 使用示例
x = torch.randn(10)
print(f"ReLU: {relu(x)}")
print(f"Sigmoid: {sigmoid(x)}")
print(f"Tanh: {tanh(x)}")
``` 

## 词向量与预训练详解

# 词向量与预训练模型详解

## 1. GloVe（Global Vectors for Word Representation）

### 核心思想

GloVe 是一种**基于全局词频统计和局部上下文窗口**的词嵌入（Word Embedding）方法，旨在结合**Word2Vec（基于局部上下文）**和**LSA（基于全局矩阵分解）**的优点，学习到既能捕捉局部语义关系又能反映全局统计信息的词向量。

其核心思想是：**词向量之间的差异应该与它们在共现矩阵中的共现概率比率相关**。

*   **共现矩阵（Co-occurrence Matrix）**：`X`，其中 `X_{ij}` 表示词 `i` 和词 `j` 在语料库中共同出现的次数。
*   **目标**：学习词向量 `w_i` 和 `w_j`，使得它们的点积能够反映它们在共现矩阵中的关系。

### 数学原理

GloVe的目标函数旨在最小化以下损失：

$$J = \sum_{i,j=1}^V f(X_{ij}) (w_i^T w_j + b_i + b_j - \log X_{ij})^2$$

其中：

*   $V$：词汇表大小。
*   $w_i, \tilde{w}_j$：词 `i` 和词 `j` 的词向量（$\tilde{w}_j$是上下文词向量，训练完成后 `w` 和 $\tilde{w}$ 可以取平均或只用 `w`）。
*   $b_i, \tilde{b}_j$：词 `i` 和词 `j` 的偏置项。
*   $X_{ij}$：词 `i` 和词 `j` 的共现次数。
*   $f(X_{ij})$：权重函数，用于给高频共现词（如停用词）降低权重，给低频共现词增加权重。

**权重函数 `f(x)`**：

$$f(x) = \begin{cases} 
(x/x_{\text{max}})^\alpha & \text{if } x < x_{\text{max}} \\
1 & \text{otherwise}
\end{cases}$$

*   $x_{\text{max}}$：最大共现次数（如100）。
*   $\alpha$：超参数（通常取0.75）。

### 训练过程

1.  **构建共现矩阵**：遍历整个语料库，统计词与词在指定窗口内的共现次数。
2.  **初始化词向量和偏置**：随机初始化 $w_i$, $\tilde{w}_j$, $b_i$, $\tilde{b}_j$。
3.  **优化目标函数**：使用梯度下降等优化算法最小化目标函数，更新词向量和偏置。

### 优点

1.  **结合全局与局部信息**：既考虑了词的局部上下文（通过窗口），又利用了全局统计信息（通过共现矩阵）。
2.  **性能优异**：在许多下游任务（如词相似度、命名实体识别）上表现与Word2Vec相当甚至更好。
3.  **可解释性**：通过共现概率比率，可以直观地解释词向量的维度。
4.  **并行化**：共现矩阵的构建和目标函数的优化都可以并行进行。

### 缺点

1.  **计算成本**：对于超大规模语料库，构建和存储共现矩阵可能需要大量内存和计算资源。
2.  **无法处理OOV**：对于训练集中未出现的词（Out-Of-Vocabulary, OOV），无法直接生成词向量。

### 实现示例

```python
import torch
import torch.nn as nn
from collections import defaultdict
import numpy as np

class GloVe(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.w = nn.Embedding(vocab_size, embedding_dim)
        self.w_bias = nn.Embedding(vocab_size, 1)
        self.v = nn.Embedding(vocab_size, embedding_dim)
        self.v_bias = nn.Embedding(vocab_size, 1)

    def forward(self, i, j):
        return (self.w(i) * self.v(j)).sum(dim=1) + self.w_bias(i).squeeze() + self.v_bias(j).squeeze()

# 构建共现矩阵
def build_cooccurrence_matrix(corpus, window_size=2):
    cooccur = defaultdict(lambda: defaultdict(float))
    vocab = set()
    
    for sentence in corpus:
        for i, word in enumerate(sentence):
            vocab.add(word)
            start = max(0, i - window_size)
            end = min(len(sentence), i + window_size + 1)
            for j in range(start, end):
                if j != i:
                    cooccur[word][sentence[j]] += 1.0 / abs(i - j)
    
    return cooccur, list(vocab)
```

## 2. Word2Vec

### Skip-gram 模型
预测目标词周围的上下文词：

$$P(w_{i+j}|w_i) = \frac{\exp(\mathbf{v}_{w_{i+j}}^T \mathbf{v}_{w_i})}{\sum_{w=1}^V \exp(\mathbf{v}_w^T \mathbf{v}_{w_i})}$$

### 负采样优化
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SkipGramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)
    
    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        out = self.linear(embeds)
        return F.log_softmax(out, dim=1)

# 负采样损失
def negative_sampling_loss(center_word_emb, context_word_emb, negative_word_emb):
    pos_score = torch.sum(center_word_emb * context_word_emb, dim=1)
    neg_score = torch.sum(center_word_emb * negative_word_emb, dim=1)
    
    pos_loss = -F.logsigmoid(pos_score)
    neg_loss = -F.logsigmoid(-neg_score)
    
    return (pos_loss + neg_loss).mean()
```

## 3. FastText

### 子词模型
使用字符n-gram来表示词向量：

```python
class FastText(nn.Module):
    def __init__(self, vocab_size, embedding_dim, ngrams=3):
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.ngram_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.ngrams = ngrams
    
    def get_ngrams(self, word):
        ngrams = []
        word = '<' + word + '>'
        for i in range(len(word) - self.ngrams + 1):
            ngrams.append(word[i:i+self.ngrams])
        return ngrams
    
    def forward(self, words):
        word_emb = self.embeddings(words)
        ngram_emb = torch.zeros_like(word_emb)
        
        for i, word in enumerate(words):
            ngrams = self.get_ngrams(word)
            ngram_vectors = []
            for ngram in ngrams:
                ngram_idx = self.get_ngram_index(ngram)
                ngram_vectors.append(self.ngram_embeddings(ngram_idx))
            if ngram_vectors:
                ngram_emb[i] = torch.stack(ngram_vectors).mean(0)
        
        return word_emb + ngram_emb
```

## 4. 预训练模型对比

| 模型 | 训练目标 | 上下文窗口 | 适用场景 |
|---|---|---|---|
| Word2Vec | 预测上下文词 | 固定窗口 | 通用词向量 |
| GloVe | 拟合共现统计 | 全局+局部 | 静态语料库 |
| FastText | 子词组合 | 字符级 | 形态丰富语言 |
| BERT | 掩码语言建模 | 双向注意力 | 上下文相关 |

## 5. 词向量评估

### 内在评估
```python
def cosine_similarity(vec1, vec2):
    return torch.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))

def analogy_task(word_vectors, word2idx, a, b, c, d):
    """解决类比任务：a:b :: c:? -> d"""
    vec_a = word_vectors[word2idx[a]]
    vec_b = word_vectors[word2idx[b]]
    vec_c = word_vectors[word2idx[c]]
    
    target = vec_b - vec_a + vec_c
    similarities = torch.cosine_similarity(target.unsqueeze(0), word_vectors)
    return similarities
```

### 外在评估
- 文本分类准确率
- 命名实体识别F1分数
- 情感分析性能

## 6. 实际应用示例

```python
# 加载预训练词向量
import gensim.downloader as api

# 加载GloVe词向量
glove_vectors = api.load('glove-wiki-gigaword-100')

# 计算词语相似度
similarity = glove_vectors.similarity('king', 'queen')
print(f"king和queen的相似度: {similarity:.3f}")

# 解决类比问题
result = glove_vectors.most_similar(positive=['king', 'woman'], negative=['man'])
print(f"king:man :: woman:? -> {result[0][0]}")

# 词向量可视化
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

words = ['king', 'queen', 'man', 'woman', 'boy', 'girl']
vectors = [glove_vectors[word] for word in words]

# 降维可视化
tsne = TSNE(n_components=2, random_state=42)
vectors_2d = tsne.fit_transform(vectors)

plt.figure(figsize=(10, 8))
for i, word in enumerate(words):
    plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1])
    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))
plt.title('词向量可视化')
plt.show()
```

## 7. 关键要点总结

1.  **GloVe**：结合全局统计和局部上下文，适合静态语料库
2.  **Word2Vec**：预测式方法，训练效率高，适合大规模数据
3.  **FastText**：子词模型，能处理OOV问题，适合形态丰富语言
4.  **评估方法**：内在评估（相似度、类比）和外在评估（下游任务）
5.  **应用场景**：文本分类、信息检索、机器翻译等NLP任务的基础

