# 02_神经网络架构

## RNN系列详解

# RNN系列神经网络详解

## 概述

循环神经网络(RNN)是专为**序列数据**设计的神经网络，通过循环结构保留历史信息，适合处理时间序列、文本等有序数据。

## 1. 基础RNN

### 核心思想

RNN通过隐藏状态 $h_t$ 保留历史信息：

$$h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$$

其中：
- $x_t$：当前时间步输入
- $h_{t-1}$：前一时间步的隐藏状态
- $W_{xh}, W_{hh}$：可训练权重

### 优势

1. **循环结构**：天然适合序列数据建模
2. **参数共享**：同一组权重在时间步间共享，减少参数量
3. **可变长度**：支持任意长度的序列输入/输出

### 缺陷

1. **梯度消失/爆炸**：长序列训练时梯度指数级衰减或膨胀
2. **短时记忆**：实际有效记忆长度有限（通常<10个时间步）
3. **计算效率低**：必须按时间步顺序计算，无法并行化

### PyTorch实现

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        h0 = torch.zeros(1, x.size(0), self.hidden_size)
        out, hidden = self.rnn(x, h0)
        # 取最后一个时间步的输出
        out = self.fc(out[:, -1, :])
        return out
```

## 2. LSTM (Long Short-Term Memory)

### 核心结构

LSTM通过引入**门控机制**和**记忆单元**解决梯度消失问题。

#### 记忆单元（Cell State）

贯穿整个时间步的"记忆通道"，信息可无损传递：

$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

#### 门控机制

1. **遗忘门（Forget Gate）**：决定丢弃哪些历史信息
   $$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

2. **输入门（Input Gate）**：控制新信息的加入
   $$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
   $$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

3. **输出门（Output Gate）**：决定当前隐藏状态的输出
   $$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
   $$h_t = o_t \odot \tanh(C_t)$$

#### 完整公式

$$\begin{split}\begin{aligned}
\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\
\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\
\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),
\end{aligned}\end{split}$$

候选记忆元：
$$\tilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c)$$

记忆元：
$$\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t$$

隐状态：
$$\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)$$

### 解决梯度消失的原理

1. **记忆单元的加法更新**：梯度通过 $C_t = C_{t-1} + \Delta C_t$ 传递时，梯度 $\frac{\partial C_t}{\partial C_{t-1}} \approx 1$
2. **门控的调节作用**：动态控制信息流，防止梯度爆炸或消失

### PyTorch实现

```python
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size)
        c0 = torch.zeros(1, x.size(0), self.hidden_size)
        out, (hidden, cell) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out
```

## 3. GRU (Gated Recurrent Unit)

### 核心结构

GRU是LSTM的简化版，合并细胞状态和隐藏状态，减少参数量。

#### 门控机制

1. **重置门（Reset Gate）**：控制历史信息的忽略程度
   $$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$

2. **更新门（Update Gate）**：平衡新旧信息的比例
   $$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$

3. **候选隐藏状态**
   $$\tilde{h}_t = \tanh(W \cdot [r_t \odot h_{t-1}, x_t] + b)$$

4. **最终隐藏状态**
   $$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

#### 完整公式

$$\begin{split}\begin{aligned}
\mathbf{R}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\
\mathbf{Z}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),
\end{aligned}\end{split}$$

候选隐状态：
$$\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h)$$

最终隐状态：
$$\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t$$

### 解决梯度消失的原理

**更新门的残差连接**：$h_t$ 是 $h_{t-1}$ 和 $\tilde{h}_t$ 的加权和，梯度可通过 $1-z_t$ 直接传递。

### PyTorch实现

```python
class GRU(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size)
        out, hidden = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        return out
```

## 4. 双向RNN (Bi-RNN)

### 核心思想

同时考虑前向和后向的上下文信息：

$$h_t^f = f(W_{xh}^f x_t + W_{hh}^f h_{t-1}^f + b_h^f)$$
$$h_t^b = f(W_{xh}^b x_t + W_{hh}^b h_{t+1}^b + b_h^b)$$
$$h_t = [h_t^f; h_t^b]$$

### 优势

1. **上下文信息**：同时利用过去和未来的信息
2. **更好的表示**：特别适合序列标注任务

### 应用场景

- 命名实体识别(NER)
- 词性标注(POS tagging)
- 序列标注任务

### PyTorch实现

```python
class BiRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, 
                           batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(2, x.size(0), self.hidden_size)  # 2 for bidirectional
        c0 = torch.zeros(2, x.size(0), self.hidden_size)
        out, (hidden, cell) = self.lstm(x, (h0, c0))
        # 连接前向和后向的隐藏状态
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        out = self.fc(hidden)
        return out
```

## 5. 深度RNN (Deep RNN)

### 核心思想

堆叠多个RNN层，增加模型的表达能力：

$$h_t^{(l)} = f(W_{xh}^{(l)} h_t^{(l-1)} + W_{hh}^{(l)} h_{t-1}^{(l)} + b_h^{(l)})$$

### 优势

1. **更强的表达能力**：多层结构可以学习更复杂的模式
2. **层次化特征**：不同层捕获不同抽象级别的特征

### 注意事项

1. **梯度问题**：层数增加会加剧梯度消失/爆炸
2. **过拟合**：需要更多的正则化技术
3. **计算复杂度**：训练时间显著增加

### PyTorch实现

```python
class DeepRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, (hidden, cell) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out
```

## 6. RNN的局限性

### 未彻底解决的问题

1. **极端长序列**：当序列极长（如1000+步）时，门控的Sigmoid梯度可能趋近0
2. **初始化敏感**：对权重初始化和超参数设置敏感
3. **并行化困难**：无法像Transformer那样并行计算

### 现代替代方案

1. **Transformer**：自注意力机制直接建模全局依赖
2. **CNN+RNN混合**：结合CNN的并行性和RNN的序列建模能力
3. **注意力机制**：在RNN基础上添加注意力机制

## 7. 选择指南

| 场景 | 推荐模型 | 说明 |
|------|----------|------|
| 短序列(<50步) | 基础RNN | 简单高效 |
| 中等序列(50-200步) | LSTM/GRU | 平衡性能和效率 |
| 长序列(>200步) | Transformer | 更好的长距离建模 |
| 序列标注 | Bi-LSTM | 利用双向上下文 |
| 需要并行化 | Transformer | 支持并行计算 |

## 8. 最佳实践

1. **梯度裁剪**：防止梯度爆炸
2. **权重初始化**：使用Xavier或He初始化
3. **Dropout**：在RNN层间添加Dropout
4. **批量归一化**：稳定训练过程
5. **学习率调度**：使用学习率衰减策略 

