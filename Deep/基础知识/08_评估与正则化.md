# 08_è¯„ä¼°ä¸æ­£åˆ™åŒ–

## è¿‡æ‹Ÿåˆè¯¦è§£

# è¿‡æ‹Ÿåˆè¯¦è§£

è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰æ˜¯æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­ä¸€ä¸ªéå¸¸å¸¸è§ä¸”å…³é”®çš„é—®é¢˜ã€‚å®ƒæŒ‡çš„æ˜¯æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨**æœªè§è¿‡çš„æ–°æ•°æ®ï¼ˆæµ‹è¯•æ•°æ®æˆ–å®é™…åº”ç”¨æ•°æ®ï¼‰ä¸Šè¡¨ç°ä¸ä½³**çš„ç°è±¡ã€‚ç®€å•æ¥è¯´ï¼Œæ¨¡å‹"è®°ä½äº†"è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°å’Œç‰¹æœ‰æ¨¡å¼ï¼Œè€Œä¸æ˜¯å­¦ä¹ åˆ°æ•°æ®çš„æ™®éè§„å¾‹ã€‚

## 1. æ ¸å¿ƒæ¦‚å¿µ

*   **è¿‡æ‹Ÿåˆ**ï¼šæ¨¡å‹å¤æ‚åº¦è¿‡é«˜ï¼Œè¿‡åº¦å­¦ä¹ äº†è®­ç»ƒæ•°æ®ä¸­çš„ç»†èŠ‚å’Œå™ªå£°ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚
*   **æ¬ æ‹Ÿåˆï¼ˆUnderfittingï¼‰**ï¼šæ¨¡å‹å¤æ‚åº¦è¿‡ä½ï¼Œæœªèƒ½å……åˆ†å­¦ä¹ è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼ï¼Œå¯¼è‡´åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¡¨ç°å‡ä¸ä½³ã€‚
*   **æ³›åŒ–èƒ½åŠ›ï¼ˆGeneralization Abilityï¼‰**ï¼šæ¨¡å‹å¯¹æœªè§è¿‡æ•°æ®çš„é¢„æµ‹èƒ½åŠ›ã€‚

## 2. è¿‡æ‹Ÿåˆçš„è¿¹è±¡

*   **è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ï¼Œä½†éªŒè¯æŸå¤±å¼€å§‹ä¸Šå‡æˆ–åœæ»**ã€‚
*   **è®­ç»ƒå‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†éªŒè¯å‡†ç¡®ç‡æ˜¾è‘—ä½äºè®­ç»ƒå‡†ç¡®ç‡**ã€‚
*   æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å®Œç¾ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šé”™è¯¯ç™¾å‡ºã€‚
*   æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®ä¸­çš„å¾®å°æ‰°åŠ¨è¿‡äºæ•æ„Ÿã€‚

## 3. å¯¼è‡´è¿‡æ‹Ÿåˆçš„åŸå› 

1.  **æ¨¡å‹å¤æ‚åº¦è¿‡é«˜**ï¼šæ¨¡å‹å‚æ•°è¿‡å¤šï¼Œå±‚æ•°è¿‡æ·±ï¼Œå®¹é‡è¿‡å¤§ï¼Œèƒ½å¤Ÿ"è®°ä½"è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸€ä¸ªæ ·æœ¬ã€‚
2.  **è®­ç»ƒæ•°æ®é‡ä¸è¶³**ï¼šæ•°æ®é‡å¤ªå°‘ï¼Œä¸è¶³ä»¥ä»£è¡¨çœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œæ¨¡å‹å®¹æ˜“å­¦ä¹ åˆ°å™ªå£°ã€‚
3.  **è®­ç»ƒæ•°æ®å­˜åœ¨å™ªå£°**ï¼šæ ‡ç­¾é”™è¯¯ã€å¼‚å¸¸å€¼ç­‰ï¼Œæ¨¡å‹ä¼šè¯•å›¾æ‹Ÿåˆè¿™äº›å™ªå£°ã€‚
4.  **è®­ç»ƒæ—¶é—´è¿‡é•¿**ï¼šæ¨¡å‹åœ¨è®­ç»ƒåæœŸå¼€å§‹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
5.  **ç‰¹å¾è¿‡å¤šæˆ–ä¸ç›¸å…³**ï¼šå¼•å…¥äº†å¤§é‡ä¸ä»»åŠ¡æ— å…³çš„ç‰¹å¾ï¼Œå¢åŠ äº†æ¨¡å‹å­¦ä¹ çš„éš¾åº¦å’Œè¿‡æ‹Ÿåˆçš„é£é™©ã€‚

## 4. è§£å†³è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼ˆæ­£åˆ™åŒ–æŠ€æœ¯ï¼‰

### 4.1 æ•°æ®å±‚é¢

1.  **å¢åŠ è®­ç»ƒæ•°æ®**ï¼šæœ€ç›´æ¥æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†æˆæœ¬é«˜ã€‚
2.  **æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰**ï¼šé€šè¿‡å¯¹ç°æœ‰æ•°æ®è¿›è¡Œå˜æ¢ï¼ˆå¦‚å›¾åƒæ—‹è½¬ã€æ–‡æœ¬åŒä¹‰è¯æ›¿æ¢ï¼‰ç”Ÿæˆæ–°æ•°æ®ã€‚
3.  **æ•°æ®æ¸…æ´—**ï¼šå»é™¤å™ªå£°ã€å¼‚å¸¸å€¼å’Œé”™è¯¯æ ‡ç­¾ã€‚

### 4.2 æ¨¡å‹å±‚é¢

1.  **ç®€åŒ–æ¨¡å‹**ï¼šå‡å°‘æ¨¡å‹å±‚æ•°ã€ç¥ç»å…ƒæ•°é‡æˆ–å‚æ•°é‡ã€‚
2.  **ç‰¹å¾é€‰æ‹©/é™ç»´**ï¼šé€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œæˆ–ä½¿ç”¨PCAç­‰æ–¹æ³•é™ç»´ã€‚

### 4.3 è®­ç»ƒç­–ç•¥å±‚é¢

1.  **æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰**ï¼š
    *   **L1/L2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒ©ç½šé¡¹ï¼Œé™åˆ¶æ¨¡å‹æƒé‡çš„å¤§å°ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´ç®€å•çš„æƒé‡ã€‚
        ```math
        \text{Loss} = \text{OriginalLoss} + \lambda \sum |w| \quad (\text{L1})
        \text{Loss} = \text{OriginalLoss} + \lambda \sum w^2 \quad (\text{L2})
        ```
    *   **Dropout**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœº"å…³é—­"ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾ï¼Œé˜²æ­¢ç¥ç»å…ƒé—´çš„å…±é€‚åº”ã€‚
        *   **åº”ç”¨**ï¼šé€šå¸¸åœ¨å…¨è¿æ¥å±‚æˆ–Transformerçš„FFNå±‚ä¹‹åã€‚
    *   **Batch Normalization / Layer Normalization**ï¼šé€šè¿‡å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒï¼Œå‡å°‘å¯¹åˆå§‹åŒ–å’Œå­¦ä¹ ç‡çš„æ•æ„Ÿæ€§ï¼Œé—´æ¥èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨ã€‚
2.  **æ—©åœï¼ˆEarly Stoppingï¼‰**ï¼š
    *   ç›‘æ§æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½ï¼Œå½“éªŒè¯æŸå¤±ä¸å†ä¸‹é™æˆ–å¼€å§‹ä¸Šå‡æ—¶ï¼Œåœæ­¢è®­ç»ƒã€‚
    *   **åŸç†**ï¼šåœ¨æ¨¡å‹å¼€å§‹è¿‡æ‹Ÿåˆä¹‹å‰åœæ­¢è®­ç»ƒï¼Œæ‰¾åˆ°æœ€ä½³æ³›åŒ–ç‚¹ã€‚
3.  **äº¤å‰éªŒè¯ï¼ˆCross-Validationï¼‰**ï¼š
    *   å°†æ•°æ®é›†åˆ†æˆå¤šä¸ªå­é›†ï¼Œè½®æµä½œä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    *   **ä½œç”¨**ï¼šæ›´å¯é åœ°è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œå¸®åŠ©é€‰æ‹©æœ€ä½³è¶…å‚æ•°ã€‚

## 5. PyTorchä¸­çš„æ­£åˆ™åŒ–ç¤ºä¾‹

### 5.1 L2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰

```python
import torch.optim as optim

# åœ¨ä¼˜åŒ–å™¨ä¸­è®¾ç½® weight_decay å‚æ•°
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) # L2æ­£åˆ™åŒ–
```

### 5.2 Dropout

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(100, 50)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.5) # éšæœºä¸¢å¼ƒ50%çš„ç¥ç»å…ƒ
        self.fc2 = nn.Linear(50, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x) # åœ¨è®­ç»ƒæ—¶åº”ç”¨Dropout
        x = self.fc2(x)
        return x

# æ³¨æ„ï¼šDropoutåœ¨evalæ¨¡å¼ä¸‹ä¼šè‡ªåŠ¨å…³é—­
model.train() # è®­ç»ƒæ¨¡å¼
# model.eval()  # è¯„ä¼°æ¨¡å¼
```

### 5.3 æ—©åœ

```python
# ä¼ªä»£ç 
best_val_loss = float('inf')
patience = 10 # å®¹å¿å¤šå°‘ä¸ªepochéªŒè¯æŸå¤±ä¸ä¸‹é™
counter = 0

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader)
    val_loss = evaluate_on_validation_set(model, val_loader)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth') # ä¿å­˜æœ€ä½³æ¨¡å‹
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

## 6. æ€»ç»“

è¿‡æ‹Ÿåˆæ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸­å¿…é¡»é¢å¯¹çš„æŒ‘æˆ˜ã€‚é€šè¿‡**å¢åŠ æ•°æ®ã€ç®€åŒ–æ¨¡å‹ã€å¼•å…¥æ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆL1/L2ã€Dropoutã€BN/LNï¼‰å’Œé‡‡ç”¨æ—©åœç­–ç•¥**ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç¼“è§£è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¡¨ç°æ›´ä½³ã€‚åœ¨å®è·µä¸­ï¼Œé€šå¸¸ä¼šç»“åˆå¤šç§æ–¹æ³•æ¥å¯¹æŠ—è¿‡æ‹Ÿåˆã€‚


## å½’ä¸€åŒ–è¯¦è§£

# å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰è¯¦è§£

å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸€ç§å¸¸ç”¨çš„æ•°æ®é¢„å¤„ç†æŠ€æœ¯ï¼Œæ—¨åœ¨å°†è¾“å…¥æ•°æ®æˆ–ä¸­é—´å±‚æ¿€æ´»å€¼è°ƒæ•´åˆ°ç»Ÿä¸€çš„å°ºåº¦èŒƒå›´ï¼Œä»è€Œ**åŠ é€Ÿæ¨¡å‹è®­ç»ƒã€æé«˜æ¨¡å‹ç¨³å®šæ€§å¹¶æ”¹å–„æ³›åŒ–èƒ½åŠ›**ã€‚

## 1. ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ

1.  **åŠ é€Ÿæ”¶æ•›**ï¼š
    *   å½“è¾“å…¥ç‰¹å¾çš„å°ºåº¦å·®å¼‚å¾ˆå¤§æ—¶ï¼ŒæŸå¤±å‡½æ•°çš„ç­‰é«˜çº¿ä¼šéå¸¸æ‰å¹³ï¼Œå¯¼è‡´æ¢¯åº¦ä¸‹é™è·¯å¾„å‘ˆ"Z"å­—å½¢ï¼Œæ”¶æ•›ç¼“æ…¢ã€‚
    *   å½’ä¸€åŒ–åï¼Œç­‰é«˜çº¿æ›´æ¥è¿‘åœ†å½¢ï¼Œæ¢¯åº¦æ–¹å‘æ›´æ¥è¿‘æœ€ä¼˜æ–¹å‘ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚
2.  **é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**ï¼š
    *   åœ¨æ·±åº¦ç½‘ç»œä¸­ï¼Œæ¿€æ´»å€¼è¿‡å¤§æˆ–è¿‡å°éƒ½å¯èƒ½å¯¼è‡´æ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚
    *   å½’ä¸€åŒ–å°†æ¿€æ´»å€¼é™åˆ¶åœ¨ç¨³å®šèŒƒå›´å†…ï¼Œæœ‰åŠ©äºæ¢¯åº¦ä¼ æ’­ã€‚
3.  **æé«˜æ¨¡å‹ç¨³å®šæ€§**ï¼š
    *   å‡å°‘å†…éƒ¨åå˜é‡åç§»ï¼ˆInternal Covariate Shiftï¼‰ï¼šæŒ‡è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ·±å±‚ç½‘ç»œè¾“å…¥åˆ†å¸ƒçš„å˜åŒ–ã€‚å½’ä¸€åŒ–ç¨³å®šäº†æ¯å±‚è¾“å…¥çš„åˆ†å¸ƒï¼Œä½¿å¾—åç»­å±‚æ›´å®¹æ˜“å­¦ä¹ ã€‚
4.  **æ”¹å–„æ³›åŒ–èƒ½åŠ›**ï¼š
    *   å‡å°‘å¯¹åˆå§‹åŒ–å‚æ•°çš„æ•æ„Ÿæ€§ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚

## 2. å¸¸è§çš„å½’ä¸€åŒ–æ–¹æ³•

### 2.1 æ•°æ®é¢„å¤„ç†å½’ä¸€åŒ–

*   **Min-Max Normalizationï¼ˆMin-Maxå½’ä¸€åŒ–ï¼‰**ï¼š
    *   å°†æ•°æ®ç¼©æ”¾åˆ° `[0, 1]` æˆ– `[-1, 1]` ä¹‹é—´ã€‚
    ```math
    x' = \frac{x - x_{min}}{x_{max} - x_{min}}
    ```
    *   **é€‚ç”¨**ï¼šæ•°æ®èŒƒå›´å·²çŸ¥ä¸”ç¨³å®šã€‚
*   **Z-score Normalizationï¼ˆæ ‡å‡†åŒ–ï¼‰**ï¼š
    *   å°†æ•°æ®è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒã€‚
    ```math
    x' = \frac{x - \mu}{\sigma}
    ```
    *   **é€‚ç”¨**ï¼šæ•°æ®åˆ†å¸ƒè¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼Œæˆ–æ•°æ®èŒƒå›´æœªçŸ¥ã€‚

### 2.2 å±‚å†…å½’ä¸€åŒ–ï¼ˆIn-layer Normalizationï¼‰

è¿™äº›æ–¹æ³•åœ¨ç¥ç»ç½‘ç»œçš„éšè—å±‚ä¸­åº”ç”¨ï¼Œå¯¹æ¿€æ´»å€¼è¿›è¡Œå½’ä¸€åŒ–ã€‚

#### 2.2.1 Batch Normalization (BN)

*   **åŸç†**ï¼šå¯¹**ä¸€ä¸ªmini-batch**çš„æ•°æ®åœ¨**ç‰¹å¾ç»´åº¦**ä¸Šè¿›è¡Œå½’ä¸€åŒ–ã€‚
    ```math
    \mu_B = \frac{1}{m} \sum_{i=1}^m x_i, \quad \sigma_B^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2
    ```
    ```math
    \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y_i = \gamma \hat{x}_i + \beta
    ```
    *   $m$ï¼šmini-batchå¤§å°ã€‚
    *   $\gamma, \beta$ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°ï¼Œå…è®¸ç½‘ç»œæ¢å¤åŸå§‹ç‰¹å¾åˆ†å¸ƒã€‚
*   **é€‚ç”¨**ï¼šCVä»»åŠ¡ï¼Œå¤§Batch Sizeã€‚
*   **ç¼ºç‚¹**ï¼š
    *   ä¾èµ–Batch Sizeï¼šBatch Sizeè¿‡å°ï¼ˆå¦‚1ã€2ï¼‰æ—¶ï¼Œç»Ÿè®¡é‡ä¸ç¨³å®šã€‚
    *   ä¸é€‚ç”¨äºRNNï¼šRNNåºåˆ—é•¿åº¦å¯å˜ï¼Œä¸”æ¯ä¸ªæ—¶é—´æ­¥çš„ç»Ÿè®¡é‡ä¸åŒã€‚

#### 2.2.2 Layer Normalization (LN)

*   **åŸç†**ï¼šå¯¹**å•ä¸ªæ ·æœ¬**åœ¨**ç‰¹å¾ç»´åº¦**ä¸Šè¿›è¡Œå½’ä¸€åŒ–ã€‚
    ```math
    \mu = \frac{1}{H} \sum_{i=1}^H x_i, \quad \sigma^2 = \frac{1}{H} \sum_{i=1}^H (x_i - \mu)^2
    ```
    ```math
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y_i = \gamma \hat{x}_i + \beta
    ```
    *   $H$ï¼šç‰¹å¾ç»´åº¦å¤§å°ã€‚
*   **é€‚ç”¨**ï¼šNLPä»»åŠ¡ï¼ˆå¦‚Transformerï¼‰ï¼ŒRNNï¼Œå°Batch Sizeã€‚
*   **ä¼˜ç‚¹**ï¼š
    *   ä¸ä¾èµ–Batch Sizeï¼šæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è®¡ç®—ç»Ÿè®¡é‡ã€‚
    *   é€‚ç”¨äºRNNå’ŒTransformerã€‚
*   **ç¼ºç‚¹**ï¼š
    *   å¯¹ä¸åŒç‰¹å¾çš„å°ºåº¦å˜åŒ–ä¸æ•æ„Ÿã€‚

#### 2.2.3 Instance Normalization (IN)

*   **åŸç†**ï¼šå¯¹**å•ä¸ªæ ·æœ¬**åœ¨**æ¯ä¸ªé€šé“**ä¸Šè¿›è¡Œå½’ä¸€åŒ–ã€‚
*   **é€‚ç”¨**ï¼šé£æ ¼è¿ç§»ï¼ˆStyle Transferï¼‰ã€‚

#### 2.2.4 Group Normalization (GN)

*   **åŸç†**ï¼šå¯¹**å•ä¸ªæ ·æœ¬**åœ¨**é€šé“åˆ†ç»„**åè¿›è¡Œå½’ä¸€åŒ–ã€‚
*   **é€‚ç”¨**ï¼šBatch Sizeå¾ˆå°ä½†é€šé“æ•°å¾ˆå¤šçš„æƒ…å†µã€‚

## 3. å½’ä¸€åŒ–å±‚çš„é€‰æ‹©

| **å½’ä¸€åŒ–ç±»å‹** | **é€‚ç”¨åœºæ™¯** | **ä¼˜ç‚¹** | **ç¼ºç‚¹** |
|---|---|---|---|
| **Batch Norm** | CVä»»åŠ¡ï¼Œå¤§Batch Size | æ•ˆæœå¥½ï¼ŒåŠ é€Ÿæ”¶æ•› | ä¾èµ–Batch Sizeï¼Œä¸é€‚ç”¨äºRNN |
| **Layer Norm** | NLPä»»åŠ¡ï¼ˆTransformerã€RNNï¼‰ï¼Œå°Batch Size | ä¸ä¾èµ–Batch Sizeï¼Œé€‚ç”¨äºåºåˆ—æ¨¡å‹ | |
| **Instance Norm** | é£æ ¼è¿ç§» | ä¿æŒå®ä¾‹ç‹¬ç«‹æ€§ | |
| **Group Norm** | å°Batch Sizeï¼Œå¤§é€šé“æ•° | ä¸ä¾èµ–Batch Sizeï¼Œä»‹äºBNå’ŒLNä¹‹é—´ | |

## 4. PyTorchå®ç°

```python
import torch
import torch.nn as nn

# Batch Normalization
bn = nn.BatchNorm1d(num_features=100) # å¯¹100ç»´ç‰¹å¾è¿›è¡ŒBN

# Layer Normalization
ln = nn.LayerNorm(normalized_shape=512) # å¯¹512ç»´ç‰¹å¾è¿›è¡ŒLN (Transformerä¸­å¸¸ç”¨)

# Instance Normalization
# in_ = nn.InstanceNorm1d(num_features=100)

# Group Normalization
# gn = nn.GroupNorm(num_groups=32, num_channels=256)

# ç¤ºä¾‹ï¼šåœ¨Transformerä¸­ä½¿ç”¨LayerNorm
class TransformerBlock(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        # ... å…¶ä»–å±‚
    
    def forward(self, x):
        # ... attention
        x = self.norm1(x + residual) # æ®‹å·®è¿æ¥åè¿›è¡ŒLayerNorm
        # ... feedforward
        x = self.norm2(x + residual)
        return x
```

## 5. æ€»ç»“

å½’ä¸€åŒ–æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸å¯æˆ–ç¼ºçš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡è°ƒæ•´æ•°æ®å°ºåº¦æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚é€‰æ‹©åˆé€‚çš„å½’ä¸€åŒ–æ–¹æ³•å–å†³äºæ¨¡å‹æ¶æ„å’Œä»»åŠ¡ç‰¹ç‚¹ï¼š**Batch Normalizationåœ¨CVé¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œè€ŒLayer Normalizationåˆ™æ˜¯NLPé¢†åŸŸï¼ˆå°¤å…¶æ˜¯Transformerï¼‰çš„é¦–é€‰**ã€‚ç†è§£ä¸åŒå½’ä¸€åŒ–æ–¹æ³•çš„åŸç†å’Œé€‚ç”¨åœºæ™¯ï¼Œå¯¹äºæ„å»ºé«˜æ•ˆç¨³å®šçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚

## æ¨¡å‹è¯„ä¼°è¯¦è§£

# æ¨¡å‹è¯„ä¼°è¯¦è§£

## 1. åˆ†ç±»ä»»åŠ¡è¯„ä¼°

### 1.1 åŸºç¡€æŒ‡æ ‡

#### å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

- **ä¼˜ç‚¹**ï¼šç›´è§‚æ˜“æ‡‚
- **ç¼ºç‚¹**ï¼šåœ¨ç±»åˆ«ä¸å¹³è¡¡æ—¶å¯èƒ½è¯¯å¯¼

#### ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰
$$Precision = \frac{TP}{TP + FP}$$

- é¢„æµ‹ä¸ºæ­£ä¾‹ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹
- å…³æ³¨é¢„æµ‹çš„å‡†ç¡®æ€§

#### å¬å›ç‡ï¼ˆRecallï¼‰
$$Recall = \frac{TP}{TP + FN}$$

- å®é™…æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- å…³æ³¨æ¨¡å‹çš„è¦†ç›–èƒ½åŠ›

#### F1åˆ†æ•°
$$F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}$$

- ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡

### 1.2 å¤šåˆ†ç±»è¯„ä¼°

#### å®å¹³å‡ï¼ˆMacro-averageï¼‰
$$Macro-P = \frac{1}{n} \sum_{i=1}^{n} P_i$$
$$Macro-R = \frac{1}{n} \sum_{i=1}^{n} R_i$$
$$Macro-F1 = \frac{2 \times Macro-P \times Macro-R}{Macro-P + Macro-R}$$

#### å¾®å¹³å‡ï¼ˆMicro-averageï¼‰
$$Micro-P = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} TP_i + \sum_{i=1}^{n} FP_i}$$
$$Micro-R = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} TP_i + \sum_{i=1}^{n} FN_i}$$

### 1.3 å®ç°ä»£ç 

```python
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix

def evaluate_classification(y_true, y_pred, y_proba=None):
    """åˆ†ç±»ä»»åŠ¡è¯„ä¼°"""
    results = {}
    
    # åŸºç¡€æŒ‡æ ‡
    results['accuracy'] = accuracy_score(y_true, y_pred)
    results['precision'] = precision_score(y_true, y_pred, average='weighted')
    results['recall'] = recall_score(y_true, y_pred, average='weighted')
    results['f1'] = f1_score(y_true, y_pred, average='weighted')
    
    # è¯¦ç»†æŠ¥å‘Š
    results['classification_report'] = classification_report(y_true, y_pred)
    results['confusion_matrix'] = confusion_matrix(y_true, y_pred)
    
    # ROC-AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹ï¼‰
    if y_proba is not None:
        from sklearn.metrics import roc_auc_score
        results['roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr')
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 0, 1]
y_proba = np.array([[0.8, 0.1, 0.1], [0.1, 0.2, 0.7], [0.2, 0.6, 0.2],
                    [0.9, 0.05, 0.05], [0.7, 0.2, 0.1], [0.1, 0.8, 0.1]])

results = evaluate_classification(y_true, y_pred, y_proba)
print(f"Accuracy: {results['accuracy']:.4f}")
print(f"F1 Score: {results['f1']:.4f}")
```

## 2. å›å½’ä»»åŠ¡è¯„ä¼°

### 2.1 åŸºç¡€æŒ‡æ ‡

#### å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

#### å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰
$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

#### å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰
$$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

#### å†³å®šç³»æ•°ï¼ˆRÂ²ï¼‰
$$R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

### 2.2 å®ç°ä»£ç 

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_regression(y_true, y_pred):
    """å›å½’ä»»åŠ¡è¯„ä¼°"""
    results = {}
    
    results['mse'] = mean_squared_error(y_true, y_pred)
    results['rmse'] = np.sqrt(results['mse'])
    results['mae'] = mean_absolute_error(y_true, y_pred)
    results['r2'] = r2_score(y_true, y_pred)
    
    # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
    results['mape'] = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
y_true = np.array([1, 2, 3, 4, 5])
y_pred = np.array([1.1, 1.9, 3.1, 3.9, 5.1])

results = evaluate_regression(y_true, y_pred)
print(f"RMSE: {results['rmse']:.4f}")
print(f"RÂ²: {results['r2']:.4f}")
```

## 3. NLPä»»åŠ¡è¯„ä¼°

### 3.1 æ–‡æœ¬ç”Ÿæˆè¯„ä¼°

#### BLEUåˆ†æ•°
```python
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def compute_bleu(references, candidates):
    """è®¡ç®—BLEUåˆ†æ•°"""
    # å¥å­çº§BLEU
    sentence_scores = []
    for ref, cand in zip(references, candidates):
        ref_tokens = [ref.split()]  # å•ä¸ªå‚è€ƒç¿»è¯‘
        cand_tokens = cand.split()
        score = sentence_bleu(ref_tokens, cand_tokens)
        sentence_scores.append(score)
    
    # è¯­æ–™åº“çº§BLEU
    ref_tokens = [[ref.split()] for ref in references]
    cand_tokens = [cand.split() for cand in candidates]
    corpus_score = corpus_bleu(ref_tokens, cand_tokens)
    
    return {
        'sentence_bleu': np.mean(sentence_scores),
        'corpus_bleu': corpus_score
    }
```

#### ROUGEåˆ†æ•°
```python
from rouge_score import rouge_scorer

def compute_rouge(references, candidates):
    """è®¡ç®—ROUGEåˆ†æ•°"""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])
    
    rouge1_scores = []
    rouge2_scores = []
    rougeL_scores = []
    
    for ref, cand in zip(references, candidates):
        scores = scorer.score(ref, cand)
        rouge1_scores.append(scores['rouge1'].fmeasure)
        rouge2_scores.append(scores['rouge2'].fmeasure)
        rougeL_scores.append(scores['rougeL'].fmeasure)
    
    return {
        'rouge1': np.mean(rouge1_scores),
        'rouge2': np.mean(rouge2_scores),
        'rougeL': np.mean(rougeL_scores)
    }
```

### 3.2 å‘½åå®ä½“è¯†åˆ«è¯„ä¼°

```python
def evaluate_ner(y_true, y_pred):
    """NERä»»åŠ¡è¯„ä¼°"""
    from seqeval.metrics import classification_report, f1_score
    
    # è®¡ç®—F1åˆ†æ•°
    f1 = f1_score(y_true, y_pred)
    
    # è¯¦ç»†æŠ¥å‘Š
    report = classification_report(y_true, y_pred)
    
    return {
        'f1': f1,
        'report': report
    }
```

## 4. äº¤å‰éªŒè¯

### 4.1 KæŠ˜äº¤å‰éªŒè¯

```python
from sklearn.model_selection import KFold, cross_val_score

def k_fold_cross_validation(model, X, y, k=5, scoring='accuracy'):
    """KæŠ˜äº¤å‰éªŒè¯"""
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    
    # äº¤å‰éªŒè¯åˆ†æ•°
    scores = cross_val_score(model, X, y, cv=kf, scoring=scoring)
    
    return {
        'scores': scores,
        'mean': scores.mean(),
        'std': scores.std(),
        'cv_results': f"{scores.mean():.4f} (+/- {scores.std() * 2:.4f})"
    }

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
cv_results = k_fold_cross_validation(model, X, y, k=5)
print(f"Cross-validation score: {cv_results['cv_results']}")
```

### 4.2 åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯

```python
from sklearn.model_selection import StratifiedKFold

def stratified_k_fold_cv(model, X, y, k=5):
    """åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯"""
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    
    scores = []
    for train_idx, val_idx in skf.split(X, y):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        score = accuracy_score(y_val, y_pred)
        scores.append(score)
    
    return {
        'scores': scores,
        'mean': np.mean(scores),
        'std': np.std(scores)
    }
```

## 5. æ¨¡å‹æ¯”è¾ƒ

### 5.1 ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

```python
from scipy import stats

def compare_models(scores_model1, scores_model2, alpha=0.05):
    """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    # tæ£€éªŒ
    t_stat, p_value = stats.ttest_rel(scores_model1, scores_model2)
    
    # Wilcoxonç¬¦å·ç§©æ£€éªŒ
    w_stat, w_p_value = stats.wilcoxon(scores_model1, scores_model2)
    
    return {
        't_test': {
            'statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < alpha
        },
        'wilcoxon_test': {
            'statistic': w_stat,
            'p_value': w_p_value,
            'significant': w_p_value < alpha
        }
    }
```

### 5.2 æ¨¡å‹é€‰æ‹©

```python
def model_selection(models, X, y, cv=5, scoring='accuracy'):
    """æ¨¡å‹é€‰æ‹©"""
    results = {}
    
    for name, model in models.items():
        cv_scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
        results[name] = {
            'mean': cv_scores.mean(),
            'std': cv_scores.std(),
            'scores': cv_scores
        }
    
    # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
    best_model = max(results.keys(), key=lambda x: results[x]['mean'])
    
    return results, best_model

# ä½¿ç”¨ç¤ºä¾‹
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(),
    'LogisticRegression': LogisticRegression()
}

results, best_model = model_selection(models, X, y)
print(f"Best model: {best_model}")
```

## 6. è¿‡æ‹Ÿåˆæ£€æµ‹

### 6.1 å­¦ä¹ æ›²çº¿

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

def plot_learning_curves(model, X, y, cv=5):
    """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=cv, n_jobs=-1, 
        train_sizes=np.linspace(0.1, 1.0, 10)
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, label='Training score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.plot(train_sizes, val_mean, label='Cross-validation score')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)
    plt.xlabel('Training Examples')
    plt.ylabel('Score')
    plt.title('Learning Curves')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()
    
    return train_sizes, train_mean, val_mean
```

### 6.2 éªŒè¯æ›²çº¿

```python
from sklearn.model_selection import validation_curve

def plot_validation_curves(model, X, y, param_name, param_range, cv=5):
    """ç»˜åˆ¶éªŒè¯æ›²çº¿"""
    train_scores, val_scores = validation_curve(
        model, X, y, param_name=param_name, param_range=param_range, cv=cv
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mean, label='Training score')
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.plot(param_range, val_mean, label='Cross-validation score')
    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1)
    plt.xlabel(param_name)
    plt.ylabel('Score')
    plt.title('Validation Curves')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()
    
    return param_range, train_mean, val_mean
```

## 7. é”™è¯¯åˆ†æ

### 7.1 æ··æ·†çŸ©é˜µåˆ†æ

```python
import seaborn as sns

def analyze_confusion_matrix(y_true, y_pred, class_names=None):
    """åˆ†ææ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    class_metrics = {}
    for i in range(len(cm)):
        tp = cm[i, i]
        fp = cm[:, i].sum() - tp
        fn = cm[i, :].sum() - tp
        tn = cm.sum() - tp - fp - fn
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        class_metrics[i] = {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    return cm, class_metrics
```

### 7.2 é”™è¯¯æ¡ˆä¾‹åˆ†æ

```python
def error_analysis(X, y_true, y_pred, feature_names=None):
    """é”™è¯¯åˆ†æ"""
    # æ‰¾å‡ºé”™è¯¯é¢„æµ‹çš„æ ·æœ¬
    error_indices = np.where(y_true != y_pred)[0]
    
    # åˆ†æé”™è¯¯ç±»å‹
    error_types = {}
    for idx in error_indices:
        true_label = y_true[idx]
        pred_label = y_pred[idx]
        error_type = f"{true_label} -> {pred_label}"
        
        if error_type not in error_types:
            error_types[error_type] = []
        error_types[error_type].append(idx)
    
    # ç»Ÿè®¡é”™è¯¯ç±»å‹
    error_stats = {error_type: len(indices) for error_type, indices in error_types.items()}
    
    # åˆ†æç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœä½¿ç”¨æ ‘æ¨¡å‹ï¼‰
    if hasattr(model, 'feature_importances_'):
        feature_importance = model.feature_importances_
        error_analysis_results = {
            'error_types': error_stats,
            'feature_importance': feature_importance,
            'error_indices': error_indices
        }
    else:
        error_analysis_results = {
            'error_types': error_stats,
            'error_indices': error_indices
        }
    
    return error_analysis_results
```

## 8. æ¨¡å‹è§£é‡Šæ€§

### 8.1 SHAPå€¼åˆ†æ

```python
import shap

def explain_model(model, X, feature_names=None):
    """ä½¿ç”¨SHAPè§£é‡Šæ¨¡å‹"""
    # åˆ›å»ºSHAPè§£é‡Šå™¨
    if hasattr(model, 'predict_proba'):
        explainer = shap.TreeExplainer(model) if hasattr(model, 'feature_importances_') else shap.KernelExplainer(model.predict_proba, X[:100])
    else:
        explainer = shap.TreeExplainer(model) if hasattr(model, 'feature_importances_') else shap.KernelExplainer(model.predict, X[:100])
    
    # è®¡ç®—SHAPå€¼
    shap_values = explainer.shap_values(X)
    
    # ç»˜åˆ¶æ‘˜è¦å›¾
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X, feature_names=feature_names)
    plt.show()
    
    return explainer, shap_values
```

### 8.2 LIMEè§£é‡Š

```python
from lime import lime_tabular

def lime_explanation(model, X, feature_names=None, instance_idx=0):
    """ä½¿ç”¨LIMEè§£é‡Šå•ä¸ªé¢„æµ‹"""
    explainer = lime_tabular.LimeTabularExplainer(
        X, feature_names=feature_names, class_names=['0', '1'], mode='classification'
    )
    
    exp = explainer.explain_instance(X[instance_idx], model.predict_proba)
    exp.show_in_notebook()
    
    return exp
```

## æ€»ç»“

æ¨¡å‹è¯„ä¼°æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­çš„å…³é”®ç¯èŠ‚ï¼š

1. **åˆ†ç±»è¯„ä¼°**ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
2. **å›å½’è¯„ä¼°**ï¼šMSEã€RMSEã€MAEã€RÂ²
3. **NLPè¯„ä¼°**ï¼šBLEUã€ROUGEç­‰ç‰¹å®šæŒ‡æ ‡
4. **äº¤å‰éªŒè¯**ï¼šKæŠ˜ã€åˆ†å±‚KæŠ˜éªŒè¯
5. **æ¨¡å‹æ¯”è¾ƒ**ï¼šç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
6. **è¿‡æ‹Ÿåˆæ£€æµ‹**ï¼šå­¦ä¹ æ›²çº¿ã€éªŒè¯æ›²çº¿
7. **é”™è¯¯åˆ†æ**ï¼šæ··æ·†çŸ©é˜µã€é”™è¯¯æ¡ˆä¾‹åˆ†æ
8. **æ¨¡å‹è§£é‡Š**ï¼šSHAPã€LIMEç­‰å¯è§£é‡Šæ€§æ–¹æ³•

é€šè¿‡ç³»ç»Ÿæ€§çš„è¯„ä¼°ï¼Œå¯ä»¥å…¨é¢äº†è§£æ¨¡å‹æ€§èƒ½å¹¶æŒ‡å¯¼æ¨¡å‹æ”¹è¿›ã€‚ 

## æ•°æ®å¢å¼ºè¯¦è§£

# æ•°æ®å¢å¼ºè¯¦è§£

åœ¨NLPä¸­ï¼Œæ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰æŠ€æœ¯è¢«å¹¿æ³›ç”¨äº**å°æ ·æœ¬åœºæ™¯**ï¼ˆå¦‚ä½èµ„æºè¯­è¨€ã€åŒ»ç–—/é‡‘èå‚ç›´é¢†åŸŸï¼‰å’Œ**æ¨¡å‹é²æ£’æ€§æå‡**ã€‚ä»¥ä¸‹æ˜¯NLPæ•°æ®å¢å¼ºçš„æ ¸å¿ƒæ–¹æ³•ã€åº”ç”¨åœºæ™¯åŠæœ€æ–°å®è·µï¼š

## 1. ä¸ºä»€ä¹ˆNLPéœ€è¦æ•°æ®å¢å¼ºï¼Ÿ

*   **æ•°æ®ç“¶é¢ˆ**ï¼šæ ‡æ³¨æˆæœ¬é«˜ï¼ˆå¦‚å‘½åå®ä½“è¯†åˆ«éœ€ä¸“å®¶æ ‡æ³¨ï¼‰ã€‚
*   **æ¨¡å‹æ³›åŒ–**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡å¯¹å™ªå£°å’Œå˜ä½“çš„é²æ£’æ€§ã€‚
*   **å…¬å¹³æ€§**ï¼šå¹³è¡¡å°‘æ•°ç±»æ ·æœ¬ï¼ˆå¦‚ç½•è§ç–¾ç—…æœ¯è¯­ï¼‰ã€‚

## 2. å¸¸è§NLPæ•°æ®å¢å¼ºæŠ€æœ¯

### 2.1 æ–‡æœ¬è¡¨é¢çº§å¢å¼º

| æ–¹æ³• | ç¤ºä¾‹ | é€‚ç”¨ä»»åŠ¡ | å·¥å…·åº“ |
|---|---|---|---|
| **åŒä¹‰è¯æ›¿æ¢** | "å¥½çš„" â†’ "å¥½çš„å‘€"/"æ²¡é—®é¢˜" | æ–‡æœ¬åˆ†ç±»/æƒ…æ„Ÿåˆ†æ | `Synonyms`ï¼ˆä¸­æ–‡ï¼‰ |
| **éšæœºæ’å…¥/åˆ é™¤** | "æˆ‘çˆ±è‹¹æœ" â†’ "æˆ‘çˆ±åƒè‹¹æœ" | æ„å›¾è¯†åˆ« | `nlpaug` |
| **å­—ç¬¦çº§æ‰°åŠ¨** | "apple" â†’ "app1e"ï¼ˆæ¨¡æ‹Ÿæ‹¼å†™é”™è¯¯ï¼‰ | æ‹¼å†™çº é”™/é²æ£’æ€§æµ‹è¯• | `TextAttack` |
| **å›è¯‘ï¼ˆBack Translationï¼‰** | ä¸­æ–‡â†’è‹±æ–‡â†’ä¸­æ–‡ï¼ˆè¯­ä¹‰ä¸å˜ï¼Œè¡¨è¿°å˜åŒ–ï¼‰ | é—®ç­”ç³»ç»Ÿ/ç”Ÿæˆä»»åŠ¡ | Google Translate API |

### 2.2 è¯­ä¹‰çº§å¢å¼º

| æ–¹æ³• | åŸç† | é€‚ç”¨åœºæ™¯ |
|---|---|---|
| **æ¨¡æ¿ç”Ÿæˆ** | åŸºäºè§„åˆ™ç”Ÿæˆæ–°å¥å­ï¼ˆå¦‚"${äººç‰©}åœ¨${åœ°ç‚¹}\${åŠ¨ä½œ}"ï¼‰ | ä½èµ„æºNER/å…³ç³»æŠ½å– |
| **é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆ** | ç”¨GPT-3ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼çš„å¥å­ | æ•°æ®æ‰©å……/å¯¹è¯ç³»ç»Ÿ |
| **å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ** | æ·»åŠ ä¸æ˜“å¯Ÿè§‰çš„æ‰°åŠ¨ï¼ˆFGSM/PGDï¼‰ | æ¨¡å‹é²æ£’æ€§æµ‹è¯• |

### 2.3 éšç©ºé—´å¢å¼º

*   **Mixup**ï¼šåœ¨åµŒå…¥ç©ºé—´çº¿æ€§æ’å€¼
    ```math
    \tilde{x} = \lambda x_i + (1-\lambda)x_j, \quad \tilde{y} = \lambda y_i + (1-\lambda)y_j
    ```
    **åº”ç”¨**ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆéœ€åœ¨BERTåµŒå…¥å±‚åæ“ä½œï¼‰
*   **EDA (Easy Data Augmentation)**ï¼šç»“åˆæ›¿æ¢/æ’å…¥/åˆ é™¤/äº¤æ¢
    **å·¥å…·**ï¼š`EDA-NLP`åº“

## 3. ä»»åŠ¡ä¸“ç”¨å¢å¼ºç­–ç•¥

### 3.1 æ–‡æœ¬åˆ†ç±»

*   **æ ‡ç­¾ä¸å˜å¢å¼º**ï¼šç¡®ä¿å¢å¼ºåæ–‡æœ¬æ ‡ç­¾ä¸å˜
    ```python
    from nlpaug.augmenter.word import SynonymAug
    aug = SynonymAug(aug_src='wordnet', aug_max=3)
    augmented_text = aug.augment("This movie is great")
    ```

### 3.2 å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

*   **å®ä½“æ›¿æ¢**ï¼šåŒç±»å‹å®ä½“äº’æ¢ï¼ˆå¦‚"åŒ—äº¬"â†’"ä¸Šæµ·"ï¼‰
*   **ä¸Šä¸‹æ–‡æ‰°åŠ¨**ï¼šä¿æŒå®ä½“ä¸å˜ï¼Œä¿®æ”¹å‘¨å›´è¯

### 3.3 æœºå™¨ç¿»è¯‘

*   **åŒå‘å›è¯‘**ï¼š
    ```text
    åŸæ–‡ï¼šä»Šå¤©å¤©æ°”çœŸå¥½  
    æ—¥è¯‘ï¼šä»Šæ—¥ã¯å¤©æ°—ãŒæœ¬å½“ã«è‰¯ã„  
    å›è¯‘ï¼šä»Šå¤©å¤©æ°”çœŸçš„å¾ˆå¥½ï¼ˆæ–°æ ·æœ¬ï¼‰
    ```

## 4. æ•°æ®å¢å¼ºçš„æ³¨æ„äº‹é¡¹

### 4.1 è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥

*   **é—®é¢˜**ï¼šåŒä¹‰è¯æ›¿æ¢å¯èƒ½æ”¹å˜è¯­ä¹‰ï¼ˆå¦‚"é“¶è¡Œ"â†’"æ²³å²¸"ï¼‰ã€‚
*   **è§£å†³æ–¹æ¡ˆ**ï¼š
    *   ä½¿ç”¨ä¸Šä¸‹æ–‡æ•æ„Ÿæ›¿æ¢ï¼ˆå¦‚BERT-Masked LMé¢„æµ‹ï¼‰ã€‚
    *   äººå·¥æŠ½æ ·éªŒè¯å¢å¼ºæ•°æ®è´¨é‡ã€‚

### 4.2 è¿‡å¢å¼ºé£é™©

*   **å®éªŒè¡¨æ˜**ï¼šå¢å¼ºæ•°æ®å æ¯”è¶…è¿‡50%å¯èƒ½æŸå®³æ€§èƒ½ã€‚
*   **æ¨èæ¯”ä¾‹**ï¼šåŸå§‹æ•°æ®çš„20%~200%ï¼ˆä¾ä»»åŠ¡è€Œå®šï¼‰ã€‚

### 4.3 é¢†åŸŸé€‚é…æ€§

*   **é€šç”¨å¢å¼º**ï¼ˆå¦‚EDAï¼‰åœ¨åŒ»ç–—/æ³•å¾‹é¢†åŸŸæ•ˆæœå·® â†’ éœ€é¢†åŸŸè¯å…¸æ”¯æŒã€‚

## 5. æœ€æ–°è¿›å±•ï¼ˆ2023ï¼‰

| æŠ€æœ¯ | è¯´æ˜ | è®ºæ–‡/å·¥å…· |
|---|---|---|
| **LLMå¢å¼º** | ç”¨ChatGPTç”Ÿæˆé«˜è´¨é‡å¢å¼ºæ•°æ® | ã€ŠGPT3 as Data Augmenterã€‹ |
| **å·®åˆ†éšç§å¢å¼º** | ä¿è¯å¢å¼ºæ•°æ®éšç§æ€§ï¼ˆå¦‚åŒ»ç–—NLPï¼‰ | `Diff-Privacy-NLP` |
| **å¼ºåŒ–å­¦ä¹ é€‰æ‹©** | è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¢å¼ºç­–ç•¥ | ã€ŠRL-Aug: NLP Data Augmentation via Reinforcement Learningã€‹ |

## 6. å®Œæ•´Pipelineç¤ºä¾‹

```python
# ä½¿ç”¨NLPAUG+å›è¯‘çš„å¢å¼ºæµç¨‹
import nlpaug.augmenter.word as naw

# åŒä¹‰è¯å¢å¼º
syn_aug = naw.SynonymAug(aug_src='wordnet')
texts = ["The quick brown fox jumps over the lazy dog"]
augmented = syn_aug.augment(texts, n=3)  # ç”Ÿæˆ3ä¸ªå˜ä½“

# å›è¯‘å¢å¼º
back_translation = naw.BackTranslationAug(
    from_model_name='facebook/wmt19-en-de',
    to_model_name='facebook/wmt19-de-en'
)
bt_text = back_translation.augment("This is a test")
```

## 7. æ€»ç»“

*   **ä½•æ—¶ä½¿ç”¨**ï¼šæ•°æ®é‡<10kæ—¶æ•ˆæœæ˜¾è‘—ï¼Œå°¤å…¶æ¨èç”¨äº**ä½èµ„æºè¯­è¨€**å’Œ**é•¿å°¾åˆ†å¸ƒ**ä»»åŠ¡ã€‚
*   **é¿å‘æŒ‡å—**ï¼š
    1.  é¿å…å¯¹**è¯­æ³•æ•æ„Ÿä»»åŠ¡**ï¼ˆå¦‚å¥æ³•åˆ†æï¼‰ä½¿ç”¨å­—ç¬¦çº§æ‰°åŠ¨ã€‚
    2.  ç”Ÿæˆå¼å¢å¼ºï¼ˆå¦‚GPT-3ï¼‰éœ€è¿‡æ»¤ä½è´¨é‡æ ·æœ¬ã€‚
*   **æœªæ¥æ–¹å‘**ï¼š
    *   å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¢å¼ºå¼•æ“
    *   å¢å¼ºç­–ç•¥çš„å…ƒå­¦ä¹ è‡ªåŠ¨åŒ–é€‰æ‹©

> ğŸ”¥ **æœ€ä½³å®è·µ**ï¼šå…ˆå°è¯•**å›è¯‘+åŒä¹‰è¯æ›¿æ¢**ç»„åˆï¼Œç›‘æ§éªŒè¯é›†è¡¨ç°å†è°ƒæ•´å¢å¼ºå¼ºåº¦!

## ç±»åˆ«ä¸å¹³è¡¡å¤„ç†è¯¦è§£

# ç±»åˆ«ä¸å¹³è¡¡å¤„ç†è¯¦è§£

## 1. ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ¦‚è¿°

åœ¨NLPä»»åŠ¡ä¸­ï¼Œ**ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜**ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æä¸­è´Ÿé¢æ ·æœ¬ä»…å 5%ï¼‰ä¼šå¯¼è‡´æ¨¡å‹åå‘å¤šæ•°ç±»ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚

### 1.1 é—®é¢˜è¡¨ç°
- æ¨¡å‹åœ¨å¤šæ•°ç±»ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå°‘æ•°ç±»å¬å›ç‡æä½
- æ•´ä½“å‡†ç¡®ç‡é«˜ä½†F1åˆ†æ•°ä½
- æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°å°‘æ•°ç±»çš„ç‰¹å¾

### 1.2 ä¸å¹³è¡¡ç¨‹åº¦åˆ†ç±»
- **è½»åº¦ä¸å¹³è¡¡**ï¼š1:3 æ¯”ä¾‹
- **ä¸­åº¦ä¸å¹³è¡¡**ï¼š1:10 æ¯”ä¾‹  
- **æåº¦ä¸å¹³è¡¡**ï¼š1:100+ æ¯”ä¾‹

## 2. æ•°æ®å±‚é¢çš„è§£å†³æ–¹æ³•

### 2.1 è¿‡é‡‡æ ·ï¼ˆOversamplingï¼‰

#### éšæœºè¿‡é‡‡æ ·
```python
import numpy as np
from collections import Counter

def random_oversampling(X, y, random_state=42):
    """éšæœºè¿‡é‡‡æ ·"""
    np.random.seed(random_state)
    
    # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
    class_counts = Counter(y)
    majority_class = max(class_counts, key=class_counts.get)
    minority_class = min(class_counts, key=class_counts.get)
    
    # è®¡ç®—éœ€è¦å¤åˆ¶çš„æ ·æœ¬æ•°
    target_count = class_counts[majority_class]
    
    # è·å–å°‘æ•°ç±»æ ·æœ¬
    minority_indices = np.where(y == minority_class)[0]
    minority_X = X[minority_indices]
    minority_y = y[minority_indices]
    
    # éšæœºå¤åˆ¶å°‘æ•°ç±»æ ·æœ¬
    indices_to_repeat = np.random.choice(
        len(minority_indices), 
        size=target_count - len(minority_indices), 
        replace=True
    )
    
    # åˆå¹¶åŸå§‹æ ·æœ¬å’Œå¤åˆ¶æ ·æœ¬
    oversampled_X = np.vstack([X, minority_X[indices_to_repeat]])
    oversampled_y = np.hstack([y, minority_y[indices_to_repeat]])
    
    return oversampled_X, oversampled_y
```

#### SMOTEï¼ˆSynthetic Minority Over-sampling Techniqueï¼‰
```python
from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer

def smote_oversampling(texts, labels):
    """ä½¿ç”¨SMOTEè¿›è¡Œè¿‡é‡‡æ ·"""
    # æ–‡æœ¬å‘é‡åŒ–
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(texts)
    
    # åº”ç”¨SMOTE
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, labels)
    
    return X_resampled, y_resampled, vectorizer

# ä½¿ç”¨ç¤ºä¾‹
texts = ["positive text", "negative text", "positive text", "positive text"]
labels = [1, 0, 1, 1]  # 3ä¸ªæ­£ç±»ï¼Œ1ä¸ªè´Ÿç±»

X_res, y_res, vectorizer = smote_oversampling(texts, labels)
print(f"åŸå§‹æ ·æœ¬æ•°: {len(texts)}, é‡é‡‡æ ·å: {len(X_res)}")
```

#### å›è¯‘å¢å¼º
```python
from googletrans import Translator

def back_translation_augmentation(texts, labels, target_language='en'):
    """å›è¯‘å¢å¼º"""
    translator = Translator()
    augmented_texts = []
    augmented_labels = []
    
    for text, label in zip(texts, labels):
        # ç¿»è¯‘åˆ°ç›®æ ‡è¯­è¨€
        translated = translator.translate(text, dest=target_language)
        # ç¿»è¯‘å›åŸè¯­è¨€
        back_translated = translator.translate(translated.text, dest='zh-cn')
        
        augmented_texts.append(back_translated.text)
        augmented_labels.append(label)
    
    return augmented_texts, augmented_labels
```

### 2.2 æ¬ é‡‡æ ·ï¼ˆUndersamplingï¼‰

#### éšæœºæ¬ é‡‡æ ·
```python
def random_undersampling(X, y, random_state=42):
    """éšæœºæ¬ é‡‡æ ·"""
    np.random.seed(random_state)
    
    class_counts = Counter(y)
    minority_class = min(class_counts, key=class_counts.get)
    target_count = class_counts[minority_class]
    
    # è·å–å¤šæ•°ç±»æ ·æœ¬
    majority_indices = np.where(y != minority_class)[0]
    
    # éšæœºé€‰æ‹©ä¸å°‘æ•°ç±»ç­‰é‡çš„å¤šæ•°ç±»æ ·æœ¬
    selected_indices = np.random.choice(
        majority_indices, 
        size=target_count, 
        replace=False
    )
    
    # è·å–å°‘æ•°ç±»æ ·æœ¬
    minority_indices = np.where(y == minority_class)[0]
    
    # åˆå¹¶æ ·æœ¬
    undersampled_X = np.vstack([X[selected_indices], X[minority_indices]])
    undersampled_y = np.hstack([y[selected_indices], y[minority_indices]])
    
    return undersampled_X, undersampled_y
```

#### Tomek Links
```python
from imblearn.under_sampling import TomekLinks

def tomek_links_undersampling(X, y):
    """Tomek Linksæ¬ é‡‡æ ·"""
    tl = TomekLinks()
    X_resampled, y_resampled = tl.fit_resample(X, y)
    return X_resampled, y_resampled
```

### 2.3 æ··åˆé‡‡æ ·

#### SMOTE + Tomek Links
```python
from imblearn.combine import SMOTETomek

def smote_tomek_sampling(X, y):
    """SMOTE + Tomek Linksæ··åˆé‡‡æ ·"""
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)
    return X_resampled, y_resampled
```

## 3. ç®—æ³•å±‚é¢çš„è§£å†³æ–¹æ³•

### 3.1 ç±»åˆ«æƒé‡è°ƒæ•´

#### æŸå¤±å‡½æ•°æƒé‡
```python
import torch
import torch.nn as nn

class WeightedCrossEntropyLoss(nn.Module):
    def __init__(self, class_weights):
        super().__init__()
        self.class_weights = torch.tensor(class_weights)
    
    def forward(self, inputs, targets):
        return nn.functional.cross_entropy(
            inputs, targets, weight=self.class_weights
        )

# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾è´Ÿç±»æ ·æœ¬æ˜¯æ­£ç±»çš„10å€
class_weights = [1.0, 10.0]  # [å¤šæ•°ç±»æƒé‡, å°‘æ•°ç±»æƒé‡]
criterion = WeightedCrossEntropyLoss(class_weights)
```

#### sklearnä¸­çš„æƒé‡è®¾ç½®
```python
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier

# SVMç±»åˆ«æƒé‡
svm_model = LinearSVC(class_weight={0: 1, 1: 10})

# éšæœºæ£®æ—ç±»åˆ«æƒé‡
rf_model = RandomForestClassifier(class_weight='balanced')
```

### 3.2 æ”¹è¿›çš„æŸå¤±å‡½æ•°

#### Focal Loss
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        bce_loss = nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none'
        )
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss
        return focal_loss.mean()

# ä½¿ç”¨ç¤ºä¾‹
focal_loss = FocalLoss(alpha=0.25, gamma=2)
```

#### æ ‡ç­¾å¹³æ»‘æŸå¤±
```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.1, dim=-1):
        super().__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.cls = classes
        self.dim = dim
    
    def forward(self, pred, target):
        pred = pred.log_softmax(dim=self.dim)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.cls - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))
```

### 3.3 é˜ˆå€¼ç§»åŠ¨ï¼ˆThreshold Movingï¼‰

```python
from sklearn.metrics import precision_recall_curve, f1_score

def find_optimal_threshold(y_true, y_pred_proba):
    """å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba)
    
    # è®¡ç®—F1åˆ†æ•°
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)
    f1_scores = f1_scores[:-1]  # ç§»é™¤æœ€åä¸€ä¸ªå…ƒç´ ï¼ˆå¯¹åº”threshold=infï¼‰
    
    # æ‰¾åˆ°æœ€å¤§F1åˆ†æ•°å¯¹åº”çš„é˜ˆå€¼
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    return optimal_threshold

# ä½¿ç”¨ç¤ºä¾‹
y_true = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred_proba = [0.1, 0.8, 0.2, 0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.9]

optimal_threshold = find_optimal_threshold(y_true, y_pred_proba)
print(f"æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.3f}")

# ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œé¢„æµ‹
y_pred = [1 if prob > optimal_threshold else 0 for prob in y_pred_proba]
```

## 4. æ¨¡å‹æ¶æ„çš„æ”¹è¿›

### 4.1 é›†æˆæ–¹æ³•

#### EasyEnsemble
```python
from imblearn.ensemble import EasyEnsembleClassifier

def easy_ensemble_training(X, y, n_estimators=10):
    """EasyEnsembleè®­ç»ƒ"""
    eec = EasyEnsembleClassifier(n_estimators=n_estimators, random_state=42)
    eec.fit(X, y)
    return eec

# ä½¿ç”¨ç¤ºä¾‹
eec_model = easy_ensemble_training(X_train, y_train)
predictions = eec_model.predict(X_test)
```

#### è‡ªå®šä¹‰é›†æˆ
```python
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import StratifiedKFold

def custom_ensemble(X, y, base_models, n_splits=5):
    """è‡ªå®šä¹‰é›†æˆæ–¹æ³•"""
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    ensemble_models = []
    
    for train_idx, val_idx in skf.split(X, y):
        X_train_fold, X_val_fold = X[train_idx], X[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]
        
        # å¯¹æ¯ä¸ªfoldè¿›è¡Œæ¬ é‡‡æ ·
        X_train_balanced, y_train_balanced = random_undersampling(
            X_train_fold, y_train_fold
        )
        
        # è®­ç»ƒæ¨¡å‹
        model = base_models[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŸºç¡€æ¨¡å‹
        model.fit(X_train_balanced, y_train_balanced)
        ensemble_models.append(model)
    
    return ensemble_models

def ensemble_predict(ensemble_models, X):
    """é›†æˆé¢„æµ‹"""
    predictions = []
    for model in ensemble_models:
        pred = model.predict(X)
        predictions.append(pred)
    
    # å¤šæ•°æŠ•ç¥¨
    ensemble_pred = np.mean(predictions, axis=0) > 0.5
    return ensemble_pred.astype(int)
```

### 4.2 ä¸¤é˜¶æ®µè®­ç»ƒ

```python
def two_stage_training(X, y, feature_extractor, classifier):
    """ä¸¤é˜¶æ®µè®­ç»ƒ"""
    # ç¬¬ä¸€é˜¶æ®µï¼šç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒç‰¹å¾æå–å™¨
    print("ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒç‰¹å¾æå–å™¨")
    feature_extractor.fit(X, y)
    
    # æå–ç‰¹å¾
    X_features = feature_extractor.transform(X)
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¯¹åˆ†ç±»å±‚ä½¿ç”¨å¹³è¡¡æ•°æ®å¾®è°ƒ
    print("ç¬¬äºŒé˜¶æ®µï¼šå¹³è¡¡æ•°æ®å¾®è°ƒåˆ†ç±»å™¨")
    X_balanced, y_balanced = smote_oversampling(X_features, y)
    classifier.fit(X_balanced, y_balanced)
    
    return feature_extractor, classifier
```

## 5. è¯„ä¼°æŒ‡æ ‡çš„è°ƒæ•´

### 5.1 ä¸å¹³è¡¡åˆ†ç±»çš„è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score

def evaluate_imbalanced_classification(y_true, y_pred, y_pred_proba=None):
    """è¯„ä¼°ä¸å¹³è¡¡åˆ†ç±»ç»“æœ"""
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred, target_names=['å¤šæ•°ç±»', 'å°‘æ•°ç±»']))
    
    # ROC-AUC
    if y_pred_proba is not None:
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        print(f"ROC-AUC: {roc_auc:.4f}")
    
    # PR-AUCï¼ˆæ›´é€‚åˆä¸å¹³è¡¡æ•°æ®ï¼‰
    if y_pred_proba is not None:
        pr_auc = average_precision_score(y_true, y_pred_proba)
        print(f"PR-AUC: {pr_auc:.4f}")
    
    # G-Mean
    from sklearn.metrics import recall_score
    recall_0 = recall_score(y_true, y_pred, pos_label=0)
    recall_1 = recall_score(y_true, y_pred, pos_label=1)
    g_mean = np.sqrt(recall_0 * recall_1)
    print(f"G-Mean: {g_mean:.4f}")

# ä½¿ç”¨ç¤ºä¾‹
evaluate_imbalanced_classification(y_true, y_pred, y_pred_proba)
```

### 5.2 æ··æ·†çŸ©é˜µå¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, class_names=None):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
plot_confusion_matrix(y_true, y_pred, class_names=['å¤šæ•°ç±»', 'å°‘æ•°ç±»'])
```

## 6. é¢†åŸŸç‰¹å®šè§£å†³æ–¹æ¡ˆ

### 6.1 æ–‡æœ¬ç”Ÿæˆä»»åŠ¡

```python
def temperature_scaling_for_generation(model, text, temperature=0.7):
    """æ¸©åº¦é‡‡æ ·è°ƒæ•´ç”Ÿæˆæ¦‚ç‡"""
    # è°ƒæ•´softmaxæ¸©åº¦ï¼Œå¢åŠ å°‘æ•°ç±»è¯æ±‡æ¦‚ç‡
    logits = model(text)
    scaled_logits = logits / temperature
    probs = torch.softmax(scaled_logits, dim=-1)
    
    return probs
```

### 6.2 å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

```python
def entity_level_oversampling(sentences, labels):
    """å®ä½“çº§åˆ«è¿‡é‡‡æ ·"""
    # ç»Ÿè®¡å„ç±»å®ä½“æ•°é‡
    entity_counts = {}
    for sentence, label_seq in zip(sentences, labels):
        for token, label in zip(sentence, label_seq):
            if label != 'O':  # éOæ ‡ç­¾
                if label not in entity_counts:
                    entity_counts[label] = []
                entity_counts[label].append((sentence, label_seq))
    
    # å¯¹å°‘æ•°ç±»å®ä½“è¿›è¡Œè¿‡é‡‡æ ·
    balanced_data = []
    max_count = max(len(entities) for entities in entity_counts.values())
    
    for entity_type, entities in entity_counts.items():
        if len(entities) < max_count:
            # é‡å¤é‡‡æ ·
            oversampled = np.random.choice(
                entities, 
                size=max_count, 
                replace=True
            )
            balanced_data.extend(oversampled)
        else:
            balanced_data.extend(entities)
    
    return zip(*balanced_data)
```

### 6.3 é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ

```python
def dynamic_masking_for_imbalanced_data(tokenizer, texts, labels, 
                                       minority_class_boost=0.2):
    """åŠ¨æ€æ©ç æ¯”ä¾‹è°ƒæ•´"""
    masked_texts = []
    
    for text, label in zip(texts, labels):
        tokens = tokenizer.tokenize(text)
        
        # å°‘æ•°ç±»æ–‡æœ¬å¢åŠ æ©ç æ¦‚ç‡
        if label == 1:  # å‡è®¾1æ˜¯å°‘æ•°ç±»
            mask_prob = 0.15 + minority_class_boost
        else:
            mask_prob = 0.15
        
        # éšæœºæ©ç 
        masked_tokens = []
        for token in tokens:
            if np.random.random() < mask_prob:
                masked_tokens.append('[MASK]')
            else:
                masked_tokens.append(token)
        
        masked_texts.append(' '.join(masked_tokens))
    
    return masked_texts
```

## 7. å®Œæ•´Pipelineç¤ºä¾‹

```python
def complete_imbalanced_classification_pipeline():
    """å®Œæ•´çš„ç±»åˆ«ä¸å¹³è¡¡å¤„ç†æµç¨‹"""
    
    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    # texts, labels = load_data()
    
    # 2. æ–‡æœ¬å‘é‡åŒ–
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(texts)
    
    # 3. æ•°æ®é‡é‡‡æ ·
    X_resampled, y_resampled = smote_oversampling(X, labels)
    
    # 4. æ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨ç±»åˆ«æƒé‡ï¼‰
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(class_weight='balanced', random_state=42)
    model.fit(X_resampled, y_resampled)
    
    # 5. é¢„æµ‹å’Œé˜ˆå€¼ä¼˜åŒ–
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    optimal_threshold = find_optimal_threshold(y_test, y_pred_proba)
    y_pred = (y_pred_proba > optimal_threshold).astype(int)
    
    # 6. è¯„ä¼°
    evaluate_imbalanced_classification(y_test, y_pred, y_pred_proba)
    
    return model, vectorizer, optimal_threshold

# ä½¿ç”¨Pipeline
model, vectorizer, threshold = complete_imbalanced_classification_pipeline()
```

## 8. æ–¹æ³•å¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|----------|------|------|
| **SMOTE** | å°è§„æ¨¡ä¸å¹³è¡¡æ–‡æœ¬ï¼ˆ<10kæ¡ï¼‰ | ç”Ÿæˆè¯­ä¹‰åˆç†çš„æ ·æœ¬ | è®¡ç®—æˆæœ¬é«˜ |
| **ç±»åˆ«æƒé‡** | å¤§è§„æ¨¡æ•°æ®+æ·±åº¦å­¦ä¹ æ¨¡å‹ | æ— éœ€ä¿®æ”¹æ•°æ®åˆ†å¸ƒ | éœ€è°ƒå‚ |
| **Focal Loss** | é«˜åº¦ä¸å¹³è¡¡ï¼ˆ1:100+ï¼‰ | è‡ªåŠ¨èšç„¦éš¾æ ·æœ¬ | å¯èƒ½è®­ç»ƒä¸ç¨³å®š |
| **ä¸¤é˜¶æ®µè®­ç»ƒ** | é¢„è®­ç»ƒæ¨¡å‹ï¼ˆBERT/GPTï¼‰ | ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯† | éœ€è¦é¢å¤–è®­ç»ƒæ­¥éª¤ |

## 9. å…³é”®è¦ç‚¹æ€»ç»“

1. **è½»åº¦ä¸å¹³è¡¡ï¼ˆ1:3ï¼‰**ï¼šä½¿ç”¨ç±»åˆ«æƒé‡æˆ–é˜ˆå€¼ç§»åŠ¨
2. **ä¸­åº¦ä¸å¹³è¡¡ï¼ˆ1:10ï¼‰**ï¼šSMOTE + é›†æˆæ–¹æ³•
3. **æåº¦ä¸å¹³è¡¡ï¼ˆ1:100+ï¼‰**ï¼šFocal Loss + ä¸¤é˜¶æ®µè®­ç»ƒ
4. **è¯„ä¼°æŒ‡æ ‡**ï¼šä¼˜å…ˆä½¿ç”¨F1-Macroã€AUC-ROCã€G-Mean
5. **æœ€ä½³å®è·µ**ï¼š
   - ä¼˜å…ˆå°è¯•å¯¹æ¨¡å‹æ— æŸçš„æ–¹æ³•ï¼ˆå¦‚æŸå¤±å‡½æ•°è°ƒæ•´ï¼‰
   - è¿‡é‡‡æ ·æ—¶ç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„è¯­ä¹‰åˆç†æ€§
   - å§‹ç»ˆç”¨å®å¹³å‡F1/AUC-ROCæ›¿ä»£å‡†ç¡®ç‡è¯„ä¼° 

